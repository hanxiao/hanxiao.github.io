<!DOCTYPE html><html lang="en"><style>html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}article,header,main{display:block}a{background-color:transparent}h1{font-size:2em;margin:.67em 0}img{border:0}body,html{width:100%;height:100%}html{width:100%;height:100vh;display:flex;flex-direction:column;justify-content:center;align-items:center;background:var(--bgColor);--bgColor:#fff;--textColor:#2c3e50;--bg2ndColor:none;--bg3rdColor:#f8f8f8;--preCodeColor:#525252;--imgOpacity:1.0}@media (prefers-color-scheme:dark){html{background:var(--bgColor);--bgColor:#121212;--textColor:#fff;--bg2ndColor:#fff;--bg3rdColor:#332940;--preCodeColor:#f8f8f8;--imgOpacity:0.5}}body{margin:0;color:var(--textColor);font-size:18px;line-height:1.6;background-color:var(--bgColor);font-family:sourcesanspro,'Helvetica Neue',Arial,sans-serif}ul.nav{margin:0;padding:0;list-style-type:none}ul{margin:1rem 0}a{color:var(--textColor);text-decoration:none}.flag-icon{height:25px;width:25px;display:inline;border-radius:50%;vertical-align:sub}.icon_item{padding-left:5px!important;padding-right:5px!important}.reading-progress-bar{background:#42b983;display:block;height:2px;left:0;position:fixed;top:0;width:0;z-index:10001}header{min-height:60px}header .logo-link{float:left}header .nav{float:right;left:80px}header .logo-link img{height:60px}header .nav-list-item{display:inline-block;padding:19px 10px}header .nav-list-item a{line-height:1.4}@media screen and (max-width:900px){header .nav-list-item a{font-size:12px}}@media screen and (min-width:900px){header .nav-list-item a{font-size:18px}}.post{padding-top:1em}.post-block .post-title{margin:.65em 0;color:var(--textColor);font-size:1.5em}.post-block .post-info{color:#7f8c8d}.post-block .post-info .read-time{text-align:right}.post-content h2,.post-content h4{position:relative;margin:1em 0}.post-content h2 :before,.post-content h4 :before{content:"#";color:#42b983;position:absolute;left:-.7em;top:-4px;font-size:1.2em;font-weight:700}.post-content h4 :before{content:">"}.post-content h2{font-size:22px}.post-content h4{font-size:18px}.post-content a{color:#42b983;word-break:break-all}main.container{margin:2em 10px}@media screen and (min-width:900px){.wrap{width:900px;margin:0 auto}header{padding:20px 60px}}@media screen and (max-width:900px){.wrap{width:100%}header{min-height:50px;padding:2px 2px;position:fixed;z-index:10000;border-radius:15px;left:50%;-webkit-transform:translateX(-50%);transform:translateX(-50%);width:-webkit-fit-content;width:-moz-fit-content;width:fit-content}header a.logo-link,header ul.nav.nav-list{float:none;display:inline;text-align:center}header li.nav-list-item{padding:10px 5px}header .logo-link img{height:20px;vertical-align:sub}header .flag-icon{height:20px;width:20px}header{background-color:rgba(255,255,255,.9)}@supports ((-webkit-backdrop-filter:blur(2em)) or (backdrop-filter:blur(2em))){header{background-color:rgba(255,255,255,.3);-webkit-backdrop-filter:blur(10px);backdrop-filter:blur(10px)}}main.container{padding-top:2em}main.container{margin:0 20px}.post-content h2,.post-content h4{max-width:300px;left:15px}}@font-face{font-family:sourcesanspro;src:url(/font/sourcesanspro.woff2) format("woff2"),url(/font/sourcesanspro.woff) format("woff");font-weight:400;font-style:normal}</style><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Video Semantic Search in Large Scale using GNES and Tensorflow 2.0 · Han Xiao Tech Blog - Neural Search & AI Engineering</title><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content="@hxiao"><meta name="twitter:creator" content="@hxiao"><meta name="description" content="Many people may know me from bert-as-service (and of course  from Fashion-MNIST &amp;lt;/bragging&amp;gt;). So when they first heard about my new project GNES ... · Han Xiao"><meta property="og:title" content="Video Semantic Search in Large Scale using GNES and Tensorflow 2.0 · Han Xiao Tech Blog - Neural Search &amp; AI Engineering"><meta property="og:description" content="Many people may know me from bert-as-service (and of course  from Fashion-MNIST &amp;lt;/bragging&amp;gt;). So when they first heard about my new project GNES ... · Han Xiao"><meta property="og:url" content="https://hanxiao.io/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0/"><meta property="og:image" content="https://hanxiao.io/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0//9ba076d0.png"><meta property="og:type" content="article"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/myavatar.png"><link rel="alternate" type="application/rss+xml" title="Han Xiao Tech Blog - Neural Search &amp; AI Engineering" href="https://hanxiao.io/atom.xml"><!-- - use css preload trick--><link rel="preload" href="/css/apollo.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="/css/apollo.css"></noscript><script>/*! loadCSS. [c]2017 Filament Group, Inc. MIT License */
/* This file is meant as a standalone workflow for
- testing support for link[rel=preload]
- enabling async CSS loading in browsers that do not support rel=preload
- applying rel preload css once loaded, whether supported or not.
*/
(function( w ){
    "use strict";
    // rel=preload support test
    if( !w.loadCSS ){
        w.loadCSS = function(){};
    }
    // define on the loadCSS obj
    var rp = loadCSS.relpreload = {};
    // rel=preload feature support test
    // runs once and returns a function for compat purposes
    rp.support = (function(){
        var ret;
        try {
            ret = w.document.createElement( "link" ).relList.supports( "preload" );
        } catch (e) {
            ret = false;
        }
        return function(){
            return ret;
        };
    })();

    // if preload isn't supported, get an asynchronous load by using a non-matching media attribute
    // then change that media back to its intended value on load
    rp.bindMediaToggle = function( link ){
        // remember existing media attr for ultimate state, or default to 'all'
        var finalMedia = link.media || "all";

        function enableStylesheet(){
            link.media = finalMedia;
        }

        // bind load handlers to enable media
        if( link.addEventListener ){
            link.addEventListener( "load", enableStylesheet );
        } else if( link.attachEvent ){
            link.attachEvent( "onload", enableStylesheet );
        }

        // Set rel and non-applicable media type to start an async request
        // note: timeout allows this to happen async to let rendering continue in IE
        setTimeout(function(){
            link.rel = "stylesheet";
            link.media = "only x";
        });
        // also enable media after 3 seconds,
        // which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
        setTimeout( enableStylesheet, 3000 );
    };

    // loop through link elements in DOM
    rp.poly = function(){
        // double check this to prevent external calls from running
        if( rp.support() ){
            return;
        }
        var links = w.document.getElementsByTagName( "link" );
        for( var i = 0; i < links.length; i++ ){
            var link = links[ i ];
            // qualify links to those with rel=preload and as=style attrs
            if( link.rel === "preload" && link.getAttribute( "as" ) === "style" && !link.getAttribute( "data-loadcss" ) ){
                // prevent rerunning on link
                link.setAttribute( "data-loadcss", true );
                // bind listeners to toggle media back
                rp.bindMediaToggle( link );
            }
        }
    };

    // if unsupported, run the polyfill
    if( !rp.support() ){
        // run once at least
        rp.poly();

        // rerun poly on an interval until onload
        var run = w.setInterval( rp.poly, 500 );
        if( w.addEventListener ){
            w.addEventListener( "load", function(){
                rp.poly();
                w.clearInterval( run );
            } );
        } else if( w.attachEvent ){
            w.attachEvent( "onload", function(){
                rp.poly();
                w.clearInterval( run );
            } );
        }
    }


    // commonjs
    if( typeof exports !== "undefined" ){
        exports.loadCSS = loadCSS;
    }
    else {
        w.loadCSS = loadCSS;
    }
}( typeof global !== "undefined" ? global : this ) );</script><script id="mcjs">!function(c,h,i,m,p){m=c.createElement(h),p=c.getElementsByTagName(h)[0],m.async=1,m.src=i,p.parentNode.insertBefore(m,p)}(document,"script","https://chimpstatic.com/mcjs-connected/js/users/7da58fc9885cb85d4a9f0ad9a/987f901145f1749fd3e800e86.js");</script><link rel="search" type="application/opensearchdescription+xml" href="https://hanxiao.io/atom.xml" title="Han Xiao Tech Blog - Neural Search &amp; AI Engineering"></head><body><div class="reading-progress-bar"></div><div class="wrap"><header><ul class="nav nav-list"><li class="nav-list-item"><a href="/about/" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="https://www.linkedin.com/in/hxiao87" target="_blank" class="nav-list-link">LINKEDIN</a></li><li class="nav-list-item"><a href="https://x.com/hxiao" target="_blank" class="nav-list-link">X</a></li><li class="nav-list-item"><a href="https://github.com/hanxiao" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="https://scholar.google.com/citations?user=jp7swwIAAAAJ" target="_blank" class="nav-list-link">SCHOLAR</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">Video Semantic Search in Large Scale using GNES and Tensorflow 2.0</h1><div class="post-info">Nov 22, 2019 by &nbsp;&nbsp;&nbsp;<img src="/myavatar.png" alt="logo" width="18px" height="18px" style="vertical-align: sub;">&nbsp;<a href="/about" target="_blank" class="nav-list-link">Han Xiao - <i>ex</i> Engineering Lead @ Tencent AI Lab</a></div><div class="post-info"><div class="read-time">◷&nbsp;&nbsp;&nbsp; 17  min read</div></div><div class="post-content"><h2><span id="background">Background</span></h2><p>Many people may know me from <a href="https://github.com/hanxiao/bert-as-service" target="_blank" rel="noopener"><code>bert-as-service</code></a> (and of course  from <a href="https://github.com/zalandoresearch/fashion-mnist" target="_blank" rel="noopener">Fashion-MNIST</a> <code>&lt;/bragging&gt;</code>). So when they first heard about my new project <a href="/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/" title="GNES: Generic Neural Elastic Search">GNES: Generic Neural Elastic Search</a>, people naturally think that I’m building a semantic text search solution. But actually, GNES has a more <em>ambitious</em> goal to become the next-generation semantic search engine for all content forms, including text, image, video and audio.  <a id="more"></a> In this post, I will show you how to use the latest GNES Flow API and Tensorflow 2.0 to build a video semantic search system. For the impatient, feel free to watch the teaser video below before continue reading. </p>
<video width="90%" controls muted loop poster="6186010f.png"><br>  <source src="gnes-video-search.mp4" type="video/mp4"><br>Your browser does not support the HTML5 video tag. :(<br></video>

<!-- toc -->
<ul>
<li><a href="#formulating-the-problem-in-gnes-framework">Formulating the Problem in GNES Framework</a></li>
<li><a href="#preprocessing-videos">Preprocessing Videos</a></li>
<li><a href="#encoding-chunks-into-vectors">Encoding Chunks into Vectors</a></li>
<li><a href="#indexing-chunks-and-documents">Indexing Chunks and Documents</a></li>
<li><a href="#scoring-results">Scoring Results</a></li>
<li><a href="#putting-it-all-together">Putting it All Together</a><ul>
<li><a href="#what-should-we-sendreceive">What Should We Send/Receive?</a></li>
</ul>
</li>
<li><a href="#summary">Summary</a></li>
</ul>
<!-- tocstop -->
<p>I plan to have a series on the topic of video semantic search using GNES. This article serves as the first part. Readers who are looking for benchmarking, evaluations and models comparision, stay tuned. </p>
<h2><span id="formulating-the-problem-in-gnes-framework">Formulating the Problem in GNES Framework</span></h2><p>The data we are using is <a href="http://raingo.github.io/TGIF-Release/" target="_blank" rel="noopener">Tumblr GIF (TGIF) dataset</a>, which contains 100K animated GIFs and 120K sentences describing visual contents. Our problem is the following: <strong>given a video database and a query video, find the top-k semantically related videos from the database.</strong></p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center"></th>
<th style="text-align:center"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0/tumblr_njqj3bMKQF1unc0x7o1_250.gif"></td>
<td style="text-align:center"><img src="/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0/tumblr_ni35trgNe41tmk5mfo1_400.gif"></td>
<td style="text-align:center"><img src="/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0/tumblr_nb2mucKMeU1tkz79uo1_250.gif"></td>
</tr>
<tr>
<td style="text-align:center">A well-dressed young guy with gelled red hair glides across a room and scans it with his eyes.</td>
<td style="text-align:center">a woman in a car is singing.</td>
<td style="text-align:center">a man wearing a suit smiles at something in the distance.</td>
</tr>
</tbody>
</table>
<p>“Semantic” is a casual and ambiguous word, I know. Depending on your applications and scenarios, it could mean motion-wise similar (sports video), emotional similar (e.g. memes), etc. Right now I will just consider semantically-related as as visually similar.</p>
<p>Text descriptions of the videos, though potentially can be very useful, are ignored at the moment. We are not building a cross-modality search solution (e.g. from text to video or vice versa), we also do not leverage textual information when building the video search solution. Nonetheless, those text descriptions can be used to evaluate/compare the effectiveness of the system in a quantitative manner. </p>
<p>Putting the problem into <a href="/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/" title="GNES framework">GNES framework</a>, this breaks down into the following steps:</p>
<blockquote>
<p><strong>Index time</strong></p>
<ol>
<li>segment each video into workable semantic units (aka “Chunk” in GNES);</li>
<li>encode each chunk as a fixed-length vector;</li>
<li>store all vector representations in a vector database.</li>
</ol>
</blockquote>
<blockquote>
<p><strong>Query time</strong></p>
<ol>
<li>do steps <code>1</code>,<code>2</code> in the index time for each incoming query;</li>
<li>retrieve relevant chunks from database;</li>
<li>aggregate the chunk-level score back to document-level;</li>
<li>return the top-k results to users.   </li>
</ol>
</blockquote>
<p>If you find these steps hard to follow, then please first <a href="/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/" title="read this blog post">read this blog post</a> to understand the philosophy behind GNES. These steps can be accomplished by using the <code>preprocessor</code>, <code>encoder</code>, <code>indexer</code> and <code>router</code> microservices in GNES. Before we dig into the concrete design of each service, we can first write down these two runtimes using <a href="/2019/10/18/GNES-Flow-a-Pythonic-Way-to-Build-Cloud-Native-Neural-Search-Pipelines/" title="the GNES Flow API">the GNES Flow API</a>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">num_rep = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">index_flow = (Flow()</span><br><span class="line">      .add_preprocessor(name=<span class="string">'chunk_proc'</span>, replicas=num_rep)</span><br><span class="line">      .add_indexer(name=<span class="string">'doc_idx'</span>)</span><br><span class="line">      .add_encoder(replicas=num_rep, recv_from=<span class="string">'chunk_proc'</span>)</span><br><span class="line">      .add_indexer(name=<span class="string">'vec_idx'</span>)</span><br><span class="line">      .add_router(name=<span class="string">'sync_barrier'</span>, yaml_path=<span class="string">'BaseReduceRouter'</span>,</span><br><span class="line">                  num_part=<span class="number">2</span>, recv_from=[<span class="string">'vec_idx'</span>, <span class="string">'doc_idx'</span>]))</span><br><span class="line"></span><br><span class="line">query_flow = (Flow()</span><br><span class="line">     .add_preprocessor(name=<span class="string">'chunk_proc'</span>, replicas=num_rep)</span><br><span class="line">     .add_encoder(replicas=num_rep)</span><br><span class="line">     .add_indexer(name=<span class="string">'vec_idx'</span>)</span><br><span class="line">     .add_router(name=<span class="string">'scorer'</span>)</span><br><span class="line">     .add_indexer(name=<span class="string">'doc_idx'</span>, sorted_response=<span class="string">'descend'</span>))</span><br></pre></td></tr></table></figure>
<p>One can visualize these flows by <code>flow.build(backend=None).to_url()</code>,  which gives:</p>
<blockquote>
<p>Index flow<br><img src="/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0/29b84023.png"></p>
</blockquote>
<blockquote>
<p>Query flow<br><img src="/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0/35dd4b9c.png"></p>
</blockquote>
<p>More usages and specifications of GNES Flow API <a href="/2019/10/18/GNES-Flow-a-Pythonic-Way-to-Build-Cloud-Native-Neural-Search-Pipelines/" title="can be found in this post.">can be found in this post.</a> We are now moving forward to the concrete logic behind each component.</p>
<h2><span id="preprocessing-videos">Preprocessing Videos</span></h2><p>In the previous post, I stated that <a href="http://hanxiao.io/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/#gnes-preliminaries-breakdown-of-neural-elastic-and-search">a good neural search is only possible when document and query are comparable semantic units</a>. The preprocessor serves exactly this purpose. It segments a document into a list of semantic units, each of which is called a “chunk” in GNES. For video, a meaningful unary chunk could a <em>frame</em> or a <em>shot</em> (i.e. a series of frames that runs for an uninterrupted period of time). In Tumblr GIF dataset, most of the animations have less than three shots. Thus, I will simply use frame as chunk to represent the document. </p>
<p>GNES itself does not contain such preprocessor (implementing all possible preprocessors/encoders is also not <a href="http://hanxiao.io/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/#model-as-docker-and-docker-as-a-plugin">the design philosophy of GNES</a>), so we need to write our own. Thanks to the well-designed GNES component API, this can be easily done by inheriting from the <code>BaseImagePreprocessor</code> and implement <code>apply()</code>, for example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gnes.component <span class="keyword">import</span> BaseImagePreprocessor</span><br><span class="line"><span class="keyword">from</span> gnes.proto <span class="keyword">import</span> array2blob</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GifPreprocessor</span><span class="params">(BaseImagePreprocessor)</span>:</span></span><br><span class="line">    img_shape = <span class="number">96</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">apply</span><span class="params">(self, doc: <span class="string">'gnes_pb2.Document'</span>)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        super().apply(doc)</span><br><span class="line">        im = Image.open(doc.raw_bytes.decode())</span><br><span class="line">        idx = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> frame <span class="keyword">in</span> get_frames(im):</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                new_frame = frame.convert(<span class="string">'RGB'</span>).resize([img_shape, ] * <span class="number">2</span>)</span><br><span class="line">                img = (np.array(new_frame) / <span class="number">255</span>).astype(np.float32)</span><br><span class="line">                c = doc.chunks.add()</span><br><span class="line">                c.doc_id = doc.doc_id</span><br><span class="line">                c.offset = idx</span><br><span class="line">                c.weight = <span class="number">1.</span></span><br><span class="line">                c.blob.CopyFrom(array2blob(img))</span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> ex:</span><br><span class="line">                self.logger.error(ex)</span><br><span class="line">            <span class="keyword">finally</span>:</span><br><span class="line">                idx = idx + <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>This preprocessor loads the animation, reads its frames into RGB format, resizes each of them to 96x96 and stores in <code>doc.chunks.blob</code> as <code>numpy.ndarray</code>. At the moment we don’t implement any keyframe detection in the preprocessor, so every chunk has a uniform weight, i.e. <code>c.weight=1</code>. </p>
<img src="/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0/tumblr_njqj3bMKQF1unc0x7o1_250.gif.jpg">
<img src="/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0/tumblr_ni35trgNe41tmk5mfo1_400.gif.jpg">
<img src="/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0/tumblr_nb2mucKMeU1tkz79uo1_250.gif.jpg">
<p>One may think of more sophisticated preprocessors. For example, smart sub-sampling to reduce the number of near-duplicated frames; using <a href="http://en.wikipedia.org/wiki/Seam_carving" target="_blank" rel="noopener">seam carving</a> for better cropping and resizing frames; or adding image effects and enhancements. Everything is possible and I will leave these possibilities to the readers.</p>
<h2><span id="encoding-chunks-into-vectors">Encoding Chunks into Vectors</span></h2><p>In the encoding step, we want to represent each chunk by a fixed-length vector. This can be easily done with the <a href="https://www.tensorflow.org/tutorials/images/transfer_learning" target="_blank" rel="noopener">pretrained models in Tensorflow 2.0</a>. For the sake of clarity and simplicity, we will employ <code>MobileNetV2</code> as our encoder. The pretrained weights on ImageNet are downloaded automatically when instantiating the encoder in <code>post_init</code>. The full list of pretrained models can be found <a href="https://keras.io/applications/" target="_blank" rel="noopener">at here</a>. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gnes.component <span class="keyword">import</span> BaseImageEncoder</span><br><span class="line"><span class="keyword">from</span> gnes.helper <span class="keyword">import</span> batching, as_numpy_array</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TF2ImageEncoder</span><span class="params">(BaseImageEncoder)</span>:</span></span><br><span class="line">    batch_size = <span class="number">128</span></span><br><span class="line">    img_shape = <span class="number">96</span></span><br><span class="line">    pooling_strategy = <span class="string">'avg'</span>,</span><br><span class="line">    model_name = <span class="string">'MobileNetV2'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">post_init</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.model = getattr(tf.keras.applications, self.model_name)(</span><br><span class="line">            input_shape=(self.img_shape, self.img_shape, <span class="number">3</span>),</span><br><span class="line">            include_top=<span class="literal">False</span>,</span><br><span class="line">            pooling=self.pool_strategy,</span><br><span class="line">            weights=<span class="string">'imagenet'</span>)</span><br><span class="line">        self.model.trainable = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @batching</span></span><br><span class="line"><span class="meta">    @as_numpy_array</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, img: List[<span class="string">'np.ndarray'</span>], *args, **kwargs)</span> -&gt; np.ndarray:</span></span><br><span class="line">        img = np.stack(img, axis=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> self.model(img)</span><br></pre></td></tr></table></figure>
<p>Code should be fairly straightforward. I create a new encoder class by inherit from <code>BaseImageEncoder</code>, in which the most important function <code>encode()</code> is simply calling the model to extract features. The <code>batching</code> decorator is a very handy helper to control the size of the data flowing into the encoder. After all, OOM error is the last thing you want to see. </p>
<h2><span id="indexing-chunks-and-documents">Indexing Chunks and Documents</span></h2><p>For indexing, I will use the built-in chunk indexers and document indexers of GNES. Chunk indexing is essentially vector indexing, we need to store a map of chunk ids and their corresponding vector representations. As GNES supports <a href="https://github.com/facebookresearch/faiss/" target="_blank" rel="noopener">Faiss</a> indexer already, you don’t need to write Python code anymore. Simply write a YAML config <code>vec.yml</code> as follows:<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">!FaissIndexer</span></span><br><span class="line"><span class="attr">parameters:</span></span><br><span class="line">  <span class="attr">num_dim:</span> <span class="number">-1</span>  <span class="comment"># automatically determined</span></span><br><span class="line">  <span class="attr">index_key:</span> <span class="string">HNSW32</span></span><br><span class="line">  <span class="attr">data_path:</span> <span class="string">$WORKDIR/idx.binary</span></span><br><span class="line"><span class="attr">gnes_config:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my_vec_indexer</span>  <span class="comment"># a customized name</span></span><br><span class="line">  <span class="attr">work_dir:</span> <span class="string">$WORKDIR</span></span><br></pre></td></tr></table></figure></p>
<p>As eventually in the query time, we are interested in documents not chunks, hence the map of doc id and chunk ids should be also stored. This is essentially a key-value database, and a simple Python <code>Dict</code> structure will do the job. Again, only a YAML config <code>doc.yml</code> is required:</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">!DictIndexer</span></span><br><span class="line"><span class="attr">gnes_config:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my_doc_indexer</span>  <span class="comment"># a customized name</span></span><br><span class="line">  <span class="attr">work_dir:</span> <span class="string">$WORKDIR</span></span><br></pre></td></tr></table></figure>
<p>Note that the doc indexer does not require the encoding step, thus it can be done in parallel with the chunk indexer. Notice how <code>chunk_proc</code> is broadcasting its output to the encoder and doc indexer, and how a sync barrier is placed afterwards to ensure all jobs are completed.</p>
<img src="/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0/29b84023.png">
<h2><span id="scoring-results">Scoring Results</span></h2><p>Scoring is <strong>important but hard</strong>, it often requires domain-specific expertise and many iterations. You can simply take the average all chunk scores as the document score, or you can weight chunks differently and combine them with some heuristics. In the current GNES, scorer or ranker can be implemented by inheriting from <code>BaseReduceRouter</code> and overriding its <code>apply</code> method. </p>
<p>When designing your own score function, make sure to use the existing ones from <code>gnes.score_fn.base</code> as your basic building blocks. Stacking and combining these score functions can create a complicated yet explainable score function, greatly reducing the effort when debugging. Besides, all score functions from <code>gnes.score_fn.base</code> are trainable (via <code>.train()</code> method), enabling advanced scoring techniques such as learning to rank. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ScoreOps</span>:</span></span><br><span class="line">    multiply = CombinedScoreFn(<span class="string">'multiply'</span>)</span><br><span class="line">    sum = CombinedScoreFn(<span class="string">'sum'</span>)</span><br><span class="line">    max = CombinedScoreFn(<span class="string">'max'</span>)</span><br><span class="line">    min = CombinedScoreFn(<span class="string">'min'</span>)</span><br><span class="line">    avg = CombinedScoreFn(<span class="string">'avg'</span>)</span><br><span class="line">    none = ModifierScoreFn(<span class="string">'none'</span>)</span><br><span class="line">    log = ModifierScoreFn(<span class="string">'log'</span>)</span><br><span class="line">    log1p = ModifierScoreFn(<span class="string">'log1p'</span>)</span><br><span class="line">    log2p = ModifierScoreFn(<span class="string">'log2p'</span>)</span><br><span class="line">    ln = ModifierScoreFn(<span class="string">'ln'</span>)</span><br><span class="line">    ln1p = ModifierScoreFn(<span class="string">'ln1p'</span>)</span><br><span class="line">    ln2p = ModifierScoreFn(<span class="string">'ln2p'</span>)</span><br><span class="line">    square = ModifierScoreFn(<span class="string">'square'</span>)</span><br><span class="line">    sqrt = ModifierScoreFn(<span class="string">'sqrt'</span>)</span><br><span class="line">    abs = ModifierScoreFn(<span class="string">'abs'</span>)</span><br><span class="line">    reciprocal = ModifierScoreFn(<span class="string">'reciprocal'</span>)</span><br><span class="line">    reciprocal1p = ModifierScoreFn(<span class="string">'reciprocal1p'</span>)</span><br><span class="line">    const = ConstScoreFn()</span><br></pre></td></tr></table></figure>
<video width="90%" controls muted loop poster="60aaf75c.png"><br>  <source src="gnes-video-search-explain-score.mp4" type="video/mp4"><br>Your browser does not support the HTML5 video tag. :(<br></video>


<h2><span id="putting-it-all-together">Putting it All Together</span></h2><p>With all the YAML config and Python module we just made, we can import them to the flow by specifying <code>py_path</code> and <code>yaml_path</code> in the flow. Besides scale out the preprocessor and encoder to <code>4</code>, I also make a small tweak in the flow: I added a thumbnail preprocessor <code>thumbnail_proc</code> to store all extracted frames in a row as a JPEG file.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">replicas = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">index_flow = (Flow()</span><br><span class="line">    .add_preprocessor(name=<span class="string">'chunk_proc'</span>, yaml_path=<span class="string">'gif2chunk.yml'</span>,</span><br><span class="line">                   py_path=[<span class="string">'gif_reader.py'</span>, <span class="string">'gif2chunk.py'</span>],</span><br><span class="line">                   replicas=replicas)</span><br><span class="line">    .add_preprocessor(name=<span class="string">'thumbnail_proc'</span>, yaml_path=<span class="string">'chunks2jpg.yml'</span>, py_path=<span class="string">'chunks2jpg.py'</span>, replicas=replicas)</span><br><span class="line">    .add_indexer(name=<span class="string">'doc_idx'</span>, yaml_path=<span class="string">'doc.yml'</span>)</span><br><span class="line">    .add_encoder(yaml_path=<span class="string">'encode.yml'</span>, py_path=<span class="string">'encode.py'</span>,</span><br><span class="line">              replicas=replicas, recv_from=<span class="string">'chunk_proc'</span>)</span><br><span class="line">    .add_indexer(name=<span class="string">'vec_idx'</span>, yaml_path=<span class="string">'vec.yml'</span>)</span><br><span class="line">    .add_router(name=<span class="string">'sync_barrier'</span>, yaml_path=<span class="string">'BaseReduceRouter'</span>,</span><br><span class="line">             num_part=<span class="number">2</span>, recv_from=[<span class="string">'vec_idx'</span>, <span class="string">'doc_idx'</span>]))</span><br><span class="line"></span><br><span class="line">query_flow = (Flow()</span><br><span class="line">     .add_preprocessor(name=<span class="string">'chunk_proc'</span>, yaml_path=<span class="string">'gif2chunk.yml'</span>,</span><br><span class="line">                        py_path=[<span class="string">'gif_reader.py'</span>, <span class="string">'gif2chunk.py'</span>],</span><br><span class="line">                        replicas=replicas)</span><br><span class="line">     .add_preprocessor(name=<span class="string">'thumbnail_proc'</span>, yaml_path=<span class="string">'chunks2jpg.yml'</span>, py_path=<span class="string">'chunks2jpg.py'</span>, replicas=replicas)</span><br><span class="line">     .add_encoder(yaml_path=<span class="string">'encode.yml'</span>, py_path=<span class="string">'encode.py'</span>, replicas=replicas, recv_from=<span class="string">'chunk_proc'</span>)</span><br><span class="line">     .add_indexer(name=<span class="string">'vec_idx'</span>, yaml_path=<span class="string">'vec.yml'</span>)</span><br><span class="line">     .add_router(name=<span class="string">'scorer'</span>, yaml_path=<span class="string">'score.yml'</span>, py_path=<span class="string">'videoscorer.py'</span>)</span><br><span class="line">     .add_indexer(name=<span class="string">'doc_idx'</span>, yaml_path=<span class="string">'doc.yml'</span>, sorted_response=<span class="string">'descend'</span>)</span><br><span class="line">     .add_router(name=<span class="string">'sync_barrier'</span>, yaml_path=<span class="string">'BaseReduceRouter'</span>,</span><br><span class="line">            num_part=<span class="number">2</span>, recv_from=[<span class="string">'thumbnail_proc'</span>, <span class="string">'doc_idx'</span>]))</span><br></pre></td></tr></table></figure>
<p>Visualizing these two flows give:</p>
<blockquote>
<p>Index flow<br><img src="/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0/2638c7e5.png"></p>
</blockquote>
<blockquote>
<p>Query flow<br><img src="/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0/cf262374.png"></p>
</blockquote>
<h4><span id="what-should-we-sendreceive">What Should We Send/Receive?</span></h4><p>Sending data to the flow is easy, simply build a <code>Iterator[bytes]</code> and feed to <code>flow.index()</code>. The example below get the absolute paths of all animation files and send those <em>paths</em> to the flow:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">bytes_gen = (g.encode() <span class="keyword">for</span> g <span class="keyword">in</span> glob.glob(<span class="string">'dataset/*.gif'</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> index_flow.build(backend=<span class="string">'process'</span>) <span class="keyword">as</span> fl:</span><br><span class="line">    fl.index(bytes_gen, batch_size=<span class="number">64</span>)</span><br></pre></td></tr></table></figure>
<p>Of course one can first <code>read()</code> the animation into the memory, and send binary animation directly to the flow. But that will give very poor efficiency. We do not want IO ops to be the bottleneck, and that’s why we spawn four preprocessors in the flow.</p>
<p>The indexing procedure is pretty fast. On my i7-8850H desktop with no GPU, indexing the full dataset (~100K videos) takes 4 hours. Things can be much faster if you have a powerful GPU.</p>
<p>Once the flow is indexed, we can throw a video query in it and retrieve relevant videos. To do that, we randomly sample some videos as queries:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bytes_gen = (g.encode() <span class="keyword">for</span> g <span class="keyword">in</span> random.sample(glob.glob(GIF_BLOB), num_docs))</span><br><span class="line"><span class="keyword">with</span> query_flow.build(backend=<span class="string">'process'</span>) <span class="keyword">as</span> fl:</span><br><span class="line">    fl.query(bytes_gen, callback=dump_result_to_json, top_k=<span class="number">60</span>, batch_size=<span class="number">32</span>)</span><br></pre></td></tr></table></figure>
<p>Note that <code>callback=dump_result_to_json</code> in the code. Every time a search result is returned, this callback function will be invoked. In this example, I simply dump the search result into the JSON format so that I can later visualize it in the web frontend.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">fp = open(<span class="string">'/topk.json'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf8'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dump_result_to_json</span><span class="params">(resp)</span>:</span></span><br><span class="line">    resp = remove_envelope(resp)</span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> resp.search.results:</span><br><span class="line">        v = MessageToDict(r, including_default_value_fields=<span class="literal">True</span>)</span><br><span class="line">        v[<span class="string">'doc'</span>][<span class="string">'rawBytes'</span>] = r.doc.raw_bytes.decode()</span><br><span class="line">        <span class="keyword">for</span> k, kk <span class="keyword">in</span> zip(v[<span class="string">'topkResults'</span>], r.topk_results):</span><br><span class="line">            k[<span class="string">'doc'</span>][<span class="string">'rawBytes'</span>] = kk.doc.raw_bytes.decode()</span><br><span class="line">            k[<span class="string">'score'</span>][<span class="string">'explained'</span>] = json.loads(kk.score.explained)</span><br><span class="line">        fp.write(json.dumps(v, sort_keys=<span class="literal">True</span>) + <span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure>
<h2><span id="summary">Summary</span></h2><p>Video semantic search is not only fun (seriously I have spent even more time on watching cat videos after building this system), but has many usages in the customer facing applications, e.g. short videos apps, movie/film editors. Though it is too early to say GNES is the defacto solution to video semantic search, I hope this article sends a good signal: GNES is <em>much more beyond</em> <code>bert-as-service</code>, and it enables the search of almost any content form including text, image, video and audio.</p>
<p>In the second part, I will use the token-based similarity of textual descriptions (e.g. Rouge-L) as the groundtruth to evaluate our video search system. I also plan to benchmark different pretrained models, preprocessors, indexers and their combinations. If you are interested in reading more on this thread or knowing more about my plan on GNES, stay tuned.</p>
</div></article></div></main><footer><div class="paginator"><a href="/2019/11/07/A-Better-Practice-for-Managing-extras-require-Dependencies-in-Python/" class="next">A Better Practice fo...&nbsp;&nbsp;❯&nbsp;</a></div><div class="footer-section"><h2><img src="/flower.png" alt="Checkout this">Check out these posts too!</h2><div class="archive-readmore"><div style="display: flex; gap: 10px; align-items: flex-start;" class="post-item"><a href="/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0/"><img src="https://hanxiao.io/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0//9ba076d0.png" alt="Video Semantic Search in Large Scale using GNES and Tensorflow 2.0" style="width: 120px; height: 90px; object-fit: cover;"></a><div style="flex: 1;" class="book-title"><h5 style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; margin: 0;" class="post-title"><a href="/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0/" class="post-title-link">Video Semantic Search in Large Scale using GNES and Tensorflow 2.0</a></h5></div></div><div style="display: flex; gap: 10px; align-items: flex-start;" class="post-item"><a href="/2019/11/07/A-Better-Practice-for-Managing-extras-require-Dependencies-in-Python/"><img src="https://hanxiao.io/2019/11/07/A-Better-Practice-for-Managing-extras-require-Dependencies-in-Python//banner.png" alt="A Better Practice for Managing Many &lt;code&gt;extras_require&lt;/code&gt; Dependencies in Python" style="width: 120px; height: 90px; object-fit: cover;"></a><div style="flex: 1;" class="book-title"><h5 style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; margin: 0;" class="post-title"><a href="/2019/11/07/A-Better-Practice-for-Managing-extras-require-Dependencies-in-Python/" class="post-title-link">A Better Practice for Managing Many <code>extras_require</code> Dependencies in Python</a></h5></div></div><div style="display: flex; gap: 10px; align-items: flex-start;" class="post-item"><a href="/2019/10/18/GNES-Flow-a-Pythonic-Way-to-Build-Cloud-Native-Neural-Search-Pipelines/"><img src="https://hanxiao.io/2019/10/18/GNES-Flow-a-Pythonic-Way-to-Build-Cloud-Native-Neural-Search-Pipelines//gnes-flow-banner.png" alt="GNES Flow: a Pythonic Way to Build Cloud-Native Neural Search Pipelines" style="width: 120px; height: 90px; object-fit: cover;"></a><div style="flex: 1;" class="book-title"><h5 style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; margin: 0;" class="post-title"><a href="/2019/10/18/GNES-Flow-a-Pythonic-Way-to-Build-Cloud-Native-Neural-Search-Pipelines/" class="post-title-link">GNES Flow: a Pythonic Way to Build Cloud-Native Neural Search Pipelines</a></h5></div></div><div style="display: flex; gap: 10px; align-items: flex-start;" class="post-item"><a href="/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/"><img src="https://hanxiao.io/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond//gnes-team-1600.JPG" alt="Generic Neural Elastic Search: From &lt;code&gt;bert-as-service&lt;/code&gt; and Go Way Beyond" style="width: 120px; height: 90px; object-fit: cover;"></a><div style="flex: 1;" class="book-title"><h5 style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; margin: 0;" class="post-title"><a href="/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/" class="post-title-link">Generic Neural Elastic Search: From <code>bert-as-service</code> and Go Way Beyond</a></h5></div></div></div></div><div class="copyright"><p>© 2017 - 2025 <a href="https://hanxiao.io">Han Xiao</a>. <img src="/by-nc-sa.svg" alt="Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License." class="image"></p></div></footer></div><link rel="stylesheet" href="/css/post/katex.min.css"><!--link(rel="stylesheet", href=url_for("css/post/gitment.css"))--><script src="/js/katex.min.js"></script><script src="/js/auto-render.min.js"></script><script>renderMathInElement(
    document.body,
    {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "\\[", right: "\\]", display: true},
            {left: "$", right: "$", display: false},
            {left: "\\(", right: "\\)", display: false}
        ]
    }
);</script><!--script(src=url_for("js/gitment.browser.js"))--><!--script(src=url_for("js/gitment.loader.js"))--><script src="/js/jquery-3.4.1.min.js"></script><script src="/js/reading_progress.min.js"></script><script src="/js/highlighter.min.js"></script><script src="/js/init-highlighter.js"></script><script async src="https://www.google-analytics.com/analytics.js"></script><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create',"UA-52114253-1",'auto');ga('send','pageview');</script></body></html>