<!DOCTYPE html><html lang="en"><style>html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}article,header,main{display:block}a{background-color:transparent}h1{font-size:2em;margin:.67em 0}img{border:0}body,html{width:100%;height:100%}html{width:100%;height:100vh;display:flex;flex-direction:column;justify-content:center;align-items:center;background:var(--bgColor);--bgColor:#fff;--textColor:#2c3e50;--bg2ndColor:none;--bg3rdColor:#f8f8f8;--preCodeColor:#525252;--imgOpacity:1.0}@media (prefers-color-scheme:dark){html{background:var(--bgColor);--bgColor:#121212;--textColor:#fff;--bg2ndColor:#fff;--bg3rdColor:#332940;--preCodeColor:#f8f8f8;--imgOpacity:0.5}}body{margin:0;color:var(--textColor);font-size:18px;line-height:1.6;background-color:var(--bgColor);font-family:sourcesanspro,'Helvetica Neue',Arial,sans-serif}ul.nav{margin:0;padding:0;list-style-type:none}ul{margin:1rem 0}a{color:var(--textColor);text-decoration:none}.flag-icon{height:25px;width:25px;display:inline;border-radius:50%;vertical-align:sub}.icon_item{padding-left:5px!important;padding-right:5px!important}.reading-progress-bar{background:#42b983;display:block;height:2px;left:0;position:fixed;top:0;width:0;z-index:10001}header{min-height:60px}header .logo-link{float:left}header .nav{float:right;left:80px}header .logo-link img{height:60px}header .nav-list-item{display:inline-block;padding:19px 10px}header .nav-list-item a{line-height:1.4}@media screen and (max-width:900px){header .nav-list-item a{font-size:12px}}@media screen and (min-width:900px){header .nav-list-item a{font-size:18px}}.post{padding-top:1em}.post-block .post-title{margin:.65em 0;color:var(--textColor);font-size:1.5em}.post-block .post-info{color:#7f8c8d}.post-block .post-info .read-time{text-align:right}.post-content h2,.post-content h4{position:relative;margin:1em 0}.post-content h2 :before,.post-content h4 :before{content:"#";color:#42b983;position:absolute;left:-.7em;top:-4px;font-size:1.2em;font-weight:700}.post-content h4 :before{content:">"}.post-content h2{font-size:22px}.post-content h4{font-size:18px}.post-content a{color:#42b983;word-break:break-all}main.container{margin:2em 10px}@media screen and (min-width:900px){.wrap{width:900px;margin:0 auto}header{padding:20px 60px}}@media screen and (max-width:900px){.wrap{width:100%}header{min-height:50px;padding:2px 2px;position:fixed;z-index:10000;border-radius:15px;left:50%;-webkit-transform:translateX(-50%);transform:translateX(-50%);width:-webkit-fit-content;width:-moz-fit-content;width:fit-content}header a.logo-link,header ul.nav.nav-list{float:none;display:inline;text-align:center}header li.nav-list-item{padding:10px 5px}header .logo-link img{height:20px;vertical-align:sub}header .flag-icon{height:20px;width:20px}header{background-color:rgba(255,255,255,.9)}@supports ((-webkit-backdrop-filter:blur(2em)) or (backdrop-filter:blur(2em))){header{background-color:rgba(255,255,255,.3);-webkit-backdrop-filter:blur(10px);backdrop-filter:blur(10px)}}main.container{padding-top:2em}main.container{margin:0 20px}.post-content h2,.post-content h4{max-width:300px;left:15px}}@font-face{font-family:sourcesanspro;src:url(/font/sourcesanspro.woff2) format("woff2"),url(/font/sourcesanspro.woff) format("woff");font-weight:400;font-style:normal}</style><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Generic Neural Elastic Search: From bert-as-service and Go Way Beyond · Han Xiao Tech Blog - Neural Search & AI Engineering</title><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content="@hxiao"><meta name="twitter:creator" content="@hxiao"><meta name="description" content="Since Jan. 2019, I have started leading a team at Tencent AI Lab and working on a new system GNES (Generic Neural Elastic Search). GNES is an open-sou ... · Han Xiao"><meta property="og:title" content="Generic Neural Elastic Search: From bert-as-service and Go Way Beyond · Han Xiao Tech Blog - Neural Search &amp; AI Engineering"><meta property="og:description" content="Since Jan. 2019, I have started leading a team at Tencent AI Lab and working on a new system GNES (Generic Neural Elastic Search). GNES is an open-sou ... · Han Xiao"><meta property="og:url" content="https://hanxiao.io/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/"><meta property="og:image" content="https://hanxiao.io/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond//gnes-team-1600.JPG"><meta property="og:type" content="article"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/myavatar.png"><link rel="alternate" type="application/rss+xml" title="Han Xiao Tech Blog - Neural Search &amp; AI Engineering" href="https://hanxiao.io/atom.xml"><!-- - use css preload trick--><link rel="preload" href="/css/apollo.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="/css/apollo.css"></noscript><script>/*! loadCSS. [c]2017 Filament Group, Inc. MIT License */
/* This file is meant as a standalone workflow for
- testing support for link[rel=preload]
- enabling async CSS loading in browsers that do not support rel=preload
- applying rel preload css once loaded, whether supported or not.
*/
(function( w ){
    "use strict";
    // rel=preload support test
    if( !w.loadCSS ){
        w.loadCSS = function(){};
    }
    // define on the loadCSS obj
    var rp = loadCSS.relpreload = {};
    // rel=preload feature support test
    // runs once and returns a function for compat purposes
    rp.support = (function(){
        var ret;
        try {
            ret = w.document.createElement( "link" ).relList.supports( "preload" );
        } catch (e) {
            ret = false;
        }
        return function(){
            return ret;
        };
    })();

    // if preload isn't supported, get an asynchronous load by using a non-matching media attribute
    // then change that media back to its intended value on load
    rp.bindMediaToggle = function( link ){
        // remember existing media attr for ultimate state, or default to 'all'
        var finalMedia = link.media || "all";

        function enableStylesheet(){
            link.media = finalMedia;
        }

        // bind load handlers to enable media
        if( link.addEventListener ){
            link.addEventListener( "load", enableStylesheet );
        } else if( link.attachEvent ){
            link.attachEvent( "onload", enableStylesheet );
        }

        // Set rel and non-applicable media type to start an async request
        // note: timeout allows this to happen async to let rendering continue in IE
        setTimeout(function(){
            link.rel = "stylesheet";
            link.media = "only x";
        });
        // also enable media after 3 seconds,
        // which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
        setTimeout( enableStylesheet, 3000 );
    };

    // loop through link elements in DOM
    rp.poly = function(){
        // double check this to prevent external calls from running
        if( rp.support() ){
            return;
        }
        var links = w.document.getElementsByTagName( "link" );
        for( var i = 0; i < links.length; i++ ){
            var link = links[ i ];
            // qualify links to those with rel=preload and as=style attrs
            if( link.rel === "preload" && link.getAttribute( "as" ) === "style" && !link.getAttribute( "data-loadcss" ) ){
                // prevent rerunning on link
                link.setAttribute( "data-loadcss", true );
                // bind listeners to toggle media back
                rp.bindMediaToggle( link );
            }
        }
    };

    // if unsupported, run the polyfill
    if( !rp.support() ){
        // run once at least
        rp.poly();

        // rerun poly on an interval until onload
        var run = w.setInterval( rp.poly, 500 );
        if( w.addEventListener ){
            w.addEventListener( "load", function(){
                rp.poly();
                w.clearInterval( run );
            } );
        } else if( w.attachEvent ){
            w.attachEvent( "onload", function(){
                rp.poly();
                w.clearInterval( run );
            } );
        }
    }


    // commonjs
    if( typeof exports !== "undefined" ){
        exports.loadCSS = loadCSS;
    }
    else {
        w.loadCSS = loadCSS;
    }
}( typeof global !== "undefined" ? global : this ) );</script><script id="mcjs">!function(c,h,i,m,p){m=c.createElement(h),p=c.getElementsByTagName(h)[0],m.async=1,m.src=i,p.parentNode.insertBefore(m,p)}(document,"script","https://chimpstatic.com/mcjs-connected/js/users/7da58fc9885cb85d4a9f0ad9a/987f901145f1749fd3e800e86.js");</script><link rel="search" type="application/opensearchdescription+xml" href="https://hanxiao.io/atom.xml" title="Han Xiao Tech Blog - Neural Search &amp; AI Engineering"></head><body><div class="reading-progress-bar"></div><div class="wrap"><header><ul class="nav nav-list"><li class="nav-list-item"><a href="/about/" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="https://www.linkedin.com/in/hxiao87" target="_blank" class="nav-list-link">LINKEDIN</a></li><li class="nav-list-item"><a href="https://x.com/hxiao" target="_blank" class="nav-list-link">X</a></li><li class="nav-list-item"><a href="https://github.com/hanxiao" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="https://scholar.google.com/citations?user=jp7swwIAAAAJ" target="_blank" class="nav-list-link">SCHOLAR</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">Generic Neural Elastic Search: From <code>bert-as-service</code> and Go Way Beyond</h1><div class="post-info">Jul 29, 2019 by &nbsp;&nbsp;&nbsp;<img src="/myavatar.png" alt="logo" width="18px" height="18px" style="vertical-align: sub;">&nbsp;<a href="/about" target="_blank" class="nav-list-link">Han Xiao - <i>ex</i> Engineering Lead @ Tencent AI Lab</a></div><div class="post-info"><div class="read-time">◷&nbsp;&nbsp;&nbsp; 35  min read</div></div><div class="post-content"><h2><span id="background">Background</span></h2><p>Since Jan. 2019, I have started leading a team at Tencent AI Lab and working on a new system <strong>GNES (Generic Neural Elastic Search)</strong>. GNES is an open-source cloud-native semantic search solution based on deep neural network. <a id="more"></a>It enables large-scale index and semantic search of <strong>text-to-text</strong>, <strong>image-to-image</strong>, <strong>video-to-video</strong>, and any-to-any content form. For readers who don’t know about this project, I strongly recommend you to <a href="https://github.com/gnes-ai/gnes" target="_blank" rel="noopener">check it out on Github</a>. </p>
<p><a href="https://github.com/gnes-ai/gnes" target="_blank" rel="noopener"><img src="/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/gnes-github-banner.png"></a></p>
<p>Unlike many AI/ML open-source projects, GNES is neither an implementation of a specific algorithm nor a collection of models. GNES has a very ambitious goal of being the next generation search engine: it aims at <em>the generality</em>, enabling the search of almost any content form (e.g. text, image, and video); it aims at <em>AI-in-production</em>, respecting the best engineering practices and being optimized for the cloud user experience. </p>
<p>In this post, I will talk about the key design principles behind GNES, explain to you why we develop this framework <em>in this way</em> and <em>at this time</em>. I would also like to share our understanding of a generic information retrieval system to the AI/ML/NLP/CV community, and how it might differ from the conventional search system. Right now GNES is still under heavy developments and fast iterations, it will keep in this way for the next months. I hope this post could draw some attention from the community, encouraging <strong>more AI researchers and engineers to join us and make GNES even better.</strong> </p>
<h4><span id="table-of-content">Table of Content</span></h4><!-- toc -->
<ul>
<li><a href="#foreword-few-thoughts-on-ai-innovation-and-engineering">Foreword: Few Thoughts on AI Innovation and Engineering</a></li>
<li><a href="#gnes-preliminaries-breakdown-of-neural-elastic-and-search">GNES Preliminaries: Breakdown of Neural, Elastic and Search</a></li>
<li><a href="#gnes-design-philosophy">GNES Design Philosophy</a><ul>
<li><a href="#a-cloud-native-system">A Cloud-Native System</a></li>
<li><a href="#model-as-docker-and-docker-as-a-plugin">Model as Docker, and Docker as a Plugin</a></li>
<li><a href="#generic-and-universal">Generic and Universal</a></li>
</ul>
</li>
<li><a href="#gnes-as-a-team">GNES as a Team</a><ul>
<li><a href="#making-no-tech-debt">Making No Tech Debt</a></li>
<li><a href="#having-a-b2d-thoughtship">Having a B2D Thoughtship</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->
<h2><span id="foreword-few-thoughts-on-ai-innovation-and-engineering">Foreword: Few Thoughts on AI Innovation and Engineering</span></h2><p>In this section, I want to pass along a few thoughts about AI innovation, engineering and open-source. They seem to be irrelevant to our theme, but they implicitly affect how GNES is designed and positioned in the market.</p>
<div class="remind"><br>  I have talked about most of the content in this section publicly in <a href="https://aaic2019.b2match.io/" target="_blank" rel="noopener">AAIC 2019</a>, <a href="https://gcaai.org" target="_blank" rel="noopener">the 2nd annual meeting of GCAAI</a>, <a href="https://www.youtube.com/watch?v=mfvvWlj5uwA" target="_blank" rel="noopener">the New Stack podcast</a> and internally at Tencent. If you were the audience of these events, feel free to skip this section.<br>  <ul><br>  <li>There are two types of innovation on AI: from 0 to 1 and from 1 to N. In the deep learning era, people are moving away from the zero-to-one and join the one-to-N innovation.</li><br>  <li>Forget about training! It is not (economically) feasible anymore.</li><br>  <li>An effective way of contributing to today’s AI community is focusing on an end-to-end application.</li><br>  </ul><br><br>  You can also fast forward to <a href="#gnes-preliminaries:-breakdown-of-neural,-elastic-and-search">the next section</a> if the ideas above are boring to you.<br></div>


<blockquote><p><strong> 1. There are two types of innovation on AI: from 0 to 1 and from 1 to N. In the deep learning era, people are moving away from the zero-to-one and join the one-to-N innovation. </strong></p>
</blockquote>
<p>I always see the progress in AI from two aspects: </p>
<ul>
<li><strong>From 0 to 1.</strong> Zero-to-one is all about hard-core and fundamental research, e.g. new learning and training paradigms, new computing infrastructure and new hardware architecture.</li>
<li><strong>From 1 to N.</strong> One-to-N focuses more on usability and engineering. Problems such as adapting an algorithm from one domain to multiple domains; serving a model to billions of users; improving existing user experience with AI algorithms; piping a set of algorithms to automize the workflow; all of them belong to the one-to-N innovation.</li>
</ul>
<p>Before 2012 (I call it the <em>pre deep learning era</em>), zero-to-one innovation was quite popular. People debated the best way to utilize data for problem-solving. Decision forests, margin-based approaches, kernel methods, graphical models, parametric and non-parametric Bayesian models, neural network; each data-driven method is backed by some supporters. They keep iterating and improving the methodology until it outperforms the peers in the benchmark. Deep neural network was also in this party, though not the buzziest at that time, it was showing off quite a bit.</p>
<img src="/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/6d1778ad.png">
<p>Things have changed since 2013. As deep neural networks have been recognized by more and more researchers and engineers, people have realized that deep learning is not just a hype, it can solve many complicated problems with a much higher accuracy comparing to the traditional “shallow learning”. As we are standing now in 2019, one can argue that deep learning is the <em>defacto</em> solution of many AI applications, e.g. image classification, machine translation, speech recognition. </p>
<p>Having agreed on the methodology, many moves away from the zero-to-one and join the party of one-to-N innovation (including myself). Nowadays, deep neural networks such as CNN and RNN have been widely used as the backbone of many AI-powered features in everyday-product, such as facial payment, voice assistants, automatic customer service and K12 education.</p>
<div class="remind"><br><strong>Impact on GNES</strong>: I consider GNES as a one-to-N type of contribution to the community. It focuses on AI-in-production and puts reusability, flexibility, and scalability as the priority.<br></div>

<blockquote><p><strong> 2. Forget about training! It is not (economically) feasible anymore. </strong></p>
</blockquote>
<p>The idea of forget about training could be pretty radical to many machine learning engineers. A model without training, do you mean throwing a randomly initialized model into production? Of course not. To be fair, I am talking about <strong>forgetting about training from scratch</strong> (but I intentionally remove the last part of the sentence to make the statement strong).</p>
<p>To elaborate, there are four typical steps in the traditional machine learning workflow: preprocessing, training, evaluating and serving. They are usually accomplished by one (e.g. a DevOps team) or two teams (e.g. data scientists + software engineers) in the company. However, the increasing training cost of a good model has changed this organization structure, producing a gap between training and serving.</p>
<p>To see that more clearly, we can compare the training cost of AlexNet (the first popular DNN based image classification algorithm in 2012) to AlphaZero (a DNN reinforcement learning-based Go-player in 2018). According to <a href="https://openai.com/blog/ai-and-compute/" target="_blank" rel="noopener">this blog post from OpenAI</a>, the training cost has increased from 0.0058 petaflop/s-day to 1,700 petaflop/s-day, that is 300,000x in only six years! Meanwhile, the training cost of Google BERT (a bidirectional transformer model that redefined the state of the art for 11 natural language processing tasks) <a href="https://syncedreview.com/2019/06/27/the-staggering-cost-of-training-sota-ai-models/" target="_blank" rel="noopener">is $6,912</a>; and GPT-2 (a large language model recently developed by OpenAI which can generate realistic paragraphs of text) <a href="https://syncedreview.com/2019/06/27/the-staggering-cost-of-training-sota-ai-models/" target="_blank" rel="noopener">takes $256 per hour for training</a>. And that does not count in bad initializations or data errors, which often require a re-training. </p>
<img src="/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/23536876.png">
<p>For small companies and startups who focus on AI applications, these numbers could be quite scary, not to mention the extra time spent on trial-and-error and the money spent on retaining the best AI talents. But even in the big tech giants, it is not economically feasible for everyone, only a small number of teams can afford to train such high-profile models from scratch. Sometimes the teams don’t have the scalable infrastructure, other times it is considered risky from a project management perspective.</p>
<p>Apart from the economic reason, from the machine learning perspective, it is also <em>counter-intuitive</em> to train models repetitively from scratch on every new task, especially when these tasks share the same low-level information. For example, in NLP tasks such as news classification and sentiment analysis, the low-level knowledge of the language (e.g. grammar, the meaning of individual words) remains constant. In CV tasks such as object recognition and autonomous driving, there is no need to re-learn common concepts such as colors, textures, and reflections from task to task.</p>
<img src="/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/3d69a409.png">
<p>That being said, it should be easy to understand the emerging <strong>AI supply-chain</strong>: only a very small number of people in the world will work on training new models from scratch, they deliver a “pretrained model” to the rest of the AI community. Most of AI engineers will then try to make sense of this pretrained model by adapting it to their applications, fine-tuning based on their data, incorporating domain knowledge to it and scaling it to serve millions of customers. The image above illustrates this new AI supply-chain.</p>
<div class="remind"><br><strong>Impact on GNES</strong>: The architecture behind GNES is optimized for inference, namely how to effectively deliver prediction results to users/developers. In GNES, we have a design philosophy of “<em>model as a docker, docker as a plugin</em>“. That is, every pretrained model is wrapped in a docker container, and this container is used as a plugin to GNES framework. Although GNES supports partial training/fine-tuning, it is not the main focus for now.<br></div>


<blockquote><p><strong> 3. An effective way of contributing to today’s AI community is focusing on an end-to-end application. </strong></p>
</blockquote>
<p>If you feel a bit upset after reading through the first two points, believing there is nothing you can contribute to the AI community unless you are in big tech giants or sufficient economic incentive exists; <strong>then you shouldn’t</strong>. Let’s look at the pyramid below which partitions the AI development landscape into three layers: </p>
<img src="/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/cc85214f.png">
<p>On the bottom, we have computing infrastructures, including CPU, GPU, FPGA, edge device, cloud computing, etc. There are a lot of progress and opportunities in this layer, especially as AI algorithms now deploying to everywhere (e.g. on your phone, in your fridge) the need for computing everywhere is quite demanding. However, advancing the computing infrastructure is not easy, both technical-wise and capital-wise it has a very high bar for just entering the competition. For startups, individuals or AI enthusiasts, I don’t think this is a realistic way to contribute. </p>
<p>The middle layer is AI frameworks. For most of AI developers/engineers, frameworks are their primary interface to the low-level hardware. Needless to say, there is huge market value in the framework layer: whoever dominates the framework market can reshape the AI dev-community and possibly affect the AI R&amp;D direction in general. As we are standing now in 2019, <a href="https://towardsdatascience.com/which-deep-learning-framework-is-growing-fastest-3f77f14aa318" target="_blank" rel="noopener">one has to admit that Tensorflow (including Keras) and Pytorch dominate the framework market</a>. As a comparison, it was a completely different landscape four years ago in 2015. If you could ask the question “what do you use for deep learning?” back in time, you would get very diverse answers such as Matlab, Mathematica, Theano, Caffe, Deeplearning4j, OpenNN, which now play very minor roles in today’s AI development. Though we still see new framework emerging from time to time, my understanding is that outperforming Tensorflow and Pytorch to grab a share of this red ocean market is extremely challenging.</p>
<p>Finally, the top layer is the vast end-to-end AI applications. There we have <strong>a lot of opportunities to make significant contributions</strong>, by leveraging the mature deep learning frameworks and off-the-shelf pretrained models. Asking your colleagues or friends the question “which AI package do you use for facial recognition/machine translation/image enhancement?”, you would most likely get “I’m not sure” or different answers from person to person. This is often an indicator that the market is still uncontested, and there is ample opportunity for growth and building a community around it. </p>
<div class="remind"><br><strong>Impact on GNES</strong>: GNES is positioned in the intersection area between the framework layer and end-to-end application layer, as illustrated in the last figure. On the one hand, it delivers an end-to-end search solution to users. On the other hand, it is also an extremely flexible and elastic framework that allows developers to implement more features on top of it.<br></div>


<h2><span id="gnes-preliminaries-breakdown-of-neural-elastic-and-search">GNES Preliminaries: Breakdown of Neural, Elastic and Search</span></h2><p>To understand why GNES is <a href="https://github.com/gnes-ai/gnes" target="_blank" rel="noopener">designed in this way</a>, one has to first understand how a standard neural search system works, which I will talk about first. I also assume you have basic knowledge of deep neural networks and representation learning, models like BERT, XLNet, VGG, AlexNet should not be alien names to you.</p>
<blockquote><p><strong> GNES has three runtimes: training, indexing, and querying. </strong></p>
</blockquote>
<p>The first concept I’d like to introduce is the <strong>runtime</strong>. In a typical search system, there are two fundamental tasks: <strong>indexing</strong> and <strong>querying</strong>. Indexing is storing the documents, querying is searching the documents on a given query, pretty straightforward. In a neural search system, one may also face another task: <strong>training</strong>, where one fine-tunes the model according to the data distribution to achieve better search relevance. These three tasks: indexing, querying, and training are what we call <em>three runtimes</em> in GNES. In the sequel, when talking about the system workflow, I will always specify the runtime it works in.</p>
<p>Now let’s consider the following pipeline consists of an encoder and an indexer. In the index runtime, the encoder transforms all documents (text, image, video, audio) into fixed-length vector representations $\{d_1, d_2, \ldots, d_n\,|\,d_i\in\mathbb{R}^D\}$, then the indexer stores all these vectors in a database that allows fast-access. In the query runtime, the encoder transforms a given query to a vector $q$, and looking for its top-k neighbours that are most similar to $q$, i.e. $\arg\max_{i\in{1,\ldots, n}} \mathrm{sim}(d_i, q)$.</p>
<blockquote><p><strong> A good neural search is only possible when document and query are comparable semantic units.</strong></p>
</blockquote>
<p>Despite being vague on the actual choice of encoder, indexer and similarity metric, the aforementioned pipeline should be quite straightforward to understand and intuitively could give you meaningful results. However, in practice, this pipeline only works in very limited scenarios. To be specific, this pipeline only makes sense if the query and the indexed document are <strong>comparable semantic units</strong>. For example, when the query and document are both sentences between 10 to 20 words, or when the query and document images are more or less in the same size. Otherwise, it will <em>fail</em>.</p>
<p>It fails not because of a bad encoder you choose, but because there is <em>no encoder</em> can embed arbitrary large information into a fixed-length vector. Like it or not, each encoder has an intrinsic optimal size it can handle, which depends on the deep neural network architecture, the training data, the atomic NN ops (e.g. the kernel and stride in <code>conv2d</code>) used. This optimal size is what I call <em>the optimum semantic unit</em>, documents/queries that are significantly smaller or larger than this size would degenerate the encoder’s performance, and yield a bad search result. The next picture illustrates this concept.</p>
<img src="/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/b80b38d6.png">
<p>In anyways, one should not expect that feeding a minimum unit to the encoder could yield any meaningful embedding. For most use cases, we have a good prior knowledge of the reasonable semantic unit, e.g. sentences split by <code>[,.!?]</code> in NLP, 32$\times$32 patches extracted a large image in CV. Finally, I leave the optimum semantic unit of each modality as a question mark, for the reasons that the optimum semantic unit often depends on the underlying encoder and the application.</p>
<p>To solve the problem of mismatched semantic units between query and document, we introduce a <strong>preprocessor</strong> module in GNES. In the index runtime, the preprocessor first segments a document into a set of chunks $D=\{c_1, c_2, \ldots, c_d\}$, each of which corresponds to a reasonable semantic unit. These chunks are then encoded and stored in a database. In the query runtime, the preprocessor segments the query into chunks again $Q=\{k_1, k_2, \ldots, k_q\}$, encoder embeds chunks into vectors and indexer searches for nearest chunks with their corresponding document IDs. Finally, a score is computed by taking the chunk weights, chunk-chunk relevances, document weights into account. More precisely, </p>
<p>$$P(Q, D) \propto \sum_{c_i\in D} \sum_{k_j\in Q} \underbrace{P(c_i, k_j)}_{\text{\tiny from indexer, computed}}\times\overbrace{P(c_i|D)}^{\text{\tiny from indexer, stored}}\times \underbrace{P(k_j|Q)}_{\text{\tiny from preprocessor}} \times\overbrace{P(D)}^{\text{\tiny{from indexer, predefined}}}$$</p>
<p>A complete sequential workflow should look like the following:</p>
<img src="/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/e654f308.png">
<blockquote><p><strong> Routers are required for building an elastic system. </strong></p>
</blockquote>
<p>As our goal is to scale up the whole workflow by allowing an arbitrary number of preprocessors, encoders, and indexers at each step, we need a router to correctly forward messages between steps. The behavior of a router depends on the topology of the workflow and the runtime. Sometimes a router serves as a mapper, splitting a message into multiple parts; other times it serves as a reducer or an aggregator. The type of a router can often be determined by the two consecutive steps, it may not be required in some corner cases.</p>
<blockquote><p><strong> GNES has four fundamental components: preprocessor, encoder, indexer and router. Each runs as an independent microservice. </strong></p>
</blockquote>
<p>To summarize, we have four fundamental components in GNES:</p>
<ul>
<li>Preprocessor: transforming a real-world object to a list of workable semantic units;</li>
<li>Encoder: representing a semantic unit with vector representation;</li>
<li>Indexer: storing the vectors into memory/disk that allows fast-access;</li>
<li>Router: forwarding messages between microservices: e.g. batching, mapping, reducing.</li>
</ul>
<p>In GNES, these components are running independently as microservices. For machine learning engineers and data scientists who are not familiar with the concept of <em>cloud-native</em> and <em>microservice</em>, one can picture a microservice as an app (on your smartphone). Each app can be independently installed, launched and deleted. To accomplish a task, (e.g. take a selfie, beautify it then post it on Facebook), you have to coordinate multiple apps working together.</p>
<h2><span id="gnes-design-philosophy">GNES Design Philosophy</span></h2><p>Now that you have learned all the fundamental concepts of GNES, I would like to talk about the key tenets of GNES. Please note that the following content is not served as <a href="https://doc.gnes.ai" target="_blank" rel="noopener">the documentation of GNES</a>, it may be vague on how GNES work internally.</p>
<h4><span id="a-cloud-native-system">A Cloud-Native System</span></h4><p>I never believe that a monolith program can serve in production, especially for the search scenario. Unlike many canonical monolithic deep learning packages, GNES is born to be cloud-native. From the very beginning, we have structured GNES as a collection of decoupled components (e.g. preprocessor, encoder, indexer, router), each of which can then be deployed, scaled and maintained independently. In GNES, each component is running as a stateless microservice in its own isolated Docker container. They follow a well­defined and narrowly-scoped API: read the incoming message, do something about it, and pass it to the next (microservice).</p>
<p>Note that the data layer is isolated between components in GNES: each component may have its own datastore and scaling characteristics and storage technology. For instance, some encoders may need to mount a file system for loading pretrained models, whereas some indexers might employ NoSQL databases. Multiple indexer services may also share the database with a simple configuration. Either way, the components have complete autonomy.</p>
<p>In the example below, we start with a simple GNES workflow for indexing. In particular, we want to index both the original documents and encoded vectors in parallel. The table below summarized the GNES YAML config we are using and the corresponding network topology (figure on the right is rendered by <a href="https://board.gnes.ai" target="_blank" rel="noopener">GNES Board</a>):</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Preprocessor</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Encoder</span></span><br><span class="line"><span class="bullet">-</span> <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Indexer</span></span><br><span class="line">   <span class="attr">yaml_path:</span> <span class="string">vector.yml</span></span><br><span class="line"> <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Indexer</span></span><br><span class="line">   <span class="attr">yaml_path:</span> <span class="string">full-text.yml</span></span><br></pre></td></tr></table></figure></td>
<td><img src="/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/e5a3f8f4.png"></td>
</tr>
</tbody>
</table>
<p>Then scaling it to:</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Preprocessor</span></span><br><span class="line"> <span class="attr">replicas:</span> <span class="number">10</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Encoder</span></span><br><span class="line"> <span class="attr">replicas:</span> <span class="number">10</span></span><br><span class="line"><span class="bullet">-</span> <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Indexer</span></span><br><span class="line">   <span class="attr">yaml_path:</span> <span class="string">vector.yml</span></span><br><span class="line">   <span class="attr">replicas:</span> <span class="number">10</span></span><br><span class="line"> <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Indexer</span></span><br><span class="line">   <span class="attr">yaml_path:</span> <span class="string">full-text.yml</span></span><br><span class="line">   <span class="attr">replicas:</span> <span class="number">10</span></span><br></pre></td></tr></table></figure></td>
<td><img src="/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/aa3561f4.png"></td>
</tr>
</tbody>
</table>
<p>Notice how routers are automatically added to the stack. And without changing the logic of any component, one can switch the whole stack to the query mode by simply changing the YAML config to:</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">services:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Preprocessor</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">10</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Encoder</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">10</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Indexer</span></span><br><span class="line">  <span class="attr">yaml_path:</span> <span class="string">vector.yml</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">10</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Indexer</span></span><br><span class="line">  <span class="attr">yaml_path:</span> <span class="string">full-text.yml</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">10</span></span><br></pre></td></tr></table></figure></td>
<td><img src="/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/70b3dee2.png"></td>
</tr>
</tbody>
</table>
<p>, where we search document IDs using the chunk embeddings first (via a vector-indexer defined in <code>vector.yml</code>), then fill in the documents with their original content (via a key-value indexer defined in <code>full-text.yml</code>). </p>
<p>Of course, in order to coordinate all microservices and maximize cloud’s advantage, we need some orchestration  technology such as Docker Swarm and Kubernetes. To that, we developed <a href="https://board.gnes.ai" target="_blank" rel="noopener">GNES Board</a>, an interactive web interface allowing one to modify GNES architecture and immediately get the corresponding Docker Swarm or Kubernetes deploy solution.</p>
<img src="/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/gnes-board-demo.gif">
<h4><span id="model-as-docker-and-docker-as-a-plugin">Model as Docker, and Docker as a Plugin</span></h4><p>After the release of Google BERT model and <a href="https://github.com/hanxiao/bert-as-service/" target="_blank" rel="noopener">my followed <code>bert-as-service</code> in late 2018</a>, the NLP community has put the research of pretrained language model on the fast-lane. People have proposed dozens of improved BERT models since then: some supports longer sentences, some outperforms the original BERT on various NLP tasks. From time to time, one frequent question I was often asked by the community is: <em>“Han, can you support model X and make it X-as-a-service?”</em></p>
<p>This makes me wonder if <code>bert-as-service</code> has a <strong>sustainable architecture</strong>, in which rapidly changing requirements can be met by constantly adding new models to it, not modifying (and breaking) old ones.</p>
<p>Frankly, I sometimes feel quite challenging to keep up with today’s AI development: new models pop up every week, refreshing the leaderboard, nagging you to give it a try. However, refactoring or porting the new model implementations into GNES framework can be time-consuming and not very cost-effective. Not to mention GNES covers algorithms from NLP, CV, database, etc. I’m also aware of some deep learning packages that attempt to incorporate as many models as they can, but I don’t think they can last long.</p>
<p>Therefore, the scope of GNES component, aka. the service boundary must be clearly defined.</p>
<p>Let’s first look at what an algorithm usually requires to run/reuse:</p>
<ul>
<li><strong>dependencies</strong>: packages or libraries required to run the algorithm, e.g. <code>ffmpeg</code>, <code>libcuda</code>, <code>tensorflow</code>;</li>
<li><strong>codes</strong>: the implementation of the logic, can be written in Python, C, Java, Scala with the help of Tensorflow, Pytorch, etc;</li>
<li><strong>a small config file</strong>: the arguments abstracted from the logic for better flexibility during training and inference. For example, <code>batch_size</code>, <code>index_strategy</code>, and <code>model_path</code>;</li>
<li><strong>big data files</strong>: the serialization of the model structure the learned parameters, e.g. a pretrained VGG/BERT model.</li>
</ul>
<p>In practice, it is extremely hard to manage all four pieces in an organized way. For example, some dependencies can not be installed via packaging tools like <code>pip</code>, and others must be first compiled and then <code>make install</code>. Meanwhile, the data file could be so big and you have to set up a separate AWS S3 bucket storing them and then do <code>wget</code>/<code>curl</code> as initialization. Did I mention CDN, MD5 checksum and version control on the data file? You get the idea: it is <em>hard</em>.</p>
<blockquote><p><strong> Model as Docker (container), and Docker as a plugin. This is the GNES solution in the face of accelerating innovation on new models from the AI community. </strong></p>
</blockquote>
<p>In GNES, changing a component is as simple as changing a line of config. The best part is, GNES allows one to change a component on different levels: on the container level, on the GNES YAML, or the component-wise YAML. For example, at the highest container level, changing the preprocessor and the encoder is done by simply modify the generated <a href="https://docs.docker.com/compose/compose-file/" target="_blank" rel="noopener">Docker Swarm config (i.e. a docker compose file)</a> from this:<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">'3.4'</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">gRPCFrontend00:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">gnes/gnes:latest</span></span><br><span class="line">    <span class="attr">command:</span> <span class="string">frontend</span> <span class="string">--grpc_port</span> <span class="number">5566</span> <span class="string">--port_out</span> <span class="number">65499</span> <span class="string">--socket_out</span> <span class="string">PUSH_BIND</span> <span class="string">--port_in</span></span><br><span class="line">      <span class="number">53346</span> <span class="string">--socket_in</span> <span class="string">PULL_BIND</span> <span class="string">--host_in</span> <span class="string">Indexer50</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">5566</span><span class="string">:5566</span></span><br><span class="line">  <span class="attr">Preprocessor10:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">gnes/gnes:latest</span></span><br><span class="line">    <span class="attr">command:</span> <span class="string">preprocess</span> <span class="string">--port_in</span> <span class="number">65499</span> <span class="string">--socket_in</span> <span class="string">PULL_CONNECT</span> <span class="string">--port_out</span> <span class="number">60390</span></span><br><span class="line">      <span class="string">--socket_out</span> <span class="string">PUSH_BIND</span> <span class="string">--host_in</span> <span class="string">gRPCFrontend00</span></span><br><span class="line">  <span class="attr">Encoder20:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">gnes/gnes:latest</span></span><br><span class="line">    <span class="attr">command:</span> <span class="string">encode</span> <span class="string">--port_in</span> <span class="number">60390</span> <span class="string">--socket_in</span> <span class="string">PULL_CONNECT</span> <span class="string">--port_out</span> <span class="number">59779</span> <span class="string">--socket_out</span></span><br><span class="line">      <span class="string">PUSH_BIND</span> <span class="string">--host_in</span> <span class="string">Preprocessor10</span></span><br><span class="line">  <span class="string">...</span></span><br></pre></td></tr></table></figure></p>
<p>to this:<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">'3.4'</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">gRPCFrontend00:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">gnes/gnes:latest</span></span><br><span class="line">    <span class="attr">command:</span> <span class="string">frontend</span> <span class="string">--grpc_port</span> <span class="number">5566</span> <span class="string">--port_out</span> <span class="number">65499</span> <span class="string">--socket_out</span> <span class="string">PUSH_BIND</span> <span class="string">--port_in</span></span><br><span class="line">      <span class="number">53346</span> <span class="string">--socket_in</span> <span class="string">PULL_BIND</span> <span class="string">--host_in</span> <span class="string">Indexer50</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">5566</span><span class="string">:5566</span></span><br><span class="line">  <span class="attr">Preprocessor10:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">my/chinese-preprocessor:latest</span>  <span class="comment"># &lt;== changing the container image</span></span><br><span class="line">    <span class="attr">command:</span> <span class="string">preprocess</span> <span class="string">--port_in</span> <span class="number">65499</span> <span class="string">--socket_in</span> <span class="string">PULL_CONNECT</span> <span class="string">--port_out</span> <span class="number">60390</span></span><br><span class="line">      <span class="string">--socket_out</span> <span class="string">PUSH_BIND</span> <span class="string">--host_in</span> <span class="string">gRPCFrontend00</span></span><br><span class="line">  <span class="attr">Encoder20:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">my/pretrained-chinese-bert:0.0.1</span>   <span class="comment"># &lt;== changing the container image</span></span><br><span class="line">    <span class="attr">command:</span> <span class="string">encode</span> <span class="string">--port_in</span> <span class="number">60390</span> <span class="string">--socket_in</span> <span class="string">PULL_CONNECT</span> <span class="string">--port_out</span> <span class="number">59779</span> <span class="string">--socket_out</span></span><br><span class="line">      <span class="string">PUSH_BIND</span> <span class="string">--host_in</span> <span class="string">Preprocessor10</span></span><br><span class="line">  <span class="string">...</span></span><br></pre></td></tr></table></figure></p>
<p>For AI engineers who want to contribute a new model, GNES provides an extremely straightforward interface in Python. One only needs to write a Python class with an accompanied YAML file. All other things like service, network communication, containerization, don’t you worry about them. They are all abstracted away from this interface and handled by GNES automatically. Let’s see the following example, where I build a dummy text encoder called <code>FooContribEncoder</code>:</p>
<table>
<thead>
<tr>
<th><code>dummy_model.py</code></th>
<th><code>dummy_model.yml</code></th>
</tr>
</thead>
<tbody>
<tr>
<td><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gnes.encoder.base <span class="keyword">import</span> BaseTextEncoder</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FooContribEncoder</span><span class="params">(BaseTextEncoder)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, bar: int, *args, **kwargs)</span>:</span></span><br><span class="line">        super().__init__(*args, **kwargs)</span><br><span class="line">        self.bar = bar</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, text, **kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">'hello %d'</span> % self.bar</span><br></pre></td></tr></table></figure></td>
<td><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">!FooContribEncoder</span></span><br><span class="line"><span class="attr">parameter:</span></span><br><span class="line">  <span class="attr">bar:</span> <span class="number">531</span></span><br><span class="line"><span class="attr">gnes_config:</span></span><br><span class="line">  <span class="attr">is_trained:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></td>
</tr>
</tbody>
</table>
<p>Likewise, one can implement its preprocessor, indexer and router. The next figure illustrates the abstraction and design in GNES. It enables AI engineers/developers to focus all of their development efforts on writing the logic of the algorithm, without a full understanding of the entire GNES system upfront, which can take an extraordinarily long period time to achieve.</p>
<img src="/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/79beac8f.png">
<h4><span id="generic-and-universal">Generic and Universal</span></h4><p>One lesson I learned from developing <code>bert-as-service</code> is that one should make the interface as universal as possible so that more people can use it via their favorite programming languages. In GNES, we employ <a href="https://developers.google.com/protocol-buffers/" target="_blank" rel="noopener">Protocol Buffers</a> and <a href="https://grpc.io/" target="_blank" rel="noopener">gRPC</a> in the very early stage of interface design. In particular, we make a clean separation between the information the end-user requires (<code>body</code>) and the information that the system requires (<code>envelope</code>). The next table summarized the protobuf design.</p>
<table>
<thead>
<tr>
<th>Protobuf: <code>Message</code></th>
<th>Protobuf: <code>Envelope</code></th>
</tr>
</thead>
<tbody>
<tr>
<td><figure class="highlight protobuf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">Message</span> </span>&#123;</span><br><span class="line">    Envelope envelope = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">oneof</span> body &#123;</span><br><span class="line">        Request request = <span class="number">2</span>;</span><br><span class="line">        Response response = <span class="number">3</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></td>
<td><figure class="highlight protobuf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">Envelope</span> </span>&#123;</span><br><span class="line">    <span class="comment">// unique id of the sender of the message</span></span><br><span class="line">    <span class="built_in">string</span> client_id = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// unique id of the request</span></span><br><span class="line">    <span class="built_in">string</span> request_id = <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// for multi-part message</span></span><br><span class="line">    <span class="built_in">uint32</span> part_id = <span class="number">3</span>;</span><br><span class="line">    <span class="keyword">repeated</span> <span class="built_in">uint32</span> num_part = <span class="number">4</span>;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">uint32</span> timeout = <span class="number">5</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// list of string represent the route of the message</span></span><br><span class="line">    <span class="class"><span class="keyword">message</span> <span class="title">route</span> </span>&#123;</span><br><span class="line">        <span class="built_in">string</span> <span class="class"><span class="keyword">service</span> = 1;</span></span><br><span class="line"><span class="class">        <span class="title">google</span>.protobuf.Timestamp timestamp = 2;</span></span><br><span class="line"><span class="class">    &#125;</span></span><br><span class="line"><span class="class">    repeated route routes = 6;</span></span><br><span class="line"><span class="class">&#125;</span></span><br></pre></td></tr></table></figure></td>
</tr>
</tbody>
</table>
<p>To end-users, the interface is simply <code>request</code> and <code>response</code>. They don’t have to know what <code>envelope</code> looks like. The table below summarizes the protobuf design of <code>request</code> and <code>response</code>.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><figure class="highlight protobuf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">Request</span> </span>&#123;</span><br><span class="line">    <span class="built_in">string</span> request_id = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">oneof</span> body &#123;</span><br><span class="line">        TrainRequest train = <span class="number">2</span>;</span><br><span class="line">        IndexRequest index = <span class="number">3</span>;</span><br><span class="line">        QueryRequest search = <span class="number">4</span>;</span><br><span class="line">        ControlRequest control = <span class="number">5</span>;</span><br><span class="line">    &#125;&#125;</span><br></pre></td></tr></table></figure></td>
<td><figure class="highlight protobuf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">Response</span> </span>&#123;</span><br><span class="line">   <span class="built_in">string</span> request_id = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">oneof</span> body &#123;</span><br><span class="line">       TrainResponse train = <span class="number">2</span>;</span><br><span class="line">       IndexResponse index = <span class="number">3</span>;</span><br><span class="line">       QueryResponse search = <span class="number">4</span>;</span><br><span class="line">       ControlResponse control = <span class="number">5</span>;</span><br><span class="line">   &#125;&#125; </span><br></pre></td></tr></table></figure></td>
</tr>
</tbody>
</table>
<p>When a user sends a request, <code>gRPCFrontend</code> first packs the message within an envelope, including all information that the core GNES components need. Each component then opens the envelope, reads/modifies the message, adds a stamp to the envelope, and passes to the next component. Finally, before sending the response back to the user, <code>gRPCFrontend</code> unpacks the message from the envelope and returns the response to the user. The complete message passing procedure is depicted in the next figure.</p>
<img src="/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/4200ee94.png">
<h2><span id="gnes-as-a-team">GNES as a Team</span></h2><blockquote><p>Any organization that designs a system (defined broadly) will produce a design whose structure is a copy of the organization’s communication structure.</p>
<footer><strong>M. Conway</strong></footer></blockquote>
<img src="/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/gnes-team-1600.JPG">
<p>As the final remark, I want to talk about the team behind GNES. Frankly, this is the first time I lead a team of five full-timely building an AI project from scratch. In the past few months, there are a lot of funs, tears and of course growths. In fact, the experience of building GNES refreshes my understanding of <a href="https://en.wikipedia.org/wiki/Conway%27s_law" target="_blank" rel="noopener">Conway’s law</a>, which I probably will share in another post. In general, building an open-source software inside a big company is actually not easy, harder than many people thought. Aligning the business interests and community’s interests is tricky, and as the founder of GNES there are many things I have to take care of and plan ahead besides writing code. Anyways, there are two tenets that I always ask my teammates to follow: </p>
<h4><span id="making-no-tech-debt">Making No Tech Debt</span></h4><p>Nobody likes tech debt, but everyone makes it, especially under business pressures. In the early days of GNES, I did spend some effort into setting up a healthy development environment inside the team. This includes code lint, <a href="https://github.com/gnes-ai/gnes/blob/master/CONTRIBUTING.md" target="_blank" rel="noopener">commit and PR protocols</a>, meeting and releasing schedules, test coverage, and the DevOps cycle. These “cultural” things seem irrelevant to what the AI community interested in, but over time the team gets better agility in managing the whole infrastructure and quicker adaption to the new requirements.</p>
<h4><span id="having-a-b2d-thoughtship">Having a B2D Thoughtship</span></h4><p>B2D is the shorthand for <em>business to developer</em>, as the products and services we are building are marketed to other developers. Having a B2D thoughtship means always keeping the developer experience in mind and thinking in switched positions. What are the pain-points for developers? Are there confusing and ambiguous in GNES APIs? How can we better support developers who use a different tech stack? Can we provide a seamless and easy way to let developers contribute to GNES? Every day our team learns the best practices from high-quality opensource software; avoids the bad code what we as developers dislike or blame at; and gradually builds up a vivid and healthy community around GNES, which is our ultimate goal.</p>
</div></article></div></main><footer><div class="paginator"><a href="/2019/10/18/GNES-Flow-a-Pythonic-Way-to-Build-Cloud-Native-Neural-Search-Pipelines/" class="prev">&nbsp;❮&nbsp;&nbsp;GNES Flow: a Pythoni...</a><a href="/2019/01/02/Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ/" class="next">Serving Google BERT ...&nbsp;&nbsp;❯&nbsp;</a></div><div class="footer-section"><h2><img src="/flower.png" alt="Checkout this">Check out these posts too!</h2><div class="archive-readmore"><div style="display: flex; gap: 10px; align-items: flex-start;" class="post-item"><a href="/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0/"><img src="https://hanxiao.io/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0//9ba076d0.png" alt="Video Semantic Search in Large Scale using GNES and Tensorflow 2.0" style="width: 120px; height: 90px; object-fit: cover;"></a><div style="flex: 1;" class="book-title"><h5 style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; margin: 0;" class="post-title"><a href="/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0/" class="post-title-link">Video Semantic Search in Large Scale using GNES and Tensorflow 2.0</a></h5></div></div><div style="display: flex; gap: 10px; align-items: flex-start;" class="post-item"><a href="/2019/11/07/A-Better-Practice-for-Managing-extras-require-Dependencies-in-Python/"><img src="https://hanxiao.io/2019/11/07/A-Better-Practice-for-Managing-extras-require-Dependencies-in-Python//banner.png" alt="A Better Practice for Managing Many &lt;code&gt;extras_require&lt;/code&gt; Dependencies in Python" style="width: 120px; height: 90px; object-fit: cover;"></a><div style="flex: 1;" class="book-title"><h5 style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; margin: 0;" class="post-title"><a href="/2019/11/07/A-Better-Practice-for-Managing-extras-require-Dependencies-in-Python/" class="post-title-link">A Better Practice for Managing Many <code>extras_require</code> Dependencies in Python</a></h5></div></div><div style="display: flex; gap: 10px; align-items: flex-start;" class="post-item"><a href="/2019/10/18/GNES-Flow-a-Pythonic-Way-to-Build-Cloud-Native-Neural-Search-Pipelines/"><img src="https://hanxiao.io/2019/10/18/GNES-Flow-a-Pythonic-Way-to-Build-Cloud-Native-Neural-Search-Pipelines//gnes-flow-banner.png" alt="GNES Flow: a Pythonic Way to Build Cloud-Native Neural Search Pipelines" style="width: 120px; height: 90px; object-fit: cover;"></a><div style="flex: 1;" class="book-title"><h5 style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; margin: 0;" class="post-title"><a href="/2019/10/18/GNES-Flow-a-Pythonic-Way-to-Build-Cloud-Native-Neural-Search-Pipelines/" class="post-title-link">GNES Flow: a Pythonic Way to Build Cloud-Native Neural Search Pipelines</a></h5></div></div><div style="display: flex; gap: 10px; align-items: flex-start;" class="post-item"><a href="/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/"><img src="https://hanxiao.io/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond//gnes-team-1600.JPG" alt="Generic Neural Elastic Search: From &lt;code&gt;bert-as-service&lt;/code&gt; and Go Way Beyond" style="width: 120px; height: 90px; object-fit: cover;"></a><div style="flex: 1;" class="book-title"><h5 style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; margin: 0;" class="post-title"><a href="/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/" class="post-title-link">Generic Neural Elastic Search: From <code>bert-as-service</code> and Go Way Beyond</a></h5></div></div></div></div><div class="copyright"><p>© 2017 - 2025 <a href="https://hanxiao.io">Han Xiao</a>. <img src="/by-nc-sa.svg" alt="Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License." class="image"></p></div></footer></div><link rel="stylesheet" href="/css/post/katex.min.css"><!--link(rel="stylesheet", href=url_for("css/post/gitment.css"))--><script src="/js/katex.min.js"></script><script src="/js/auto-render.min.js"></script><script>renderMathInElement(
    document.body,
    {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "\\[", right: "\\]", display: true},
            {left: "$", right: "$", display: false},
            {left: "\\(", right: "\\)", display: false}
        ]
    }
);</script><!--script(src=url_for("js/gitment.browser.js"))--><!--script(src=url_for("js/gitment.loader.js"))--><script src="/js/jquery-3.4.1.min.js"></script><script src="/js/reading_progress.min.js"></script><script src="/js/highlighter.min.js"></script><script src="/js/init-highlighter.js"></script><script async src="https://www.google-analytics.com/analytics.js"></script><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create',"UA-52114253-1",'auto');ga('send','pageview');</script></body></html>