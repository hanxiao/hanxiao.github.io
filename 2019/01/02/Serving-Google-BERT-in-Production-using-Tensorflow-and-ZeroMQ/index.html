<!DOCTYPE html><html lang="en"><style>html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}article,header,main{display:block}a{background-color:transparent}h1{font-size:2em;margin:.67em 0}img{border:0}body,html{width:100%;height:100%}html{width:100%;height:100vh;display:flex;flex-direction:column;justify-content:center;align-items:center;background:var(--bgColor);--bgColor:#fff;--textColor:#2c3e50;--bg2ndColor:none;--bg3rdColor:#f8f8f8;--preCodeColor:#525252;--imgOpacity:1.0}@media (prefers-color-scheme:dark){html{background:var(--bgColor);--bgColor:#121212;--textColor:#fff;--bg2ndColor:#fff;--bg3rdColor:#332940;--preCodeColor:#f8f8f8;--imgOpacity:0.5}}body{margin:0;color:var(--textColor);font-size:18px;line-height:1.6;background-color:var(--bgColor);font-family:sourcesanspro,'Helvetica Neue',Arial,sans-serif}ul.nav{margin:0;padding:0;list-style-type:none}ul{margin:1rem 0}a{color:var(--textColor);text-decoration:none}.flag-icon{height:25px;width:25px;display:inline;border-radius:50%;vertical-align:sub}.icon_item{padding-left:5px!important;padding-right:5px!important}.reading-progress-bar{background:#42b983;display:block;height:2px;left:0;position:fixed;top:0;width:0;z-index:10001}header{min-height:60px}header .logo-link{float:left}header .nav{float:right;left:80px}header .logo-link img{height:60px}header .nav-list-item{display:inline-block;padding:19px 10px}header .nav-list-item a{line-height:1.4}@media screen and (max-width:900px){header .nav-list-item a{font-size:12px}}@media screen and (min-width:900px){header .nav-list-item a{font-size:18px}}.post{padding-top:1em}.post-block .post-title{margin:.65em 0;color:var(--textColor);font-size:1.5em}.post-block .post-info{color:#7f8c8d}.post-block .post-info .read-time{text-align:right}.post-content h2,.post-content h4{position:relative;margin:1em 0}.post-content h2 :before,.post-content h4 :before{content:"#";color:#42b983;position:absolute;left:-.7em;top:-4px;font-size:1.2em;font-weight:700}.post-content h4 :before{content:">"}.post-content h2{font-size:22px}.post-content h4{font-size:18px}.post-content a{color:#42b983;word-break:break-all}main.container{margin:2em 10px}@media screen and (min-width:900px){.wrap{width:900px;margin:0 auto}header{padding:20px 60px}}@media screen and (max-width:900px){.wrap{width:100%}header{min-height:50px;padding:2px 2px;position:fixed;z-index:10000;border-radius:15px;left:50%;-webkit-transform:translateX(-50%);transform:translateX(-50%);width:-webkit-fit-content;width:-moz-fit-content;width:fit-content}header a.logo-link,header ul.nav.nav-list{float:none;display:inline;text-align:center}header li.nav-list-item{padding:10px 5px}header .logo-link img{height:20px;vertical-align:sub}header .flag-icon{height:20px;width:20px}header{background-color:rgba(255,255,255,.9)}@supports ((-webkit-backdrop-filter:blur(2em)) or (backdrop-filter:blur(2em))){header{background-color:rgba(255,255,255,.3);-webkit-backdrop-filter:blur(10px);backdrop-filter:blur(10px)}}main.container{padding-top:2em}main.container{margin:0 20px}.post-content h2,.post-content h4{max-width:300px;left:15px}}@font-face{font-family:sourcesanspro;src:url(/font/sourcesanspro.woff2) format("woff2"),url(/font/sourcesanspro.woff) format("woff");font-weight:400;font-style:normal}</style><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Serving Google BERT in Production using Tensorflow and ZeroMQ · Han Xiao Tech Blog - Neural Search & AI Engineering</title><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content="@hxiao"><meta name="twitter:creator" content="@hxiao"><meta name="description" content="When we look back at 2018, one of the biggest news in the world of ML and NLP is Google’s Bidirectional Encoder Representations from Transformers, ... · Han Xiao"><meta property="og:title" content="Serving Google BERT in Production using Tensorflow and ZeroMQ · Han Xiao Tech Blog - Neural Search &amp; AI Engineering"><meta property="og:description" content="When we look back at 2018, one of the biggest news in the world of ML and NLP is Google’s Bidirectional Encoder Representations from Transformers, ... · Han Xiao"><meta property="og:url" content="https://hanxiao.io/2019/01/02/Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ/"><meta property="og:image" content="https://hanxiao.io/2019/01/02/Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ//bert-as-service-architecture.png"><meta property="og:type" content="article"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/myavatar.png"><link rel="alternate" type="application/rss+xml" title="Han Xiao Tech Blog - Neural Search &amp; AI Engineering" href="https://hanxiao.io/atom.xml"><!-- - use css preload trick--><link rel="preload" href="/css/apollo.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="/css/apollo.css"></noscript><script>/*! loadCSS. [c]2017 Filament Group, Inc. MIT License */
/* This file is meant as a standalone workflow for
- testing support for link[rel=preload]
- enabling async CSS loading in browsers that do not support rel=preload
- applying rel preload css once loaded, whether supported or not.
*/
(function( w ){
    "use strict";
    // rel=preload support test
    if( !w.loadCSS ){
        w.loadCSS = function(){};
    }
    // define on the loadCSS obj
    var rp = loadCSS.relpreload = {};
    // rel=preload feature support test
    // runs once and returns a function for compat purposes
    rp.support = (function(){
        var ret;
        try {
            ret = w.document.createElement( "link" ).relList.supports( "preload" );
        } catch (e) {
            ret = false;
        }
        return function(){
            return ret;
        };
    })();

    // if preload isn't supported, get an asynchronous load by using a non-matching media attribute
    // then change that media back to its intended value on load
    rp.bindMediaToggle = function( link ){
        // remember existing media attr for ultimate state, or default to 'all'
        var finalMedia = link.media || "all";

        function enableStylesheet(){
            link.media = finalMedia;
        }

        // bind load handlers to enable media
        if( link.addEventListener ){
            link.addEventListener( "load", enableStylesheet );
        } else if( link.attachEvent ){
            link.attachEvent( "onload", enableStylesheet );
        }

        // Set rel and non-applicable media type to start an async request
        // note: timeout allows this to happen async to let rendering continue in IE
        setTimeout(function(){
            link.rel = "stylesheet";
            link.media = "only x";
        });
        // also enable media after 3 seconds,
        // which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
        setTimeout( enableStylesheet, 3000 );
    };

    // loop through link elements in DOM
    rp.poly = function(){
        // double check this to prevent external calls from running
        if( rp.support() ){
            return;
        }
        var links = w.document.getElementsByTagName( "link" );
        for( var i = 0; i < links.length; i++ ){
            var link = links[ i ];
            // qualify links to those with rel=preload and as=style attrs
            if( link.rel === "preload" && link.getAttribute( "as" ) === "style" && !link.getAttribute( "data-loadcss" ) ){
                // prevent rerunning on link
                link.setAttribute( "data-loadcss", true );
                // bind listeners to toggle media back
                rp.bindMediaToggle( link );
            }
        }
    };

    // if unsupported, run the polyfill
    if( !rp.support() ){
        // run once at least
        rp.poly();

        // rerun poly on an interval until onload
        var run = w.setInterval( rp.poly, 500 );
        if( w.addEventListener ){
            w.addEventListener( "load", function(){
                rp.poly();
                w.clearInterval( run );
            } );
        } else if( w.attachEvent ){
            w.attachEvent( "onload", function(){
                rp.poly();
                w.clearInterval( run );
            } );
        }
    }


    // commonjs
    if( typeof exports !== "undefined" ){
        exports.loadCSS = loadCSS;
    }
    else {
        w.loadCSS = loadCSS;
    }
}( typeof global !== "undefined" ? global : this ) );</script><script id="mcjs">!function(c,h,i,m,p){m=c.createElement(h),p=c.getElementsByTagName(h)[0],m.async=1,m.src=i,p.parentNode.insertBefore(m,p)}(document,"script","https://chimpstatic.com/mcjs-connected/js/users/7da58fc9885cb85d4a9f0ad9a/987f901145f1749fd3e800e86.js");</script><link rel="search" type="application/opensearchdescription+xml" href="https://hanxiao.io/atom.xml" title="Han Xiao Tech Blog - Neural Search &amp; AI Engineering"></head><body><div class="reading-progress-bar"></div><div class="wrap"><header><ul class="nav nav-list"><li class="nav-list-item"><a href="/about/" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="https://www.linkedin.com/in/hxiao87" target="_blank" class="nav-list-link">LINKEDIN</a></li><li class="nav-list-item"><a href="https://x.com/hxiao" target="_blank" class="nav-list-link">X</a></li><li class="nav-list-item"><a href="https://github.com/hanxiao" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="https://scholar.google.com/citations?user=jp7swwIAAAAJ" target="_blank" class="nav-list-link">SCHOLAR</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">Serving Google BERT in Production using Tensorflow and ZeroMQ</h1><div class="post-info">Jan 2, 2019 by &nbsp;&nbsp;&nbsp;<img src="/myavatar.png" alt="logo" width="18px" height="18px" style="vertical-align: sub;">&nbsp;<a href="/about" target="_blank" class="nav-list-link">Han Xiao - <i>ex</i> Engineering Lead @ Tencent AI Lab</a></div><div class="post-info"><div class="read-time">◷&nbsp;&nbsp;&nbsp; 25  min read</div></div><div class="post-content"><div class="quiz"><br>  This is a post explaining the design philosphy behind my open-source project <a href="https://github.com/hanxiao/bert-as-service" target="_blank" rel="noopener"><code>bert-as-service</code></a>, a highly-scalable sentence encoding service based on Google BERT and ZeroMQ. It allows one to map a variable-length sentence to a fixed-length vector. In case you haven’t checked it out yet, <a href="https://github.com/hanxiao/bert-as-service" target="_blank" rel="noopener">https://github.com/hanxiao/bert-as-service</a><br></div>


<h2><span id="background">Background</span></h2><p>When we look back at 2018, one of the biggest news in the world of ML and NLP is Google’s <a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">Bidirectional Encoder Representations from Transformers</a>, aka BERT. BERT is a method of pre-training language representations which achieves not only state-of-the-art but <em>record-breaking</em> results on a wide array of NLP tasks, such as <a href="/2018/04/21/Teach-Machine-to-Comprehend-Text-and-Answer-Question-with-Tensorflow/" title="machine reading comprehension">machine reading comprehension</a>.</p>
<p>To my team at Tencent AI Lab, BERT is particularly interesting as it provides a novel way to represent the semantic of text using real-valued fixed-length vectors. For many real-world NLP/AI applications that we are working on, an effective vector representation is the cornerstone. For example in the neural information retrieval, query and document need to be mapped to the same vector space, so that their relatedness can be computed using a metric defined in this space, e.g. Euclidean or cosine distance. The effectiveness of the representation directly determines the quality of the search.</p>
<a id="more"></a>
<p>So if many NLP applications rely on semantic features, then why don’t we build a sentence encoding infrastructure that can serve multiple teams? This idea, despite how simple and straightforward, is not really practical until recently. Because many deep learning algorithms tailor the vector representation to a specific task or domain. Consequently, the representation from one application/team is not really reusable to other applications/teams. On contrary, BERT (as well as <a href="https://allennlp.org/elmo" target="_blank" rel="noopener">ELMo</a> and <a href="http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html" target="_blank" rel="noopener">ULMFit</a>) decomposes an NLP task into <em>task-independent</em> pretraining and <em>task-specific</em> fine-tuning stages, where pretraining learns a model that is general enough and can be reused in many downstream tasks.</p>
<p>Over the past few weeks, I implemented this idea as <a href="https://github.com/hanxiao/bert-as-service" target="_blank" rel="noopener">an open-source project <code>bert-as-service</code></a>. It wraps the BERT model as a sentence encoding service, allowing one to map a variable-length sentence to a fixed-length vector. It is optimized for inference speed, low memory footprint, and scalability. The <a href="https://github.com/hanxiao/bert-as-service/blob/master/README.md" target="_blank" rel="noopener"><code>README.md</code></a> and <a href="http://bert-as-service.readthedocs.io" target="_blank" rel="noopener">documentation</a> already cover the usage and APIs very well. In this article, I will focus on the technical details especially the design philosophy about this project, which I hope can offer you some references when serving a Tensorflow model in production.</p>
<!-- toc -->
<ul>
<li><a href="#bert-recap">BERT Recap</a></li>
<li><a href="#bert-as-a-service">BERT as a service</a></li>
<li><a href="#research-finding-an-effective-embeddingencoding">Research: Finding an Effective Embedding/Encoding</a></li>
<li><a href="#engineering-building-a-scalable-service">Engineering: Building a Scalable Service</a><ul>
<li><a href="#decoupling-bert-and-downstream-network">Decoupling BERT and downstream network</a></li>
<li><a href="#serving-with-fast-inference-speed">Serving with fast inference speed</a></li>
<li><a href="#serving-with-low-latency">Serving with low latency</a></li>
<li><a href="#serving-with-high-scalability">Serving with high scalability</a></li>
</ul>
</li>
<li><a href="#summary">Summary</a></li>
</ul>
<!-- tocstop -->
<h2><span id="bert-recap">BERT Recap</span></h2><p>There are several highlights in BERT: multi-head self-attention network, dual training task (i.e. masked language model and next sentence prediction), large-scale TPU training. All these features together make this record-breaking NLP model. But good performance is just one part of the story. What I like the most is the <em>design pattern</em> of BERT: it trains a general-purpose “language understanding” model on a large text corpus (e.g. Wikipedia), and then uses that model for a variety of NLP tasks that we care about. Unlike the end-to-end learning, BERT decomposes a traditional NLP learning task into two stages: <strong>pretraining</strong> and <strong>fine-tuning</strong>:</p>
<ul>
<li>Pretraining learns from plain text data that is publicly available on the web, with the goal of capturing the general grammar and contextual information of a language. </li>
<li>Fine-tuning learns from a specific task (e.g. sentiment analysis, machine translation, reading comprehension) or data from a particular domain (e.g. legal documents).</li>
</ul>
<img src="/2019/01/02/Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ/9c4dd21b.png">
<p>However, pretraining is a time-consuming procedure which requires a lot of computational resources. According to Google, it took them four days on 4 to 16 Cloud TPUs to finish the pretraining. This is way beyond the patience of most researchers and engineers (and of course product owners). Fortunately, this is a one-time thing for every language, and Google has released <a href="https://github.com/hanxiao/bert-as-service#1-download-a-pre-trained-bert-model" target="_blank" rel="noopener">pretrained BERT models for multiple languages</a>. In practice, you can simply download a pretrained BERT, feed its output representation to a  downstream network customized to your task. Note, as the general language information has been already memorized in the pretrained BERT, a light-weight downstream network is often quite sufficient. You may also fine-tune the complete network (i.e. BERT + downstream network) in an end-to-end manner.</p>
<h2><span id="bert-as-a-service">BERT as a service</span></h2><p>What do we actually want from a BERT model? In the IR/search domain, we want the vector representation of query and document, using it to compute similarity or relatedness between query and document. In the classification-oriented tasks, we want to use the vector representations as well as the pre-annotated labels to train a downstream classifier. In the ensemble learning system, we want the BERT vector as a part of our feature pool, which is concatenated together to build a richer representation. <em>I give you a text, you return me a vector.</em> This  concludes a large part of the requirements for many AI teams. </p>
<img src="/2019/01/02/Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ/dd4c4faa.png">
<p>What do we need from a “BERT service”? Fast inference speed, low memory footprint and high scalability. That’s the basic requirement for the service provider. From the customer side (i.e. other engineers and product teams), the service should be easy to use. The API should be as intuitive as <code>vector = encode(texts)</code> without worrying about <em>anything</em> else.</p>
<h2><span id="research-finding-an-effective-embeddingencoding">Research: Finding an Effective Embedding/Encoding</span></h2><p>Before we get the hands dirty, let’s first think about how to get an effective sentence embedding from a BERT model. A pretrained BERT model has 12/24 layers, each “self-attends” on the previous one and outputs a <code>[batch_size, seq_length, num_hidden]</code> tensor. If you are unfamiliar with the concept of self-attention, I strongly suggest you <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">read this paper first</a>. Getting word embedding is straightforward, but if the goal is getting a sentence embedding, we need to pool such <code>[B,T,D]</code> tensor into a <code>[B,D]</code> embedding matrix. In my <a href="/2018/06/24/4-Encoding-Blocks-You-Need-to-Know-Besides-LSTM-RNN-in-Tensorflow/" title="previous blog post">previous blog post</a>, I talked about different pooling strategies. Among them, average pooling, max pooling, hierarchical pooling as well as concatenate avg-max pooling can be applied here directly as they do not introduce new parameters (hence no extra training).</p>
<p>Despite those general pooling strategies, if you read <a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">Google BERT paper</a> careful enough, you should notice that two special tokens <code>[CLS]</code> and <code>[SEP]</code> are padded to the beginning and the end of an input sequence, respectively. Once fine-tuned with downstream tasks, the embedding of those two tokens can represent the whole sequence. In fact, this is <a href="https://github.com/google-research/bert/blob/master/modeling.py#L229" target="_blank" rel="noopener">explicitly used in the official BERT source code</a>. Thus, we can include them in as well. However, if the BERT model is only pretrained and not fine-tuned on any downstream task, embeddings on those two symbols are meaningless.</p>
<p>Now we have a bunch of pooling strategies, which layer should we apply them to? Empirically, one might use the last layer, same as in stacked LSTM/CNN. However, keep in mind that BERT is a model pretrained with a bi-partite target: masked language model and next sentence prediction. The last layer is trained in the way to fit this target, making it too “biased” to those two targets. For the sake of generalization, we could simply take the second-to-last layer and do the pooling.</p>
<p>Different BERT layers capture different information. To see that more clearly, I made a visualization on <a href="https://www.kaggle.com/uciml/news-aggregator-dataset" target="_blank" rel="noopener">UCI-News Aggregator Dataset</a> with <a href="https://github.com/hanxiao/bert-as-service#1-download-a-pre-trained-bert-model" target="_blank" rel="noopener">pretrained <code>uncased_L-12_H-768_A-12</code></a>, where I randomly sample 20K news titles; get sentence encodes from different layers using max and average pooling, finally reduce it to 2D via PCA. There are only four classes of the data, illustrated in red, blue, yellow and green.</p>
<img src="/2019/01/02/Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ/pool_mean.png">
<img src="/2019/01/02/Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ/pool_max.png">
<p>One can observe that consecutive layers have similar representation, whereas the first few layers represent considerably different meaning compared to the last few layers. The deepest layer is the one next to the word embedding. It may preserve the very original word information (with no fancy self-attention etc.). On the other hand, one may achieve the very same performance by simply using word embedding. Therefore, anything in-between the first layer and the last layer is then a trade-off. The next animation illustrates the difference between layers more clearly.</p>
<img src="/2019/01/02/Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ/pool_animate.gif">
<h2><span id="engineering-building-a-scalable-service">Engineering: Building a Scalable Service</span></h2><p>Fortunately, pooling is the only research problem that we have to face at the moment. The rest of the work is mostly about engineering.</p>
<h4><span id="decoupling-bert-and-downstream-network">Decoupling BERT and downstream network</span></h4><p>The first thing to do is decoupling the main BERT model and the downstream network. More specifically, that 12/24-layer stacked multi-head attention network should be hosted in another process or even on another machine. For example, you can put it on a cost-per-use GPU machine, serving multiple teams simultaneously. The downstream network/models are often light-weighted and may not need deep learning libraries at all, they can run on a CPU machine or a mobile device.</p>
<img src="/2019/01/02/Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ/d5ff2731.png">
<p>Decoupling also clarifies the C/S role. When the feature extraction becomes the bottleneck, then scale up the GPU server. When the downstream network is the bottleneck, then optimize the client by adding more CPU machines or doing quantization. When training data is too old or concept-drifted, then retrain the BERT and version-control the server, all downstream networks immediately enjoy the updated feature vectors. Finally, as all requests come to one place, your GPU server has less idle cycles and every penny is spent worthily.</p>
<p>To build the communication stack, I use <a href="http://zeromq.org/" target="_blank" rel="noopener">ZeroMQ</a> and its python bindings <a href="https://github.com/zeromq/pyzmq" target="_blank" rel="noopener">PyZMQ</a>, which offer a lightweight and fast messaging implementation. You can send and receive an inter-process message via TCP/IPC/<a href="http://api.zeromq.org/4-2:zmq-ipc" target="_blank" rel="noopener">many other protocols</a> simply as follows:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> zmq</span><br><span class="line"><span class="keyword">import</span> zmq.decorators <span class="keyword">as</span> zmqd</span><br><span class="line"></span><br><span class="line"><span class="meta">@zmqd.socket(zmq.PUSH)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">send</span><span class="params">(sock)</span>:</span></span><br><span class="line">    sock.bind(<span class="string">'tcp://*:5555'</span>)</span><br><span class="line">    sock.send(<span class="string">b'hello'</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># in another process   </span></span><br><span class="line"><span class="meta">@zmqd.socket(zmq.PULL)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">recv</span><span class="params">(sock)</span>:</span></span><br><span class="line">    sock.connect(<span class="string">'tcp://localhost:5555'</span>)</span><br><span class="line">    print(sock.recv())  <span class="comment"># shows b'hello'</span></span><br></pre></td></tr></table></figure>
<h4><span id="serving-with-fast-inference-speed">Serving with fast inference speed</span></h4><p>The original BERT code released by Google supports training and evaluating. This introduces some auxiliary nodes that should be removed from the computational graph before serving. Also notice that if one uses the k-th layer for pooling, then all parameters from (k+1)-th to the last layers are not necessary for the inference and thus can be safely removed as well. The following picture summarizes a general procedure before serving a deep neural network in production.</p>
<img src="/2019/01/02/Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ/1ecc781a.png">
<p>Specifically, freezing substitutes all variables by constants, i.e. from <code>tf.Variable</code> to <code>tf.Constant</code>. Pruning removes all unnecessary nodes and edges from the graph. Quantizing replaces all parameters by their lower precision counterparts, e.g. from <code>tf.float32</code> to <code>tf.float16</code> or even <code>tf.uint8</code>. Currently, most quantization methods are implemented for mobile devices and therefore one may not observe significant speedup on X86 architectures.</p>
<p>Tensorflow provides APIs for freezing and pruning. We only need to specify the input and output nodes before optimizing the graph, such as:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input_tensors = [input_ids, input_mask, input_type_ids]</span><br><span class="line">output_tensors = [pooled]</span><br></pre></td></tr></table></figure></p>
<p>Then simply do:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.python.tools.optimize_for_inference_lib <span class="keyword">import</span> optimize_for_inference</span><br><span class="line"><span class="keyword">from</span> tensorflow.graph_util <span class="keyword">import</span> convert_variables_to_constants</span><br><span class="line"></span><br><span class="line"><span class="comment"># get graph</span></span><br><span class="line">tmp_g = tf.get_default_graph().as_graph_def()</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"><span class="comment"># load parameters then freeze</span></span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">tmp_g = convert_variables_to_constants(sess, tmp_g, [n.name[:<span class="number">-2</span>] <span class="keyword">for</span> n <span class="keyword">in</span> output_tensors])</span><br><span class="line"></span><br><span class="line"><span class="comment"># pruning</span></span><br><span class="line">dtypes = [n.dtype <span class="keyword">for</span> n <span class="keyword">in</span> input_tensors]</span><br><span class="line">tmp_g = optimize_for_inference(tmp_g, [n.name[:<span class="number">-2</span>] <span class="keyword">for</span> n <span class="keyword">in</span> input_tensors],</span><br><span class="line">    [n.name[:<span class="number">-2</span>] <span class="keyword">for</span> n <span class="keyword">in</span> output_tensors],</span><br><span class="line">    [dtype.as_datatype_enum <span class="keyword">for</span> dtype <span class="keyword">in</span> dtypes], <span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">with</span> tf.gfile.GFile(<span class="string">'optimized.graph'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(tmp_g.SerializeToString())</span><br></pre></td></tr></table></figure>
<h4><span id="serving-with-low-latency">Serving with low latency</span></h4><p>We don’t want to spawn a new BERT model every time a new request comes in, instead, we want to spawn the model only one time in the beginning and listen to the request in an event-loop. Calling <code>sess.run(feed_dict={...})</code> is one solution, but it’s not efficient enough. Besides, as the original BERT code is wrapped with high-level <code>tf.Estimator</code> API, we need to do some tweaks to inject the listener. A perfect place for such injection is in the generator of <code>input_fn</code>, which is the fundamental element of <code>tf.Data</code> API. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">input_fn_builder</span><span class="params">(sock)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">gen</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># receive request</span></span><br><span class="line">            client_id, raw_msg = sock.recv_multipart()</span><br><span class="line">            msg = jsonapi.loads(raw_msg)</span><br><span class="line">            tmp_f = convert_lst_to_features(msg)</span><br><span class="line">            <span class="keyword">yield</span> &#123;<span class="string">'client_id'</span>: client_id,</span><br><span class="line">                   <span class="string">'input_ids'</span>: [f.input_ids <span class="keyword">for</span> f <span class="keyword">in</span> tmp_f],</span><br><span class="line">                   <span class="string">'input_mask'</span>: [f.input_mask <span class="keyword">for</span> f <span class="keyword">in</span> tmp_f],</span><br><span class="line">                   <span class="string">'input_type_ids'</span>: [f.input_type_ids <span class="keyword">for</span> f <span class="keyword">in</span> tmp_f]&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">input_fn</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (tf.data.Dataset.from_generator(gen,</span><br><span class="line">            output_types=&#123;<span class="string">'input_ids'</span>: tf.int32, <span class="string">'input_mask'</span>: tf.int32, <span class="string">'input_type_ids'</span>: tf.int32, <span class="string">'client_id'</span>: tf.string&#125;,</span><br><span class="line">            output_shapes=&#123;<span class="string">'client_id'</span>: (), <span class="string">'input_ids'</span>: (<span class="literal">None</span>, max_seq_len), <span class="string">'input_mask'</span>: (<span class="literal">None</span>, max_seq_len),<span class="string">'input_type_ids'</span>: (<span class="literal">None</span>, max_seq_len)&#125;)</span><br><span class="line">                .prefetch(<span class="number">10</span>))</span><br><span class="line">    <span class="keyword">return</span> input_fn</span><br></pre></td></tr></table></figure>
<p>Then, one can simply call:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># initialize BERT model once</span></span><br><span class="line">estimator = Estimator(model_fn=bert_model_fn)</span><br><span class="line"><span class="comment"># keep listen and predict</span></span><br><span class="line"><span class="keyword">for</span> result <span class="keyword">in</span> estimator.predict(input_fn_builder(client), yield_single_examples=<span class="literal">False</span>):</span><br><span class="line">    send_back(result)</span><br></pre></td></tr></table></figure></p>
<p>Note that <code>estimator.predict()</code> returns a generator and the above for-loop never ends. When there is a new request, the generator will prepare the data and feed it to the estimator. Otherwise, the generator will be blocked at <code>sock.recv_multipart()</code> until the next request.</p>
<p>Careful readers may notice <code>.prefetch(10)</code> at the end of <code>tf.data.Dataset</code>. Adding the prefetch mechanism can effectively hide the batch preparing time (on CPU, introduced by <code>convert_lst_to_features()</code>) behind the actual prediction time (on GPU). When the model is doing prediction and a new request comes in, no-prefetch will simply be blocked at <code>yield</code> until the prediction is finished, whereas with <code>.prefetch(10)</code> it will keep preparing batches until there are 10 pending batches queued for prediction. In practice, I found adding <code>prefetch</code> gives 10% speedup. Of course, this is only effective on a GPU machine.</p>
<p>There are other optimization tricks implemented in <code>bert-as-service</code>, including turn on XLA compiler. Interested readers are encouraged to <a href="https://github.com/hanxiao/bert-as-service/tree/master/server/bert_serving/server" target="_blank" rel="noopener">read the source directly</a>.</p>
<h4><span id="serving-with-high-scalability">Serving with high scalability</span></h4><p>Say multiple clients are sending requests to a server simultaneously. Parallelization on the computational work is one thing, but first, how should the server even handle receiving? Should it receive the first request, hold this connection until it sends back the result; then proceed to the second request? What happens if there are 100 clients? Should the server use the same logic to manage 100 connections?</p>
<p>As the second example, consider a client is sending 10K sentences at every 10ms. The server parallelizes the work into sub-tasks and assigns them to multiple GPU workers. Then another client joins in, sending one sentence per second. Instinctively, this small-batch-client should get the result instantly. Unfortunately, as all GPU workers are busy computing for and receiving from the first client, the second client will never get a time slot until the server finishes 100 batches (each with 10K sentences) from the first client.</p>
<p>These are scalability and load-balancing issues when multiple clients connect to one server. In <code>bert-as-service</code>, I implement a ventilator-worker-sink pipeline equipped with push/pull and publish/subscribe sockets. The <strong>ventilator</strong> acts like a batch scheduler and a load balancer. It partitions the large request from clients into mini-jobs. It balances the load of those mini-jobs before sending them to workers. The <strong>worker</strong> receives mini-jobs from the ventilator and does the actual BERT inference, finally sends the result to the sink. The <strong>sink</strong> collects the output of mini-jobs from all workers. It checks the completeness of all requests from the ventilator, publishes the complete results to clients.</p>
<p>The overall architecture is depicted below.</p>
<img src="/2019/01/02/Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ/bert-as-service-architecture.png">
<p>I have to confess this is not the first thing came to my mind. What you see here is the pipeline after multiple iterations, the one that very well solves the scalability issues mentioned above. Let me explain it in details.</p>
<ul>
<li><p><strong>Separated sockets for send and receive on the client.</strong> Comparing to the standard REQ-REP socket (which is in lockstep), the PUSH-PULL socket does not wait for the response before the next PUSH. The client can send multiple requests in a row, then ask for the results later. When the computation is done, the server broadcasts the result via the PUB socket and uses client IDs as the header. The client listens to the broadcast from the SUB socket and fetches the subscriptions matching its own identity.</p>
<p>  There are at least two advantages of the PUSH-PULL and PUB-SUB design. First, there is <em>no need to</em> keep the connection alive between the server and the client. As a client, you just put the data to the predefined location, job’s done. You know someone (the server) will pick it up (via PULL). You don’t care about whether the server is alive or dead or restarted or parallelized. As the server, you go to the predefined location and get jobs. You don’t care about who put it there and how many are there. You do your best and load’em all! Same goes for receiving, every client gets its result from the predefined location as it knows that’s the place where results would appear. This design makes the system more scalable and robust. In <code>bert-as-service</code>, you can simply kill the server and restart it, the same <code>BertClient</code> still works fine.</p>
<p>  The second advantage is that it enables cool features such as asynchronous encoding and multicasting. Async encoding is particularly useful when you want to overlap of sentence pre-processing time and the encoding time for efficiency reasons. Now multicasting is a part of <a href="https://bert-as-service.readthedocs.io/en/latest/source/client.html#client.BertClient.encode_async" target="_blank" rel="noopener">BertClient API</a>, which allows multiple clients with the same identity to receive the results simultaneously while only encoding once. The next figure illustrates this idea. The complete example can be <a href="https://github.com/hanxiao/bert-as-service#asynchronous-encoding" target="_blank" rel="noopener">found here</a>.</p>
  <img src="/2019/01/02/Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ/multicast.gif">
</li>
<li><p><strong>Kill back-chatters.</strong> Careful readers may notice that data always flows down the pipeline: all messages are not sent upstream but only sent downstream to another socket, and the recipients don’t talk back to senders. Killing back-chatter is essential to real scalability. When we remove back-chatter, the overall message flow becomes much simpler and non-blocking, which lets us make simpler APIs and protocols, and in general lower latency.</p>
</li>
</ul>
<ul>
<li><p><strong>Multiple sockets between the ventilator and workers.</strong> If the size of a request is smaller than 16 sentences, then the ventilator pushes the job to the first socket (Q0 in the architecture graph). Otherwise, the ventilator partitions the job into mini-jobs where each has at most 256 sentences, then push these mini-jobs to a random socket from 1 to 7. The workers keep pulling jobs from all eight sockets following the ascending index order. </p>
<p>  This ensures that the small request won’t be blocked by the large-and-high-frequency request. A small request always goes to Q0, which gets pulled by the workers first. For large-and-high-frequency requests from multiple clients, they will be pushed to different sockets and get an equal chance for computing on the workers.    </p>
</li>
<li><p><strong>Separated processes for the ventilator, workers and the sink.</strong> Separating components at the process-level improves the robustness and reduces the latency. One may argue that the sink and the ventilator can be combined into one module. But then a result message would still have to travel across process, which won’t reduce the communication cost. Besides, having separated processes keeps the overall logic and message flow simple. Every component focuses on one job only. We can easily scale up the sink or the ventilator when either becomes the bottleneck.</p>
</li>
</ul>
<p>Finally, interested readers are always welcome to <a href="https://github.com/hanxiao/bert-as-service/blob/master/server/bert_serving/server/__init__.py" target="_blank" rel="noopener">read the source directly</a>. To validate the efficiency of this architecture, I benchmarked the service with different parameters, achieving ~900 sentences/s on single GPU and scaling nicely on multi-GPU or with multi-client. The full result can be found <a href="https://github.com/hanxiao/bert-as-service#zap-benchmark" target="_blank" rel="noopener">in the last section of the Github README.md</a>. </p>
<h2><span id="summary">Summary</span></h2><p>Since the release of <code>bert-as-service</code> in Nov. 2018, it has received quite some attention from the community. It collected more than 1K Github stars in a month. People message me and appreciate for its simplicity and scalability, allowing them to quickly try the latest NLP technique. Personally, deploying a deep learning model into production is always a great learning experience. As written in this post, it requires knowledge on both research and engineering sides to make things really work. I also greatly appreciate the community for pointing out bugs, improvements and new ideas. There was indeed some busy days. But these feedbacks eventually bring the quality of the project to the next level. Having a deep learning model in production is more than a lesson, especially when it starts to receive real traffic. You can simply lay down on the chair, <a href="https://github.com/hanxiao/bert-as-service#monitoring-the-service-status-in-a-dashboard" target="_blank" rel="noopener">watch the dashboard pulsing back and forth</a>, and enjoy this great accomplishment for the rest of the day.</p>
<img src="/2019/01/02/Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ/baf17688.png"></div></article></div></main><footer><div class="paginator"><a href="/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/" class="prev">&nbsp;❮&nbsp;&nbsp;Generic Neural Elast...</a><a href="/2018/09/28/Fashion-MNIST-Year-In-Review/" class="next">Fashion-MNIST: Year ...&nbsp;&nbsp;❯&nbsp;</a></div><div class="footer-section"><h2><img src="/flower.png" alt="Checkout this">Check out these posts too!</h2><div class="archive-readmore"><div style="display: flex; gap: 10px; align-items: flex-start;" class="post-item"><a href="/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0/"><img src="https://hanxiao.io/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0//9ba076d0.png" alt="Video Semantic Search in Large Scale using GNES and Tensorflow 2.0" style="width: 120px; height: 90px; object-fit: cover;"></a><div style="flex: 1;" class="book-title"><h5 style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; margin: 0;" class="post-title"><a href="/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0/" class="post-title-link">Video Semantic Search in Large Scale using GNES and Tensorflow 2.0</a></h5></div></div><div style="display: flex; gap: 10px; align-items: flex-start;" class="post-item"><a href="/2019/11/07/A-Better-Practice-for-Managing-extras-require-Dependencies-in-Python/"><img src="https://hanxiao.io/2019/11/07/A-Better-Practice-for-Managing-extras-require-Dependencies-in-Python//banner.png" alt="A Better Practice for Managing Many &lt;code&gt;extras_require&lt;/code&gt; Dependencies in Python" style="width: 120px; height: 90px; object-fit: cover;"></a><div style="flex: 1;" class="book-title"><h5 style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; margin: 0;" class="post-title"><a href="/2019/11/07/A-Better-Practice-for-Managing-extras-require-Dependencies-in-Python/" class="post-title-link">A Better Practice for Managing Many <code>extras_require</code> Dependencies in Python</a></h5></div></div><div style="display: flex; gap: 10px; align-items: flex-start;" class="post-item"><a href="/2019/10/18/GNES-Flow-a-Pythonic-Way-to-Build-Cloud-Native-Neural-Search-Pipelines/"><img src="https://hanxiao.io/2019/10/18/GNES-Flow-a-Pythonic-Way-to-Build-Cloud-Native-Neural-Search-Pipelines//gnes-flow-banner.png" alt="GNES Flow: a Pythonic Way to Build Cloud-Native Neural Search Pipelines" style="width: 120px; height: 90px; object-fit: cover;"></a><div style="flex: 1;" class="book-title"><h5 style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; margin: 0;" class="post-title"><a href="/2019/10/18/GNES-Flow-a-Pythonic-Way-to-Build-Cloud-Native-Neural-Search-Pipelines/" class="post-title-link">GNES Flow: a Pythonic Way to Build Cloud-Native Neural Search Pipelines</a></h5></div></div><div style="display: flex; gap: 10px; align-items: flex-start;" class="post-item"><a href="/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/"><img src="https://hanxiao.io/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond//gnes-team-1600.JPG" alt="Generic Neural Elastic Search: From &lt;code&gt;bert-as-service&lt;/code&gt; and Go Way Beyond" style="width: 120px; height: 90px; object-fit: cover;"></a><div style="flex: 1;" class="book-title"><h5 style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; margin: 0;" class="post-title"><a href="/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/" class="post-title-link">Generic Neural Elastic Search: From <code>bert-as-service</code> and Go Way Beyond</a></h5></div></div></div></div><div class="copyright"><p>© 2017 - 2025 <a href="https://hanxiao.io">Han Xiao</a>. <img src="/by-nc-sa.svg" alt="Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License." class="image"></p></div></footer></div><link rel="stylesheet" href="/css/post/katex.min.css"><!--link(rel="stylesheet", href=url_for("css/post/gitment.css"))--><script src="/js/katex.min.js"></script><script src="/js/auto-render.min.js"></script><script>renderMathInElement(
    document.body,
    {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "\\[", right: "\\]", display: true},
            {left: "$", right: "$", display: false},
            {left: "\\(", right: "\\)", display: false}
        ]
    }
);</script><!--script(src=url_for("js/gitment.browser.js"))--><!--script(src=url_for("js/gitment.loader.js"))--><script src="/js/jquery-3.4.1.min.js"></script><script src="/js/reading_progress.min.js"></script><script src="/js/highlighter.min.js"></script><script src="/js/init-highlighter.js"></script><script async src="https://www.google-analytics.com/analytics.js"></script><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create',"UA-52114253-1",'auto');ga('send','pageview');</script></body></html>