<!DOCTYPE html><html lang="en"><style>html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}article,header,main{display:block}a{background-color:transparent}h1{font-size:2em;margin:.67em 0}img{border:0}body,html{width:100%;height:100%}html{width:100%;height:100vh;display:flex;flex-direction:column;justify-content:center;align-items:center;background:var(--bgColor);--bgColor:#fff;--textColor:#2c3e50;--bg2ndColor:none;--bg3rdColor:#f8f8f8;--preCodeColor:#525252;--imgOpacity:1.0}@media (prefers-color-scheme:dark){html{background:var(--bgColor);--bgColor:#121212;--textColor:#fff;--bg2ndColor:#fff;--bg3rdColor:#332940;--preCodeColor:#f8f8f8;--imgOpacity:0.5}}body{margin:0;color:var(--textColor);font-size:18px;line-height:1.6;background-color:var(--bgColor);font-family:sourcesanspro,'Helvetica Neue',Arial,sans-serif}ul.nav{margin:0;padding:0;list-style-type:none}ul{margin:1rem 0}a{color:var(--textColor);text-decoration:none}.flag-icon{height:25px;width:25px;display:inline;border-radius:50%;vertical-align:sub}.icon_item{padding-left:5px!important;padding-right:5px!important}.reading-progress-bar{background:#42b983;display:block;height:2px;left:0;position:fixed;top:0;width:0;z-index:10001}header{min-height:60px}header .logo-link{float:left}header .nav{float:right;left:80px}header .logo-link img{height:60px}header .nav-list-item{display:inline-block;padding:19px 10px}header .nav-list-item a{line-height:1.4}@media screen and (max-width:900px){header .nav-list-item a{font-size:12px}}@media screen and (min-width:900px){header .nav-list-item a{font-size:18px}}.post{padding-top:1em}.post-block .post-title{margin:.65em 0;color:var(--textColor);font-size:1.5em}.post-block .post-info{color:#7f8c8d}.post-block .post-info .read-time{text-align:right}.post-content h2,.post-content h4{position:relative;margin:1em 0}.post-content h2 :before,.post-content h4 :before{content:"#";color:#42b983;position:absolute;left:-.7em;top:-4px;font-size:1.2em;font-weight:700}.post-content h4 :before{content:">"}.post-content h2{font-size:22px}.post-content h4{font-size:18px}.post-content a{color:#42b983;word-break:break-all}main.container{margin:2em 10px}@media screen and (min-width:900px){.wrap{width:900px;margin:0 auto}header{padding:20px 60px}}@media screen and (max-width:900px){.wrap{width:100%}header{min-height:50px;padding:2px 2px;position:fixed;z-index:10000;border-radius:15px;left:50%;-webkit-transform:translateX(-50%);transform:translateX(-50%);width:-webkit-fit-content;width:-moz-fit-content;width:fit-content}header a.logo-link,header ul.nav.nav-list{float:none;display:inline;text-align:center}header li.nav-list-item{padding:10px 5px}header .logo-link img{height:20px;vertical-align:sub}header .flag-icon{height:20px;width:20px}header{background-color:rgba(255,255,255,.9)}@supports ((-webkit-backdrop-filter:blur(2em)) or (backdrop-filter:blur(2em))){header{background-color:rgba(255,255,255,.3);-webkit-backdrop-filter:blur(10px);backdrop-filter:blur(10px)}}main.container{padding-top:2em}main.container{margin:0 20px}.post-content h2,.post-content h4{max-width:300px;left:15px}}@font-face{font-family:sourcesanspro;src:url(/font/sourcesanspro.woff2) format("woff2"),url(/font/sourcesanspro.woff) format("woff");font-weight:400;font-style:normal}</style><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Fashion-MNIST: Year In Review · Han Xiao Tech Blog - Neural Search & AI Engineering</title><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content="@hxiao"><meta name="twitter:creator" content="@hxiao"><meta name="description" content="It’s been one year since I released the Fashion-MNIST dataset in Aug. 2017. As I wrote in the README.md, Fashion-MNIST is intended to serve as a drop- ... · Han Xiao"><meta property="og:title" content="Fashion-MNIST: Year In Review · Han Xiao Tech Blog - Neural Search &amp; AI Engineering"><meta property="og:description" content="It’s been one year since I released the Fashion-MNIST dataset in Aug. 2017. As I wrote in the README.md, Fashion-MNIST is intended to serve as a drop- ... · Han Xiao"><meta property="og:url" content="https://hanxiao.io/2018/09/28/Fashion-MNIST-Year-In-Review/"><meta property="og:image" content="https://hanxiao.io/2018/09/28/Fashion-MNIST-Year-In-Review//8c38a09f.png"><meta property="og:type" content="article"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/myavatar.png"><link rel="alternate" type="application/rss+xml" title="Han Xiao Tech Blog - Neural Search &amp; AI Engineering" href="https://hanxiao.io/atom.xml"><!-- - use css preload trick--><link rel="preload" href="/css/apollo.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="/css/apollo.css"></noscript><script>/*! loadCSS. [c]2017 Filament Group, Inc. MIT License */
/* This file is meant as a standalone workflow for
- testing support for link[rel=preload]
- enabling async CSS loading in browsers that do not support rel=preload
- applying rel preload css once loaded, whether supported or not.
*/
(function( w ){
    "use strict";
    // rel=preload support test
    if( !w.loadCSS ){
        w.loadCSS = function(){};
    }
    // define on the loadCSS obj
    var rp = loadCSS.relpreload = {};
    // rel=preload feature support test
    // runs once and returns a function for compat purposes
    rp.support = (function(){
        var ret;
        try {
            ret = w.document.createElement( "link" ).relList.supports( "preload" );
        } catch (e) {
            ret = false;
        }
        return function(){
            return ret;
        };
    })();

    // if preload isn't supported, get an asynchronous load by using a non-matching media attribute
    // then change that media back to its intended value on load
    rp.bindMediaToggle = function( link ){
        // remember existing media attr for ultimate state, or default to 'all'
        var finalMedia = link.media || "all";

        function enableStylesheet(){
            link.media = finalMedia;
        }

        // bind load handlers to enable media
        if( link.addEventListener ){
            link.addEventListener( "load", enableStylesheet );
        } else if( link.attachEvent ){
            link.attachEvent( "onload", enableStylesheet );
        }

        // Set rel and non-applicable media type to start an async request
        // note: timeout allows this to happen async to let rendering continue in IE
        setTimeout(function(){
            link.rel = "stylesheet";
            link.media = "only x";
        });
        // also enable media after 3 seconds,
        // which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
        setTimeout( enableStylesheet, 3000 );
    };

    // loop through link elements in DOM
    rp.poly = function(){
        // double check this to prevent external calls from running
        if( rp.support() ){
            return;
        }
        var links = w.document.getElementsByTagName( "link" );
        for( var i = 0; i < links.length; i++ ){
            var link = links[ i ];
            // qualify links to those with rel=preload and as=style attrs
            if( link.rel === "preload" && link.getAttribute( "as" ) === "style" && !link.getAttribute( "data-loadcss" ) ){
                // prevent rerunning on link
                link.setAttribute( "data-loadcss", true );
                // bind listeners to toggle media back
                rp.bindMediaToggle( link );
            }
        }
    };

    // if unsupported, run the polyfill
    if( !rp.support() ){
        // run once at least
        rp.poly();

        // rerun poly on an interval until onload
        var run = w.setInterval( rp.poly, 500 );
        if( w.addEventListener ){
            w.addEventListener( "load", function(){
                rp.poly();
                w.clearInterval( run );
            } );
        } else if( w.attachEvent ){
            w.attachEvent( "onload", function(){
                rp.poly();
                w.clearInterval( run );
            } );
        }
    }


    // commonjs
    if( typeof exports !== "undefined" ){
        exports.loadCSS = loadCSS;
    }
    else {
        w.loadCSS = loadCSS;
    }
}( typeof global !== "undefined" ? global : this ) );</script><script id="mcjs">!function(c,h,i,m,p){m=c.createElement(h),p=c.getElementsByTagName(h)[0],m.async=1,m.src=i,p.parentNode.insertBefore(m,p)}(document,"script","https://chimpstatic.com/mcjs-connected/js/users/7da58fc9885cb85d4a9f0ad9a/987f901145f1749fd3e800e86.js");</script><link rel="search" type="application/opensearchdescription+xml" href="https://hanxiao.io/atom.xml" title="Han Xiao Tech Blog - Neural Search &amp; AI Engineering"></head><body><div class="reading-progress-bar"></div><div class="wrap"><header><ul class="nav nav-list"><li class="nav-list-item"><a href="/about/" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="https://www.linkedin.com/in/hxiao87" target="_blank" class="nav-list-link">LINKEDIN</a></li><li class="nav-list-item"><a href="https://x.com/hxiao" target="_blank" class="nav-list-link">X</a></li><li class="nav-list-item"><a href="https://github.com/hanxiao" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="https://scholar.google.com/citations?user=jp7swwIAAAAJ" target="_blank" class="nav-list-link">SCHOLAR</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">Fashion-MNIST: Year In Review</h1><div class="post-info">Sep 28, 2018 by &nbsp;&nbsp;&nbsp;<img src="/myavatar.png" alt="logo" width="18px" height="18px" style="vertical-align: sub;">&nbsp;<a href="/about" target="_blank" class="nav-list-link">Han Xiao - <i>ex</i> Engineering Lead @ Tencent AI Lab</a></div><div class="post-info"><div class="read-time">◷&nbsp;&nbsp;&nbsp; 14  min read</div></div><div class="post-content"><p>It’s been one year since I released <a href="https://github.com/zalandoresearch/fashion-mnist" target="_blank" rel="noopener">the Fashion-MNIST dataset</a> in Aug. 2017. As I wrote in the <a href="https://github.com/zalandoresearch/fashion-mnist" target="_blank" rel="noopener"><code>README.md</code></a>, Fashion-MNIST is intended to serve as a <em>drop-in replacement</em> for the original MNIST dataset, helping people to benchmark and understand machine learning algorithms. Over a year, I have seen a great deal of trends and developments in the machine learning<a id="more"></a> field towards this direction. The dataset received much love from researchers, engineers, students and enthusiasts in the community. </p>
<p>On the web today, you can find thousands of discussions, code snippets, tutorials about Fashion-MNIST from all over the world. <a href="https://github.com/search?q=fashion-mnist" target="_blank" rel="noopener">On Github</a>, Fashion-MNIST has collected 4000 stars; it is referred in more than 400 repositories, 1000 commits and 7000 code snippets. <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=fashion-mnist&amp;btnG=&amp;oq=fas" target="_blank" rel="noopener">On Google Scholar</a>, more than 250 academic research papers conduct their experiments on Fashion-MNIST. It is even mentioned in <a href="https://www.sciencemag.org/" target="_blank" rel="noopener">the Science Magazine from AAAS</a>. <a href="https://www.kaggle.com/zalando-research/fashionmnist" target="_blank" rel="noopener">On Kaggle</a>, it is one of the most popular datasets accompanied with more than 300 Kernels. All mainstream deep learning frameworks have native support for Fashion-MNIST. With just one line of <code>import</code> and you are ready to go.</p>
<p>Needless to say, Fashion-MNIST is a successful project. In this post, I will make a brief review of the important milestones it has achieved over a year. </p>
<h4><span id="table-of-content">Table of Content</span></h4><!-- toc -->
<ul>
<li><a href="#where-it-all-starts">Where it all starts</a></li>
<li><a href="#in-academics">In Academics</a><ul>
<li><a href="#top-ai-institutes-love-fashion-mnist">Top AI Institutes Love Fashion-MNIST</a></li>
<li><a href="#usa-and-china-dominate-the-publications">USA and China Dominate the Publications</a></li>
<li><a href="#top-tier-conferences-love-fashion-mnist">Top-tier Conferences Love Fashion-MNIST</a></li>
<li><a href="#fashion-mnist-is-on-science">Fashion-MNIST is on Science!</a></li>
<li><a href="#gan-researchers-love-fashion-mnist">GAN Researchers love Fashion-MNIST</a></li>
</ul>
</li>
<li><a href="#in-community">In Community</a></li>
<li><a href="#summary">Summary</a></li>
</ul>
<!-- tocstop -->
<h2><span id="where-it-all-starts">Where it all starts</span></h2><p>It was an ordinary summer day at Zalando Research. My boss <a href="https://www.linkedin.com/in/rolandvollgraf/" target="_blank" rel="noopener">Roland Vollgraf</a> asked me to survey generative networks and implement one or two to get familiar with this topic. So I did it and <a href="/2017/08/16/Why-I-use-raw-rnn-Instead-of-dynamic-rnn-in-Tensorflow-So-Should-You-0/" title="tested my implementation on MNIST.">tested my implementation on MNIST.</a> But I got bored quickly: the task is too trivial on MNIST, as all generated images seem equally good disregarding the model complexity. Thus, I decide to make it more challenging by throwing random fashion images from our inhouse SKU database into the model. This of course requires a new input pipeline to load and transform the data properly, which I was too lazy to write. So I preprocessed those fashion images offline and stored them in the MNIST format. That’s the very first version of Fashion-MNIST.</p>
<p>While playing with this new toy, I kept improving its quality and later made it as an inner source project. Encouraged by my former colleagues <a href="https://www.linkedin.com/in/kashif-rasul/" target="_blank" rel="noopener">Kashif Rasul</a> and <a href="https://www.linkedin.com/in/lauri-apple-csm/" target="_blank" rel="noopener">Lauri Apple</a>, we finally decided to publish it on Github &amp; arXiv. The rest of the story is probably known to you: the dataset was first discussed in reddit <code>r/MachineLearning</code>, then quickly spread its popularity over HackerNews, Github, Twitter, Facebook and other social networks. Few days later, even Yann LeCun himself posted Fashion-MNIST on his Facebook page:</p>
<iframe src="https://www.facebook.com/plugins/post.php?href=https%3A%2F%2Fwww.facebook.com%2Fyann.lecun%2Fposts%2F10154714939492143&width=500" width="500" height="616" style="border:none;overflow:hidden" scrolling="no" frameborder="0" allowtransparency="true" allow="encrypted-media"></iframe>

<p>I’d like to give special thanks to <a href="https://www.linkedin.com/in/lauri-apple-csm/" target="_blank" rel="noopener">Lauri Apple</a>, who encouraged and supported me during this journey. In the beginning, I was sometimes mocked for the idea of “replacing MNIST”, they don’t believe the community will accept it. Lauri stood out and told me that the people who actually make change are the ones who believe that change is possible.</p>
<h2><span id="in-academics">In Academics</span></h2><p>Today, as I write this post, 260 academic papers either cite or conduct their experiments on Fashion-MNIST <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=fashion-mnist&amp;btnG=&amp;oq=fas" target="_blank" rel="noopener">according to Google Scholar.</a> After some cleaning and quick read-through, I collected 247 of them and <a href="https://docs.google.com/spreadsheets/d/1cGX7Juedn_KVUgjDk298v5uUjc_wPk930tKyEoZhTQM/edit?usp=sharing" target="_blank" rel="noopener">put into a spreadsheet</a>. So who wrote those papers? What are their research problems? Where are they published? In the sequel, I will share those interesting statistics with you.</p>
<h4><span id="top-ai-institutes-love-fashion-mnist">Top AI Institutes Love Fashion-MNIST</span></h4><p>The figure below shows the number of publications per institute. Note, if one publication is co-authored by multiple institutes, I add one for each of them. Multiple authors from the same institute are counted only once for one paper. Institutes that have the same parent are grouped together, e.g. Google Research, Google Brain, Deep Mind are grouped into “Google”; Max Planck Institute of Informatics, Intelligent Systems and Quantum Optics are grouped into “Max Planck Institute”. For the sake of clarity, I made a cut-off at 3. The complete list of institutes can be found in <a href="https://docs.google.com/spreadsheets/d/1cGX7Juedn_KVUgjDk298v5uUjc_wPk930tKyEoZhTQM/edit?usp=sharing" target="_blank" rel="noopener">here</a>.</p>
<img src="/2018/09/28/Fashion-MNIST-Year-In-Review/a6500de3.png">
<p>One can find all top AI institutes from North America,  Asia and Europe in this list. Among them, Google is at the top with 9 unique publications using Fashion-MNIST in the experiment, followed by University of Cambridge with 7; IBM Research, Université de Montréal, Peking University and University of California Los Angeles with 6 publications, respectively. One can also find other tech companies from the full list, e.g. Facebook (2), Telefónica Research (2) Uber (1), Apple (1), Samsung (1), Huawei (1) and Twitter (1). Furthermore, there are some computer vision startups worked with Fashion-MNIST and published very interesting results.</p>
<h4><span id="usa-and-china-dominate-the-publications">USA and China Dominate the Publications</span></h4><p>If we group the publications by country, then we get the following results. Again, if one publication is co-authored by multiple countries, I add one for each of them. A cut-off is made at 4. The complete list of countries can be found <a href="https://docs.google.com/spreadsheets/d/1cGX7Juedn_KVUgjDk298v5uUjc_wPk930tKyEoZhTQM/edit?usp=sharing" target="_blank" rel="noopener">here</a>.</p>
<img src="/2018/09/28/Fashion-MNIST-Year-In-Review/48907035.png">
<p>USA is taking the absolute majority with 94 unique publications, followed by China with 44. Researchers from Canada, UK and Germany also show strong interests in Fashion-MNIST. These five countries contributes ~50% of the publications. Overall, researchers from 38 different countries have used Fashion-MNIST in their publications.</p>
<p>Despite the ongoing trade war and AI competition between US and China, these two countries have quite some collaboration in AI research. Over a year, they have co-authored 10 publications with Fashion-MNIST, more than any other countries. The next pair is UK and Germany, with 4 co-authored publications. The complete list of country-country collaboration can be found here.</p>
<p>Politics aside, I’d love to see more and more collaboration between countries. Especially with AI technology outracing institutions, cooperation between countries will be crucial.</p>
<h4><span id="top-tier-conferences-love-fashion-mnist">Top-tier Conferences Love Fashion-MNIST</span></h4><p>Of course, publications are not measured by the quantity but by the quality. So where are these papers published? Are they good? The next figure gives you an idea about it. Note, only accepted papers are counted. Papers that under review of some conference/journal are not included in this list. The complete list of conferences/jounals can be found <a href="https://docs.google.com/spreadsheets/d/1cGX7Juedn_KVUgjDk298v5uUjc_wPk930tKyEoZhTQM/edit?usp=sharing" target="_blank" rel="noopener">here</a>.</p>
<img src="/2018/09/28/Fashion-MNIST-Year-In-Review/781a7147.png">
<p>Most of the papers are published in this year (2018). One can find top-tier conferences such as NIPS, ICLR, ICML, AAAI, ECCV from the list. In NIPS 2018, there are 17 accepted papers using Fashion-MNIST in the experiment. This number is zero for NIPS 2017, which makes totally sense as Fashion-MNIST was not exist by the submission deadline in May 2017. But for NIPS workshops 2017, of which the deadline are in Nov., we saw researchers using Fashion-MNIST in their papers already.</p>
<p>Besides the conference publications, there are also some journal publications using Fashion-MNIST, including Journal of Machine Learning Research (2), Neurocomputing (2), Nature Communications (1), Science (1) and other high impact journals.</p>
<h4><span id="fashion-mnist-is-on-science">Fashion-MNIST is on Science!</span></h4><img src="/2018/09/28/Fashion-MNIST-Year-In-Review/b08ff24c.png">
<p>That’s right, <em>the</em> Science, from AAAS.</p>
<p>In the paper <a href="http://innovate.ee.ucla.edu/wp-content/uploads/2018/07/2018-optical-ml-neural-network.pdf" target="_blank" rel="noopener">“All-optical machine learning using diffractive deep neural networks”</a>, a research team from UCLA builds an all-optical diffractive deep neural network architecture and 3D-printed it. So it can classify fashion images <strong>at the speed of light</strong>!</p>
<p>The next figure shows five different trained layers of MNIST and Fashion-MNIST, respectively. These layers can be physically created, where each point on a given layer either transmits or reflects the incoming wave, representing an artificial neuron that is connected to other neurons of the following layers through optical diffraction. On the right side, an illustration of the corresponding 3D-printed all-optical network is shown. </p>
<img src="/2018/09/28/Fashion-MNIST-Year-In-Review/22ec55b4.png">
<p>During testing, the 3D-printed network is placed in the  environment below. The classification criterion is to find the detector with the maximum optical signal.</p>
<img src="/2018/09/28/Fashion-MNIST-Year-In-Review/ad5cb67a.png">
<h4><span id="gan-researchers-love-fashion-mnist">GAN Researchers love Fashion-MNIST</span></h4><p>Generative adversarial networks (GANs) have been at the forefront of research on deep neural networks in the last couple of years. GANs have been used for image generation, image processing and many other applications, often leading to state of the art results. It is not surprising that GAN researchers would love Fashion-MNIST dataset: it is light and small; no extra data loader is needed; more complicated and diverse patterns comparing to MNIST, it is an ideal dataset for validating a quick hack. The next figure shows the publications grouped by keywords. For publications without provided keywords, I read through the abstract and related work section and then extract few keywords manually. A cut-off is made at 5. A full list of keywords can be found <a href="https://docs.google.com/spreadsheets/d/1cGX7Juedn_KVUgjDk298v5uUjc_wPk930tKyEoZhTQM/edit?usp=sharing" target="_blank" rel="noopener">here</a>.</p>
<img src="/2018/09/28/Fashion-MNIST-Year-In-Review/7aff570f.png">
<p>To Fashion-MNIST, novel learning algorithms and regularization techniques are also heat topics, where researchers often do sanity-check first on standard datasets. Take Capsule Networks for example, ever since it is published, there is much discussion about whether it could actually work as intended. People question the design decisions made in making CapsNets work on MNIST. Does it generalize to other dataset? Forget about ImageNet, does it work on Fashion-MNIST (same size, same format as MNIST)? Over a year, there has been 9 new publications of CapsNet using Fashion-MNIST in the experiment. They proposed novel routing algorithms to encode the spatial relationships more robustly.</p>
<p>Since 2018, architecture search has gained quite a bit of popularity. The problem arises from the topology selection of neural networks, which has always been a difficult manual task for both researchers and practitioners. Google recently started to offer AutoML as a part of its cloud service for automated machine learning. It can design models that achieve accuracies on par with state-of-art models designed by machine learning experts. For researchers who study the architecture search, they can experiment the algorithms more exhaustively on standard dataset such as Fashion-MNIST. One can also confirm this trend by looking at the number of architecture search publications using Fashion-MNIST.</p>
<h2><span id="in-community">In Community</span></h2><p>Few weeks after I published the Fashion-MNIST, I was invited to <a href="https://www.slideshare.net/HanXiao4/fashionmnist-a-novel-image-dataset-for-benchmarking-machine-learning-algorithms" target="_blank" rel="noopener">Amazon Berlin and gave a talk about it</a>. During the QA session, one guy asked me if Fashion-MNIST would just wind up as an excuse for ML researchers who don’t want to work on real-world problems. After all, they can now argue that the model has been “double”-checked on two MNISTs.</p>
<p>The AI community never lets me down. Those high quality publications already speak for themselves. But besides the hard-core research, this community has found another good use of Fashion-MNIST: teaching. Nowadays, you can find thousands of tutorials, videos, slides, online courses, meetups, seminars, workshops using Fashion-MNIST in Machine Learning 101. Fashion-MNIST also improves the diversity of the community by attracting more young female students, enthusiasts, artists and designers. They find this dataset “cute” as the first impression, and therefore feel more engaged when learning ML/AI. In Sept. 2018 during Google Developer Days in China, speakers from Google used Fashion-MNIST in a Keras tutorial for introducing machine learning to hundreds of participants.</p>
<img src="/2018/09/28/Fashion-MNIST-Year-In-Review/c3a8c7de.png">
<h2><span id="summary">Summary</span></h2><p>Making progress in the ML/AI field requires joint and consistent efforts from the whole community. I’m really glad that Fashion-MNIST continuously contributes to the community in its own way: it improves the diversity of the community by attracting broader enthusiasts; it encourages researchers to design unequivocal experiments to understand algorithms better. In the future, I will continue maintaining Fashion-MNIST, making it accessible to more people around the globe. It doesn’t matter whether you are a researcher, a student, a professor or an enthusiast. You are welcome to use Fashion-MNIST in papers, meetups, hackathons, tutorials, classrooms or even <a href="https://www.amazon.com/Computer-Data-Science-Machine-Learning/dp/B072LSQ8H5" target="_blank" rel="noopener">on T-shirt</a> (God I should add this image to the dataset back in the day, MNISTception!). Sharing is caring, meanwhile happy hacking!</p>
<p>Papers mentioned in this post can be found in <a href="https://docs.google.com/spreadsheets/d/1cGX7Juedn_KVUgjDk298v5uUjc_wPk930tKyEoZhTQM/edit?usp=sharing" target="_blank" rel="noopener">this Google Spreadsheet</a>.</p>
</div></article></div></main><footer><div class="paginator"><a href="/2019/01/02/Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ/" class="prev">&nbsp;❮&nbsp;&nbsp;Serving Google BERT ...</a><a href="/2018/09/09/Dual-Ask-Answer-Network-for-Machine-Reading-Comprehension/" class="next">Machine Reading Comp...&nbsp;&nbsp;❯&nbsp;</a></div><div class="footer-section"><h2><img src="/flower.png" alt="Checkout this">Check out these posts too!</h2><div class="archive-readmore"><div style="display: flex; gap: 10px; align-items: flex-start;" class="post-item"><a href="/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0/"><img src="https://hanxiao.io/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0//9ba076d0.png" alt="Video Semantic Search in Large Scale using GNES and Tensorflow 2.0" style="width: 120px; height: 90px; object-fit: cover;"></a><div style="flex: 1;" class="book-title"><h5 style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; margin: 0;" class="post-title"><a href="/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0/" class="post-title-link">Video Semantic Search in Large Scale using GNES and Tensorflow 2.0</a></h5></div></div><div style="display: flex; gap: 10px; align-items: flex-start;" class="post-item"><a href="/2019/11/07/A-Better-Practice-for-Managing-extras-require-Dependencies-in-Python/"><img src="https://hanxiao.io/2019/11/07/A-Better-Practice-for-Managing-extras-require-Dependencies-in-Python//banner.png" alt="A Better Practice for Managing Many &lt;code&gt;extras_require&lt;/code&gt; Dependencies in Python" style="width: 120px; height: 90px; object-fit: cover;"></a><div style="flex: 1;" class="book-title"><h5 style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; margin: 0;" class="post-title"><a href="/2019/11/07/A-Better-Practice-for-Managing-extras-require-Dependencies-in-Python/" class="post-title-link">A Better Practice for Managing Many <code>extras_require</code> Dependencies in Python</a></h5></div></div><div style="display: flex; gap: 10px; align-items: flex-start;" class="post-item"><a href="/2019/10/18/GNES-Flow-a-Pythonic-Way-to-Build-Cloud-Native-Neural-Search-Pipelines/"><img src="https://hanxiao.io/2019/10/18/GNES-Flow-a-Pythonic-Way-to-Build-Cloud-Native-Neural-Search-Pipelines//gnes-flow-banner.png" alt="GNES Flow: a Pythonic Way to Build Cloud-Native Neural Search Pipelines" style="width: 120px; height: 90px; object-fit: cover;"></a><div style="flex: 1;" class="book-title"><h5 style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; margin: 0;" class="post-title"><a href="/2019/10/18/GNES-Flow-a-Pythonic-Way-to-Build-Cloud-Native-Neural-Search-Pipelines/" class="post-title-link">GNES Flow: a Pythonic Way to Build Cloud-Native Neural Search Pipelines</a></h5></div></div><div style="display: flex; gap: 10px; align-items: flex-start;" class="post-item"><a href="/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/"><img src="https://hanxiao.io/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond//gnes-team-1600.JPG" alt="Generic Neural Elastic Search: From &lt;code&gt;bert-as-service&lt;/code&gt; and Go Way Beyond" style="width: 120px; height: 90px; object-fit: cover;"></a><div style="flex: 1;" class="book-title"><h5 style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; margin: 0;" class="post-title"><a href="/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/" class="post-title-link">Generic Neural Elastic Search: From <code>bert-as-service</code> and Go Way Beyond</a></h5></div></div></div></div><div class="copyright"><p>© 2017 - 2025 <a href="https://hanxiao.io">Han Xiao</a>. <img src="/by-nc-sa.svg" alt="Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License." class="image"></p></div></footer></div><link rel="stylesheet" href="/css/post/katex.min.css"><!--link(rel="stylesheet", href=url_for("css/post/gitment.css"))--><script src="/js/katex.min.js"></script><script src="/js/auto-render.min.js"></script><script>renderMathInElement(
    document.body,
    {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "\\[", right: "\\]", display: true},
            {left: "$", right: "$", display: false},
            {left: "\\(", right: "\\)", display: false}
        ]
    }
);</script><!--script(src=url_for("js/gitment.browser.js"))--><!--script(src=url_for("js/gitment.loader.js"))--><script src="/js/jquery-3.4.1.min.js"></script><script src="/js/reading_progress.min.js"></script><script src="/js/highlighter.min.js"></script><script src="/js/init-highlighter.js"></script><script async src="https://www.google-analytics.com/analytics.js"></script><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create',"UA-52114253-1",'auto');ga('send','pageview');</script></body></html>