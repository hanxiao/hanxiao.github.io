<!DOCTYPE html><html lang="en"><style>html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}article,header,main{display:block}a{background-color:transparent}h1{font-size:2em;margin:.67em 0}img{border:0}body,html{width:100%;height:100%}html{width:100%;height:100vh;display:flex;flex-direction:column;justify-content:center;align-items:center;background:var(--bgColor);--bgColor:#fff;--textColor:#2c3e50;--bg2ndColor:none;--bg3rdColor:#f8f8f8;--preCodeColor:#525252;--imgOpacity:1.0}@media (prefers-color-scheme:dark){html{background:var(--bgColor);--bgColor:#121212;--textColor:#fff;--bg2ndColor:#fff;--bg3rdColor:#332940;--preCodeColor:#f8f8f8;--imgOpacity:0.5}}body{margin:0;color:var(--textColor);font-size:18px;line-height:1.6;background-color:var(--bgColor);font-family:sourcesanspro,'Helvetica Neue',Arial,sans-serif}ul.nav{margin:0;padding:0;list-style-type:none}ul{margin:1rem 0}a{color:var(--textColor);text-decoration:none}.flag-icon{height:25px;width:25px;display:inline;border-radius:50%;vertical-align:sub}.icon_item{padding-left:5px!important;padding-right:5px!important}.reading-progress-bar{background:#42b983;display:block;height:2px;left:0;position:fixed;top:0;width:0;z-index:10001}header{min-height:60px}header .logo-link{float:left}header .nav{float:right;left:80px}header .logo-link img{height:60px}header .nav-list-item{display:inline-block;padding:19px 10px}header .nav-list-item a{line-height:1.4}@media screen and (max-width:900px){header .nav-list-item a{font-size:12px}}@media screen and (min-width:900px){header .nav-list-item a{font-size:18px}}.post{padding-top:1em}.post-block .post-title{margin:.65em 0;color:var(--textColor);font-size:1.5em}.post-block .post-info{color:#7f8c8d}.post-block .post-info .read-time{text-align:right}.post-content h2,.post-content h4{position:relative;margin:1em 0}.post-content h2 :before,.post-content h4 :before{content:"#";color:#42b983;position:absolute;left:-.7em;top:-4px;font-size:1.2em;font-weight:700}.post-content h4 :before{content:">"}.post-content h2{font-size:22px}.post-content h4{font-size:18px}.post-content a{color:#42b983;word-break:break-all}main.container{margin:2em 10px}@media screen and (min-width:900px){.wrap{width:900px;margin:0 auto}header{padding:20px 60px}}@media screen and (max-width:900px){.wrap{width:100%}header{min-height:50px;padding:2px 2px;position:fixed;z-index:10000;border-radius:15px;left:50%;-webkit-transform:translateX(-50%);transform:translateX(-50%);width:-webkit-fit-content;width:-moz-fit-content;width:fit-content}header a.logo-link,header ul.nav.nav-list{float:none;display:inline;text-align:center}header li.nav-list-item{padding:10px 5px}header .logo-link img{height:20px;vertical-align:sub}header .flag-icon{height:20px;width:20px}header{background-color:rgba(255,255,255,.9)}@supports ((-webkit-backdrop-filter:blur(2em)) or (backdrop-filter:blur(2em))){header{background-color:rgba(255,255,255,.3);-webkit-backdrop-filter:blur(10px);backdrop-filter:blur(10px)}}main.container{padding-top:2em}main.container{margin:0 20px}.post-content h2,.post-content h4{max-width:300px;left:15px}}@font-face{font-family:sourcesanspro;src:url(/font/sourcesanspro.woff2) format("woff2"),url(/font/sourcesanspro.woff) format("woff");font-weight:400;font-style:normal}</style><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> 4 Sequence Encoding Blocks You Must Know Besides RNN/LSTM in Tensorflow · Han Xiao Tech Blog - Neural Search & AI Engineering</title><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content="@hxiao"><meta name="twitter:creator" content="@hxiao"><meta name="description" content="Understanding human language is a challenging task for computers, as they were originally designed for crunching numbers. To let computers comprehend ... · Han Xiao"><meta property="og:title" content="4 Sequence Encoding Blocks You Must Know Besides RNN/LSTM in Tensorflow · Han Xiao Tech Blog - Neural Search &amp; AI Engineering"><meta property="og:description" content="Understanding human language is a challenging task for computers, as they were originally designed for crunching numbers. To let computers comprehend ... · Han Xiao"><meta property="og:url" content="https://hanxiao.io/2018/06/24/4-Encoding-Blocks-You-Need-to-Know-Besides-LSTM-RNN-in-Tensorflow/"><meta property="og:image" content="https://hanxiao.io/2018/06/24/4-Encoding-Blocks-You-Need-to-Know-Besides-LSTM-RNN-in-Tensorflow//f3020ac6.png"><meta property="og:type" content="article"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/myavatar.png"><link rel="alternate" type="application/rss+xml" title="Han Xiao Tech Blog - Neural Search &amp; AI Engineering" href="https://hanxiao.io/atom.xml"><!-- - use css preload trick--><link rel="preload" href="/css/apollo.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="/css/apollo.css"></noscript><script>/*! loadCSS. [c]2017 Filament Group, Inc. MIT License */
/* This file is meant as a standalone workflow for
- testing support for link[rel=preload]
- enabling async CSS loading in browsers that do not support rel=preload
- applying rel preload css once loaded, whether supported or not.
*/
(function( w ){
    "use strict";
    // rel=preload support test
    if( !w.loadCSS ){
        w.loadCSS = function(){};
    }
    // define on the loadCSS obj
    var rp = loadCSS.relpreload = {};
    // rel=preload feature support test
    // runs once and returns a function for compat purposes
    rp.support = (function(){
        var ret;
        try {
            ret = w.document.createElement( "link" ).relList.supports( "preload" );
        } catch (e) {
            ret = false;
        }
        return function(){
            return ret;
        };
    })();

    // if preload isn't supported, get an asynchronous load by using a non-matching media attribute
    // then change that media back to its intended value on load
    rp.bindMediaToggle = function( link ){
        // remember existing media attr for ultimate state, or default to 'all'
        var finalMedia = link.media || "all";

        function enableStylesheet(){
            link.media = finalMedia;
        }

        // bind load handlers to enable media
        if( link.addEventListener ){
            link.addEventListener( "load", enableStylesheet );
        } else if( link.attachEvent ){
            link.attachEvent( "onload", enableStylesheet );
        }

        // Set rel and non-applicable media type to start an async request
        // note: timeout allows this to happen async to let rendering continue in IE
        setTimeout(function(){
            link.rel = "stylesheet";
            link.media = "only x";
        });
        // also enable media after 3 seconds,
        // which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
        setTimeout( enableStylesheet, 3000 );
    };

    // loop through link elements in DOM
    rp.poly = function(){
        // double check this to prevent external calls from running
        if( rp.support() ){
            return;
        }
        var links = w.document.getElementsByTagName( "link" );
        for( var i = 0; i < links.length; i++ ){
            var link = links[ i ];
            // qualify links to those with rel=preload and as=style attrs
            if( link.rel === "preload" && link.getAttribute( "as" ) === "style" && !link.getAttribute( "data-loadcss" ) ){
                // prevent rerunning on link
                link.setAttribute( "data-loadcss", true );
                // bind listeners to toggle media back
                rp.bindMediaToggle( link );
            }
        }
    };

    // if unsupported, run the polyfill
    if( !rp.support() ){
        // run once at least
        rp.poly();

        // rerun poly on an interval until onload
        var run = w.setInterval( rp.poly, 500 );
        if( w.addEventListener ){
            w.addEventListener( "load", function(){
                rp.poly();
                w.clearInterval( run );
            } );
        } else if( w.attachEvent ){
            w.attachEvent( "onload", function(){
                rp.poly();
                w.clearInterval( run );
            } );
        }
    }


    // commonjs
    if( typeof exports !== "undefined" ){
        exports.loadCSS = loadCSS;
    }
    else {
        w.loadCSS = loadCSS;
    }
}( typeof global !== "undefined" ? global : this ) );</script><script id="mcjs">!function(c,h,i,m,p){m=c.createElement(h),p=c.getElementsByTagName(h)[0],m.async=1,m.src=i,p.parentNode.insertBefore(m,p)}(document,"script","https://chimpstatic.com/mcjs-connected/js/users/7da58fc9885cb85d4a9f0ad9a/987f901145f1749fd3e800e86.js");</script><link rel="search" type="application/opensearchdescription+xml" href="https://hanxiao.io/atom.xml" title="Han Xiao Tech Blog - Neural Search &amp; AI Engineering"></head><body><div class="reading-progress-bar"></div><div class="wrap"><header><ul class="nav nav-list"><li class="nav-list-item"><a href="/about/" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="https://www.linkedin.com/in/hxiao87" target="_blank" class="nav-list-link">LINKEDIN</a></li><li class="nav-list-item"><a href="https://x.com/hxiao" target="_blank" class="nav-list-link">X</a></li><li class="nav-list-item"><a href="https://github.com/hanxiao" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="https://scholar.google.com/citations?user=jp7swwIAAAAJ" target="_blank" class="nav-list-link">SCHOLAR</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">4 Sequence Encoding Blocks You Must Know Besides RNN/LSTM in Tensorflow</h1><div class="post-info">Jun 24, 2018 by &nbsp;&nbsp;&nbsp;<img src="/myavatar.png" alt="logo" width="18px" height="18px" style="vertical-align: sub;">&nbsp;<a href="/about" target="_blank" class="nav-list-link">Han Xiao - <i>ex</i> Engineering Lead @ Tencent AI Lab</a></div><div class="post-info"><div class="read-time">◷&nbsp;&nbsp;&nbsp; 17  min read</div></div><div class="post-content"><h2><span id="background">Background</span></h2><p>Understanding human language is a challenging task for computers, as they were originally designed for crunching numbers. To let computers comprehend text as humans do, one needs to encode the complexities and nuances of natural language into numbers. For many years, <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network" target="_blank" rel="noopener">recurrent neural networks (RNN)</a> or <a href="https://en.wikipedia.org/wiki/Long_short-term_memory" target="_blank" rel="noopener">long-short term memory (LSTM)</a> was the way to solve sequence encoding problem. They have indeed accomplished amazing results in many applications, e.g. machine translation and voice recognition. As for me, they were the first solution that comes to my mind when facing an NLP problem. You can find my previous posts about RNN/LSTM in <a href="/2018/04/21/Teach-Machine-to-Comprehend-Text-and-Answer-Question-with-Tensorflow/" title="here">here</a>, <a href="/2018/01/10/Build-Cross-Lingual-End-to-End-Product-Search-using-Tensorflow/" title="here">here</a>, <a href="/2017/08/16/Why-I-use-raw-rnn-Instead-of-dynamic-rnn-in-Tensorflow-So-Should-You-0/" title="and here">and here</a>. </p>
<p>Now at Tencent AI Lab, however, I’m <em>sunsetting</em> RNN/LSTM in my team. Why? Because they are <strong>computationally expensive</strong>, aka slow! The recursive nature <a id="more"></a>requires it to maintain a hidden state of the entire past that prevents parallel computation within a sequence. Also, don’t forget that sequence encoding is often just a low-level subtask of our ultimate goal (e.g. translation, question-answering). The upper layers are waiting for the encodes so that they can carry on more interesting tasks. Thus, it is not worth spending so much time on the encoding task itself.</p>
<p>All in all, we need our sequence encoding to be fast. A faster sequence encoder boosts the development cycle of our team, enabling us to find the optimal model by exploring more architectures and hyperparameters, which is crucial for bringing AI into production.</p>
<p>In this article, I will introduce four sequence encoding blocks as a replacement of RNN/LSTM. I’d like to call them “block”, as they all consist of multiple layers or even other blocks. They can be further nested, duplicated or joined to compose a more power super-block, improving a network’s capacity and expressivity.</p>
<p>The code mentioned in this post <a href="https://github.com/hanxiao/encoding-blocks" target="_blank" rel="noopener">can be found on my Github</a>. Readers are welcome to contribute or give suggestions.</p>
<h4><span id="table-of-content">Table of Content</span></h4><!-- toc -->
<ul>
<li><a href="#what-is-a-sequence-encoding-block">What is a Sequence Encoding Block</a></li>
<li><a href="#pooling-block">Pooling Block</a><ul>
<li><a href="#average-pooling-and-max-pooling">Average Pooling and Max Pooling</a></li>
<li><a href="#hierarchical-pooling">Hierarchical Pooling</a></li>
<li><a href="#attentive-pooling">Attentive Pooling</a></li>
<li><a href="#final-touch">Final Touch</a></li>
</ul>
</li>
<li><a href="#cnn-block">CNN Block</a><ul>
<li><a href="#causal-convolutions">Causal Convolutions</a></li>
</ul>
</li>
<li><a href="#multi-resolution-cnn-block">Multi-Resolution CNN Block</a></li>
<li><a href="#multi-head-multi-resolution-cnn-block">Multi-Head Multi-Resolution CNN Block</a><ul>
<li><a href="#implementation-trick">Implementation Trick</a></li>
</ul>
</li>
<li><a href="#summary">Summary</a></li>
</ul>
<!-- tocstop -->
<h2><span id="what-is-a-sequence-encoding-block">What is a Sequence Encoding Block</span></h2><p>As you may find people referring the encoding task differently, such as fusing, composing and aggregating. I’d like to first clarify the definition of an encoding block, so we are on the same page.</p>
<p>Considering a text sequence (e.g. a sentence, a paragraph, a document) represented as $\mathbf{s}:=\{w_1, w_2, \ldots, w_L\,|\,w_l \in \mathcal{V}\}$, where $w$ denotes a word from the vocabulary $\mathcal{V}$ and $L$ denotes the length of the sequence. Let $\mathbf{M}$ be a word embedding matrix of size $|\mathcal{V}|\times D$, each row represents a word in $\mathbb{R}^D$. The word embedding matrix $\mathbf{M}$ can be obtained with <a href="https://nlp.stanford.edu/projects/glove/" target="_blank" rel="noopener">GloVe</a> or <a href="https://en.wikipedia.org/wiki/Word2vec" target="_blank" rel="noopener">Word2Vec</a>, or be initialized randomly and learned from scratch. Either way, one can now represent $\mathbf{s}$ as a $L\times D$ matrix via <code>tf.embedding_lookup</code> on $\mathbf{M}$. The goal of a sequence encoding block is to find a function $\mathbb{R}^{L\times D}\rightarrow\mathbb{R}^{D^\prime}$, which encodes information of a variable length sentence into a fixed-length vector representation. The $D^\prime$-dimensional output vector is then fed to the upper layers for the final task. The next figure illustrates this idea.</p>
<img src="/2018/06/24/4-Encoding-Blocks-You-Need-to-Know-Besides-LSTM-RNN-in-Tensorflow/fc9bf65e.png">
<h2><span id="pooling-block">Pooling Block</span></h2><h4><span id="average-pooling-and-max-pooling">Average Pooling and Max Pooling</span></h4><p>Let’s start with the simplest ones: reducing the dimension $L$ to 1 by taking the average/maximum over each of the $D$ dimensions. They can be (almost) easily implemented <code>tf.reduce_mean</code> and <code>tf.reduce_max</code>, respectively. I say <em>almost</em>, as one has to be careful with the padded symbols when dealing with a batch of sequences of different lengths. Those paddings should <em>never</em> be involved in the calculation. Tiny details, yet people often forget about them.</p>
<p>These two strategies are also called <em>average pooling</em> and <em>max pooling</em>, respectively. Both of them make perfect sense intuitively: in average pooling the meaning of a sentence is represented by all words contained; whereas in max pooling the meaning is represented by a small number of keywords and their salient features only.</p>
<h4><span id="hierarchical-pooling">Hierarchical Pooling</span></h4><p>To keep the best part of two sides, one can combine these two strategies together: via concatenating, i.e. <code>tf.concat</code>, or <em>hierarchical pooling</em>. In hierarchical pooling, we first define a small sliding window. The window moves across the dimension $L$. Then we do <code>reduce_mean</code> at every move and collect the result, finally do <code>reduce_max</code> on all results. Here I show a Tensorflow implementation of the hierarchical pooling strategy:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reshape_seqs</span><span class="params">(x, avg_window_size=<span class="number">3</span>, **kwargs)</span>:</span></span><br><span class="line">    B = tf.shape(x)[<span class="number">0</span>]</span><br><span class="line">    L = tf.cast(tf.shape(x)[<span class="number">1</span>], tf.float32)</span><br><span class="line">    D = x.get_shape().as_list()[<span class="number">-1</span>]</span><br><span class="line">    b = tf.transpose(x, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line">    extra_pads = tf.cast(tf.ceil(L / avg_window_size) * avg_window_size - L, tf.int32)</span><br><span class="line">    c = tf.pad(b, tf.concat([tf.zeros([<span class="number">2</span>, <span class="number">2</span>], dtype=tf.int32), [[<span class="number">0</span>, extra_pads]]], axis=<span class="number">0</span>))</span><br><span class="line">    <span class="keyword">return</span> tf.reshape(c, [B, D, avg_window_size, <span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># avg pooling with mask, be careful with all zero mask</span></span><br><span class="line">d = tf.reduce_sum(reshape_seqs(seqs, **kwargs), axis=<span class="number">2</span>) / \</span><br><span class="line">    tf.reduce_sum(reshape_seqs(tf.expand_dims(mask + <span class="number">1e-10</span>, axis=<span class="number">-1</span>), **kwargs), axis=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># max pooling</span></span><br><span class="line">pooled = tf.reduce_max(d, axis=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>The trick here is to reshape the batch sequence in advance from the shape <code>[B,L,D]</code> to <code>[B,D,W,L&#39;/W]</code>, where <code>W</code> is the size of the sliding window, and <code>L&#39;</code> is the length with <em>extra paddings</em> so that it can be divided by <code>W</code>. As a consequence, one can simply apply average pooling on the third dimension followed by a max pooling on the last dimension. Just be careful with padded symbols and the mask.</p>
<p>Comparing to max and average pooling, hierarchical pooling captures more local spatial information because of the preservation of the word-order in each window. The size of the window is a trade-off between the max and average pooling. One may also relate it with <a href="https://arxiv.org/abs/1509.01626" target="_blank" rel="noopener">bag-of-n-grams described in this paper</a>.</p>
<h4><span id="attentive-pooling">Attentive Pooling</span></h4><p>We can also let the model automatically pick up the important words to represent a sentence. One solution is attentive pooling. Specifically, I introduce a $D$-dimensional global variable $\mathbf{q}$. We first multiply the sequence with $\mathbf{q}$ and get a “score” of length $L$. Each element of the score represents the importance of a word to the sequence. Finally, a weighted sum is conducted over all words according to the score. The figure below illustrates the attention procedure.</p>
<img src="/2018/06/24/4-Encoding-Blocks-You-Need-to-Know-Besides-LSTM-RNN-in-Tensorflow/e191ea49.png">
<p>The code is simple as follows:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">B = tf.shape(seqs)[<span class="number">0</span>]</span><br><span class="line">D = tf.shape(seqs)[<span class="number">-1</span>]</span><br><span class="line">query = tf.tile(get_var(<span class="string">'query'</span>, shape=[<span class="number">1</span>, D, <span class="number">1</span>]), [B, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">score = tf.nn.softmax(minus_mask(tf.matmul(seqs, query) / (D ** <span class="number">0.5</span>), mask), axis=<span class="number">1</span>)  <span class="comment"># [B,L,1]</span></span><br><span class="line">pooled = tf.reduce_sum(score * seqs, axis=<span class="number">1</span>)  <span class="comment"># [B, D]</span></span><br></pre></td></tr></table></figure></p>
<p>Again, the mask and padded symbols need to be taken care of before computing <code>softmax</code>. This can be done by subtracting a large number using <code>minus_mask</code>. Besides the scaled matrix multiplication mentioned above, there are other attention functions. Interested readers are recommended to read <a href="https://arxiv.org/abs/1711.07341" target="_blank" rel="noopener">Sect. 4.3 in this paper</a>. </p>
<h4><span id="final-touch">Final Touch</span></h4><p>As our first block, I don’t want to make it too complicated. So let’s finish it with some final touch. Here I add layer normalization and dropout to improve the robustness and generalization ability. Finally, a complete pooling block looks like the following:</p>
<img src="/2018/06/24/4-Encoding-Blocks-You-Need-to-Know-Besides-LSTM-RNN-in-Tensorflow/761e3373.png">
<p>As we shall see later, the pooling block is a cornerstone for more sophisticated encoding blocks.</p>
<h2><span id="cnn-block">CNN Block</span></h2><p>There has been excellent work with CNNs on NLP before, as in <a href="http://arxiv.org/abs/1103.0398" target="_blank" rel="noopener">“Natural Language Processing (almost) from Scratch”</a>. The basic idea is to use a 1-D kernel with a small width and slide it over the entire sequence. In some sense, this idea is very similar to the hierarchical pooling as I described in the last section. The major difference is that CNN’s kernel is not a simple average: it is parameterized and needs to be learned from the data. The code below describes how to apply convolution on a sequence:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">seq_dim = seqs.get_shape().as_list()[<span class="number">-1</span>]</span><br><span class="line">W = get_var(<span class="string">'conv_W'</span>, [kernel_width, seq_dim, num_filters])</span><br><span class="line">b = get_var(<span class="string">'conv_bias'</span>, [num_filters])</span><br><span class="line">conv = tf.nn.conv1d(seqs, W, stride=<span class="number">1</span>, padding=<span class="string">'SAME'</span>)</span><br><span class="line">output = tf.nn.relu(tf.nn.bias_add(conv, b))</span><br></pre></td></tr></table></figure></p>
<p>Note that the output has the shape <code>[L, D]</code> when using <code>padding=&#39;SAME&#39;</code>, meaning the full input sequence length $L$ is retained. To introduce some non-linearities, I add a bias to it and wrap it with a ReLU activation function. One may also understand it as a “gate”, keeping only the salient values for the output. Finally, the output is fed to a pooling block for transforming it into a fixed-length vector. The next figure illustrates this process:</p>
<img src="/2018/06/24/4-Encoding-Blocks-You-Need-to-Know-Besides-LSTM-RNN-in-Tensorflow/06a2a68b.png">
<h4><span id="causal-convolutions">Causal Convolutions</span></h4><p>One may argue that the above CNN block is <em>cheating</em> as it leverages the future information. To make it clear, assuming we have a block with <code>kernel_width=3</code> then the convoluted output of time $l$ depends on the input of $l-1$, $l$ and $l+1$. This is different than standard RNN/LSTM, where each step $l$ only depends on the past ($\leq l$) not on the future. </p>
<p>To make it more RNN/LSTM-like, I introduce <em>causal convolutions</em>, in which an output at time $l$ is convolved only with elements from time $l$ and earlier in the input. This can be simply done by padding <code>kernel_width -1</code> empty symbols to the left of the sequence. The next figure illustrates this idea vs. the standard convolution.</p>
<img src="/2018/06/24/4-Encoding-Blocks-You-Need-to-Know-Besides-LSTM-RNN-in-Tensorflow/09bdc17c.png">
<p>The code is simply as follows:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">padding = (kernel_width - <span class="number">1</span>)</span><br><span class="line">padded_seq = tf.pad(seqs, [[<span class="number">0</span>, <span class="number">0</span>], [padding, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line">output = tf.layers.conv1d(padded_seq, num_filters, kernel_width, activation=tf.nn.relu)</span><br></pre></td></tr></table></figure>
<p>Shifting the sequence to change time-dependency of the model is nothing new. It is essentially the same tweak as described in the <a href="https://www.cs.toronto.edu/~hinton/absps/waibelTDNN.pdf" target="_blank" rel="noopener">time delay neural network proposed nearly 30 years ago</a>. </p>
<h2><span id="multi-resolution-cnn-block">Multi-Resolution CNN Block</span></h2><p>The hyperparameter <code>kernel_width</code> controls the resolution of the CNN block. When applying it on a sequence, <code>kernel_width</code> corresponds to the context size $n$ of n-gram. An ideal kernel width is important but also task-dependent. As a workaround, one can simply employ multiple CNN blocks with different kernel widths and concatenate their outputs together. The code is presented below:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">outputs = [Pool_Block(seqs, mask, **kwargs)] <span class="keyword">if</span> highway <span class="keyword">else</span> []</span><br><span class="line"><span class="keyword">for</span> fw <span class="keyword">in</span> kernel_widths:</span><br><span class="line">    outputs.append(CNN_Block(seqs, mask, fw, scope=<span class="string">'cnn_%d'</span> % fw, **kwargs))</span><br><span class="line">output = tf.concat(outputs, axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p>
<p>Note that I let the input bypass all CNN blocks when <code>highway=True</code>. This shortcut is particularly useful when training a very deep network, as the gradient can be propagated back to the low-level layers more easily through this highway. The next figure visualizes this block:</p>
 <img src="/2018/06/24/4-Encoding-Blocks-You-Need-to-Know-Besides-LSTM-RNN-in-Tensorflow/e0e8ffff.png">
<p>One may also stack CNN blocks on top of each other to increase the context size. For instance, stacking 5 CNN blocks with kernel width of 3 results in an input field of 11 words, i.e., each output depends on 11 input words. Personally, I found such deep encoders are more difficult to train well in practice, and its improvement on NLP tasks is marginal. <a href="https://arxiv.org/abs/1611.02344" target="_blank" rel="noopener">But who am I to say that?</a></p>
<h2><span id="multi-head-multi-resolution-cnn-block">Multi-Head Multi-Resolution CNN Block</span></h2><p>The term “multi-head” comes from Google’s paper <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention is All You Need</a>, where it stacks multiple attention bocks for a single input and creates a powerful but fast-to-train model. The parameters of those attention blocks are <em>not</em> shared. The expectation is that each “head” can capture a unique aspect of the sequence, thus together they can improve the expressivity of the model.</p>
<p>Inspired by this idea, here I stack multiple MR-CNN blocks to build a multi-head multi-resolution CNN block. A fully expanded view of this block is depicted in the next figure. A global highway is probably not necessary here as each MR-CNN block already has its own highway.</p>
<img src="/2018/06/24/4-Encoding-Blocks-You-Need-to-Know-Besides-LSTM-RNN-in-Tensorflow/91797b86.png">
<h4><span id="implementation-trick">Implementation Trick</span></h4><p>While we keep stacking and nesting blocks into one, the number of hyperparameters keeps increasing. This also means the function identifier contains quite some arguments. Take a MH-MR-CNN block as an example,</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">encode = MH_MR_CNN(seq, mask,</span><br><span class="line">                  num_heads=num_heads,</span><br><span class="line">                  kernel_sizes=kernel_sizes,</span><br><span class="line">                  num_units=args.fuse_num_units,</span><br><span class="line">                  dropout=ph_dropout_keep_prob,</span><br><span class="line">                  norm_by_layer=args.global_layer_norm,</span><br><span class="line">                  reduce=args.fuse_reduce,</span><br><span class="line">                  highway=args.fuse_highway)</span><br></pre></td></tr></table></figure>
<p>Note that most of the arguments are <em>not</em> for MH-MR-CNN except <code>num_heads</code>. Nonetheless, they still need to be carried all the way back to the deepest block, i.e. the pooling block. A naive way to implement this is writing all hyperparameters in all function identifiers, passing them via explicit function arguments. A better way is to use <code>**kwargs</code>, a Python feature which allows you to pass a variable number of arguments to a function. If a function can’t handle some of those arguments, then you can simply forward <code>**kwargs</code> to its sub-functions, etc. The code below demonstrates this trick:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Pool</span><span class="params">(seqs, mask, reduce=<span class="string">'concat_mean_max'</span>, norm_by_layer=False, dropout_keep=<span class="number">1.</span>, **kwargs)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">CNN</span><span class="params">(seqs, mask, kernel_size, num_units=None, **kwargs)</span>:</span></span><br><span class="line">    <span class="comment"># ... POOL(seqs, mask, **kwargs) ...</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MR_CNN</span><span class="params">(seqs, mask, kernel_sizes=<span class="params">(<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span>, highway=False, **kwargs)</span>:</span></span><br><span class="line">    <span class="comment"># ... CNN(seqs, mask, **kwargs)  ...</span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MH_MR_CNN</span><span class="params">(seqs, mask, num_heads=<span class="number">3</span>, **kwargs)</span>:</span></span><br><span class="line">    <span class="comment"># ... MR_CNN(seqs, mask, **kwargs) ...</span></span><br></pre></td></tr></table></figure>
<p>As one can see, by using <code>**kwargs</code> each block only declares its own hyperparameters in the function identifier, resulting in a much cleaner and more readable code.</p>
<h2><span id="summary">Summary</span></h2><p>A sequence encoder is the cornerstone of many advanced AI applications, such as semantic search, question-answering, machine reading comprehension. Therefore, how to encode sequences good and fast is considered as a fundamental problem in the ML/NLP community. In this post, I have introduced four non-RNN encoding blocks which are extremely useful in practice. In some NLP task where long-term dependencies are not so important (e.g. classification), those non-RNN encoding blocks can perform competitively to recurrent alternatives and save you a lot of computational time. </p>
<p>The code mentioned in this post <a href="https://github.com/hanxiao/encoding-blocks" target="_blank" rel="noopener">can be found on my Github</a>.</p>
</div></article></div></main><footer><div class="paginator"><a href="/2018/09/09/Dual-Ask-Answer-Network-for-Machine-Reading-Comprehension/" class="prev">&nbsp;❮&nbsp;&nbsp;Machine Reading Comp...</a><a href="/2018/04/21/Teach-Machine-to-Comprehend-Text-and-Answer-Question-with-Tensorflow/" class="next">Teach Machine to Com...&nbsp;&nbsp;❯&nbsp;</a></div><div class="footer-section"><h2><img src="/flower.png" alt="Checkout this">Check out these posts too!</h2><div class="archive-readmore"><div style="display: flex; gap: 10px; align-items: flex-start;" class="post-item"><a href="/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0/"><img src="https://hanxiao.io/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0//9ba076d0.png" alt="Video Semantic Search in Large Scale using GNES and Tensorflow 2.0" style="width: 120px; height: 90px; object-fit: cover;"></a><div style="flex: 1;" class="book-title"><h5 style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; margin: 0;" class="post-title"><a href="/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0/" class="post-title-link">Video Semantic Search in Large Scale using GNES and Tensorflow 2.0</a></h5></div></div><div style="display: flex; gap: 10px; align-items: flex-start;" class="post-item"><a href="/2019/11/07/A-Better-Practice-for-Managing-extras-require-Dependencies-in-Python/"><img src="https://hanxiao.io/2019/11/07/A-Better-Practice-for-Managing-extras-require-Dependencies-in-Python//banner.png" alt="A Better Practice for Managing Many &lt;code&gt;extras_require&lt;/code&gt; Dependencies in Python" style="width: 120px; height: 90px; object-fit: cover;"></a><div style="flex: 1;" class="book-title"><h5 style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; margin: 0;" class="post-title"><a href="/2019/11/07/A-Better-Practice-for-Managing-extras-require-Dependencies-in-Python/" class="post-title-link">A Better Practice for Managing Many <code>extras_require</code> Dependencies in Python</a></h5></div></div><div style="display: flex; gap: 10px; align-items: flex-start;" class="post-item"><a href="/2019/10/18/GNES-Flow-a-Pythonic-Way-to-Build-Cloud-Native-Neural-Search-Pipelines/"><img src="https://hanxiao.io/2019/10/18/GNES-Flow-a-Pythonic-Way-to-Build-Cloud-Native-Neural-Search-Pipelines//gnes-flow-banner.png" alt="GNES Flow: a Pythonic Way to Build Cloud-Native Neural Search Pipelines" style="width: 120px; height: 90px; object-fit: cover;"></a><div style="flex: 1;" class="book-title"><h5 style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; margin: 0;" class="post-title"><a href="/2019/10/18/GNES-Flow-a-Pythonic-Way-to-Build-Cloud-Native-Neural-Search-Pipelines/" class="post-title-link">GNES Flow: a Pythonic Way to Build Cloud-Native Neural Search Pipelines</a></h5></div></div><div style="display: flex; gap: 10px; align-items: flex-start;" class="post-item"><a href="/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/"><img src="https://hanxiao.io/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond//gnes-team-1600.JPG" alt="Generic Neural Elastic Search: From &lt;code&gt;bert-as-service&lt;/code&gt; and Go Way Beyond" style="width: 120px; height: 90px; object-fit: cover;"></a><div style="flex: 1;" class="book-title"><h5 style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; margin: 0;" class="post-title"><a href="/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/" class="post-title-link">Generic Neural Elastic Search: From <code>bert-as-service</code> and Go Way Beyond</a></h5></div></div></div></div><div class="copyright"><p>© 2017 - 2025 <a href="https://hanxiao.io">Han Xiao</a>. <img src="/by-nc-sa.svg" alt="Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License." class="image"></p></div></footer></div><link rel="stylesheet" href="/css/post/katex.min.css"><!--link(rel="stylesheet", href=url_for("css/post/gitment.css"))--><script src="/js/katex.min.js"></script><script src="/js/auto-render.min.js"></script><script>renderMathInElement(
    document.body,
    {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "\\[", right: "\\]", display: true},
            {left: "$", right: "$", display: false},
            {left: "\\(", right: "\\)", display: false}
        ]
    }
);</script><!--script(src=url_for("js/gitment.browser.js"))--><!--script(src=url_for("js/gitment.loader.js"))--><script src="/js/jquery-3.4.1.min.js"></script><script src="/js/reading_progress.min.js"></script><script src="/js/highlighter.min.js"></script><script src="/js/init-highlighter.js"></script><script async src="https://www.google-analytics.com/analytics.js"></script><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create',"UA-52114253-1",'auto');ga('send','pageview');</script></body></html>