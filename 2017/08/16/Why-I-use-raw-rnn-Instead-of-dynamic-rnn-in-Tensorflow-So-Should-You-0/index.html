<!DOCTYPE html><html lang="en"><style>html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}article,header,main{display:block}a{background-color:transparent}h1{font-size:2em;margin:.67em 0}img{border:0}body,html{width:100%;height:100%}html{width:100%;height:100vh;display:flex;flex-direction:column;justify-content:center;align-items:center;background:var(--bgColor);--bgColor:#fff;--textColor:#2c3e50;--bg2ndColor:none;--bg3rdColor:#f8f8f8;--preCodeColor:#525252;--imgOpacity:1.0}@media (prefers-color-scheme:dark){html{background:var(--bgColor);--bgColor:#121212;--textColor:#fff;--bg2ndColor:#fff;--bg3rdColor:#332940;--preCodeColor:#f8f8f8;--imgOpacity:0.5}}body{margin:0;color:var(--textColor);font-size:18px;line-height:1.6;background-color:var(--bgColor);font-family:sourcesanspro,'Helvetica Neue',Arial,sans-serif}ul.nav{margin:0;padding:0;list-style-type:none}ul{margin:1rem 0}a{color:var(--textColor);text-decoration:none}.flag-icon{height:25px;width:25px;display:inline;border-radius:50%;vertical-align:sub}.icon_item{padding-left:5px!important;padding-right:5px!important}.reading-progress-bar{background:#42b983;display:block;height:2px;left:0;position:fixed;top:0;width:0;z-index:10001}header{min-height:60px}header .logo-link{float:left}header .nav{float:right;left:80px}header .logo-link img{height:60px}header .nav-list-item{display:inline-block;padding:19px 10px}header .nav-list-item a{line-height:1.4}@media screen and (max-width:900px){header .nav-list-item a{font-size:12px}}@media screen and (min-width:900px){header .nav-list-item a{font-size:18px}}.post{padding-top:1em}.post-block .post-title{margin:.65em 0;color:var(--textColor);font-size:1.5em}.post-block .post-info{color:#7f8c8d}.post-block .post-info .read-time{text-align:right}.post-content h2,.post-content h4{position:relative;margin:1em 0}.post-content h2 :before,.post-content h4 :before{content:"#";color:#42b983;position:absolute;left:-.7em;top:-4px;font-size:1.2em;font-weight:700}.post-content h4 :before{content:">"}.post-content h2{font-size:22px}.post-content h4{font-size:18px}.post-content a{color:#42b983;word-break:break-all}main.container{margin:2em 10px}@media screen and (min-width:900px){.wrap{width:900px;margin:0 auto}header{padding:20px 60px}}@media screen and (max-width:900px){.wrap{width:100%}header{min-height:50px;padding:2px 2px;position:fixed;z-index:10000;border-radius:15px;left:50%;-webkit-transform:translateX(-50%);transform:translateX(-50%);width:-webkit-fit-content;width:-moz-fit-content;width:fit-content}header a.logo-link,header ul.nav.nav-list{float:none;display:inline;text-align:center}header li.nav-list-item{padding:10px 5px}header .logo-link img{height:20px;vertical-align:sub}header .flag-icon{height:20px;width:20px}header{background-color:rgba(255,255,255,.9)}@supports ((-webkit-backdrop-filter:blur(2em)) or (backdrop-filter:blur(2em))){header{background-color:rgba(255,255,255,.3);-webkit-backdrop-filter:blur(10px);backdrop-filter:blur(10px)}}main.container{padding-top:2em}main.container{margin:0 20px}.post-content h2,.post-content h4{max-width:300px;left:15px}}@font-face{font-family:sourcesanspro;src:url(/font/sourcesanspro.woff2) format("woff2"),url(/font/sourcesanspro.woff) format("woff");font-weight:400;font-style:normal}</style><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Why I Use raw_rnn Instead of dynamic_rnn in Tensorflow and So Should You · Han Xiao Tech Blog - Neural Search & AI Engineering</title><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content="@hxiao"><meta name="twitter:creator" content="@hxiao"><meta name="description" content="Recently I am working on search queries with Tensorflow. Given an arbitrary query, I am interested in two things: the probability of it and the vector ... · Han Xiao"><meta property="og:title" content="Why I Use raw_rnn Instead of dynamic_rnn in Tensorflow and So Should You · Han Xiao Tech Blog - Neural Search &amp; AI Engineering"><meta property="og:description" content="Recently I am working on search queries with Tensorflow. Given an arbitrary query, I am interested in two things: the probability of it and the vector ... · Han Xiao"><meta property="og:url" content="https://hanxiao.io/2017/08/16/Why-I-use-raw-rnn-Instead-of-dynamic-rnn-in-Tensorflow-So-Should-You-0/"><meta property="og:image" content="https://hanxiao.io/2017/08/16/Why-I-use-raw-rnn-Instead-of-dynamic-rnn-in-Tensorflow-So-Should-You-0//cc3232c0.png"><meta property="og:type" content="article"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/myavatar.png"><link rel="alternate" type="application/rss+xml" title="Han Xiao Tech Blog - Neural Search &amp; AI Engineering" href="https://hanxiao.io/atom.xml"><!-- - use css preload trick--><link rel="preload" href="/css/apollo.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="/css/apollo.css"></noscript><script>/*! loadCSS. [c]2017 Filament Group, Inc. MIT License */
/* This file is meant as a standalone workflow for
- testing support for link[rel=preload]
- enabling async CSS loading in browsers that do not support rel=preload
- applying rel preload css once loaded, whether supported or not.
*/
(function( w ){
    "use strict";
    // rel=preload support test
    if( !w.loadCSS ){
        w.loadCSS = function(){};
    }
    // define on the loadCSS obj
    var rp = loadCSS.relpreload = {};
    // rel=preload feature support test
    // runs once and returns a function for compat purposes
    rp.support = (function(){
        var ret;
        try {
            ret = w.document.createElement( "link" ).relList.supports( "preload" );
        } catch (e) {
            ret = false;
        }
        return function(){
            return ret;
        };
    })();

    // if preload isn't supported, get an asynchronous load by using a non-matching media attribute
    // then change that media back to its intended value on load
    rp.bindMediaToggle = function( link ){
        // remember existing media attr for ultimate state, or default to 'all'
        var finalMedia = link.media || "all";

        function enableStylesheet(){
            link.media = finalMedia;
        }

        // bind load handlers to enable media
        if( link.addEventListener ){
            link.addEventListener( "load", enableStylesheet );
        } else if( link.attachEvent ){
            link.attachEvent( "onload", enableStylesheet );
        }

        // Set rel and non-applicable media type to start an async request
        // note: timeout allows this to happen async to let rendering continue in IE
        setTimeout(function(){
            link.rel = "stylesheet";
            link.media = "only x";
        });
        // also enable media after 3 seconds,
        // which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
        setTimeout( enableStylesheet, 3000 );
    };

    // loop through link elements in DOM
    rp.poly = function(){
        // double check this to prevent external calls from running
        if( rp.support() ){
            return;
        }
        var links = w.document.getElementsByTagName( "link" );
        for( var i = 0; i < links.length; i++ ){
            var link = links[ i ];
            // qualify links to those with rel=preload and as=style attrs
            if( link.rel === "preload" && link.getAttribute( "as" ) === "style" && !link.getAttribute( "data-loadcss" ) ){
                // prevent rerunning on link
                link.setAttribute( "data-loadcss", true );
                // bind listeners to toggle media back
                rp.bindMediaToggle( link );
            }
        }
    };

    // if unsupported, run the polyfill
    if( !rp.support() ){
        // run once at least
        rp.poly();

        // rerun poly on an interval until onload
        var run = w.setInterval( rp.poly, 500 );
        if( w.addEventListener ){
            w.addEventListener( "load", function(){
                rp.poly();
                w.clearInterval( run );
            } );
        } else if( w.attachEvent ){
            w.attachEvent( "onload", function(){
                rp.poly();
                w.clearInterval( run );
            } );
        }
    }


    // commonjs
    if( typeof exports !== "undefined" ){
        exports.loadCSS = loadCSS;
    }
    else {
        w.loadCSS = loadCSS;
    }
}( typeof global !== "undefined" ? global : this ) );</script><script id="mcjs">!function(c,h,i,m,p){m=c.createElement(h),p=c.getElementsByTagName(h)[0],m.async=1,m.src=i,p.parentNode.insertBefore(m,p)}(document,"script","https://chimpstatic.com/mcjs-connected/js/users/7da58fc9885cb85d4a9f0ad9a/987f901145f1749fd3e800e86.js");</script><link rel="search" type="application/opensearchdescription+xml" href="https://hanxiao.io/atom.xml" title="Han Xiao Tech Blog - Neural Search &amp; AI Engineering"></head><body><div class="reading-progress-bar"></div><div class="wrap"><header><ul class="nav nav-list"><li class="nav-list-item"><a href="/about/" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="https://www.linkedin.com/in/hxiao87" target="_blank" class="nav-list-link">LINKEDIN</a></li><li class="nav-list-item"><a href="https://x.com/hxiao" target="_blank" class="nav-list-link">X</a></li><li class="nav-list-item"><a href="https://github.com/hanxiao" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="https://scholar.google.com/citations?user=jp7swwIAAAAJ" target="_blank" class="nav-list-link">SCHOLAR</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">Why I Use <code>raw_rnn</code> Instead of <code>dynamic_rnn</code> in Tensorflow and So Should You</h1><div class="post-info">Aug 16, 2017 by &nbsp;&nbsp;&nbsp;<img src="/myavatar.png" alt="logo" width="18px" height="18px" style="vertical-align: sub;">&nbsp;<a href="/about" target="_blank" class="nav-list-link">Han Xiao - <i>ex</i> Senior Research Scientist @ Zalando Research</a></div><div class="post-info"><div class="read-time">◷&nbsp;&nbsp;&nbsp; 21  min read</div></div><div class="post-content"><h2><span id="background">Background</span></h2><p>Recently I am working on search queries with Tensorflow. Given an arbitrary query, I am interested in two things: the probability of it and the vector representation of it. After a discussion with my team, I started with a simple generative neural network called <a href="https://arxiv.org/pdf/1605.02226.pdf" target="_blank" rel="noopener"><em>Neural Autoregressive Distribution Estimation</em></a> (NADE), which is designed for modeling the distribution $p(\mathbf{x})$ of input vector $\mathbf{x}$. While I was implementing NADE using <a href="https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn" target="_blank" rel="noopener"><code>dynamic_rnn</code></a> Tensorflow API, I found it is kind of <strong>hacky especially for sampling</strong>. Later, I resorted to a low-level API called <code>raw_rnn</code>, which turns out to be more powerful for generative recurrent neural network.</p>
<p>In this article, I want to highlight the advantages of <code>raw_rnn</code> over <code>dynamic_rnn</code>. In particular, I will describe how to use this API to implement <strong>NADE</strong> and <strong>a sequence-to-sequence model</strong>. Although <code>raw_rnn</code> is <a id="more"></a>described as <a href="https://www.tensorflow.org/api_docs/python/tf/nn/raw_rnn" target="_blank" rel="noopener">a low-level API in the Tensorflow documentation</a>, its usage is quite straightforward. Most importantly, a sampling process implemented by <code>raw_rnn</code> is <strong>much more efficient</strong> comparing to <code>dynamic_rnn</code> (e.g. <a href="https://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html" target="_blank" rel="noopener">this</a>, <a href="http://deeplearningathome.com/2016/10/Text-generation-using-deep-recurrent-neural-networks.html" target="_blank" rel="noopener">this</a> and <a href="http://blog.otoro.net/2015/12/12/handwriting-generation-demo-in-tensorflow/" target="_blank" rel="noopener">this</a>). It is also easier for debugging. Given that there are not so many articles on the web about using <code>raw_rnn</code> in practice, I hope this article could shed a light on this API and give you some guidance when implementing an RNN next time.   </p>
<h2><span id="recap-of-neural-autoregressive-distribution-estimation">Recap of <em>Neural Autoregressive Distribution Estimation</em></span></h2><p><a href="https://arxiv.org/pdf/1605.02226.pdf" target="_blank" rel="noopener">NADE</a> is one of the most classic generative neural networks developed in 2011. It allows one to model probability density of the input data (i.e. $p(\mathbf{x})$) using neural network. Comparing to nowadays most popular Generative Adversarial Networks (GAN), NADE models the density explicitly in a tractable way. </p>
<p>Formally, let $\mathbf{x}$ be a $D$-dimensional input vector. The distribution $p(\mathbf{x})$ can be factorized as a product of conditional distributions:<br>$$p(\mathbf{x}) = p(x_1)p(x_2|x_1)\ldots p(x_D|x_{D-1},\ldots, x_1)$$ </p>
<p>As you can see, there is a recurrent pattern in the equation above. In fact, this is true for any $D$-dimensional distribution. Given a data set $ \{ \mathbf{x}_{i}\}_{i=1}^{N}$, NADE can be trained by maximizing the likelihood, or equivalently by minimizing the average negative log-likelihood:</p>
<p>$$\frac{1}{N}\sum_{n=1}^N-\log p(\mathbf{x}^{(n)}) =\frac{1}{N} \sum_{n=1}^N\sum_{d=1}^D-\log p(x_d^{(n)}|\mathbf{x}_{&lt;d}^{(n)})$$</p>
<p>Here $\mathbf{x}_{&lt;d}$ contains the first $(d-1)$ dimensions. Depending on the characteristics of the input data $\mathbf{x}$, there are many ways to parametrize those conditional distributions. For example, if your data contains only binary values then <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution" target="_blank" rel="noopener">Bernoulli distribution</a> is an obivious choice, i.e. $p(x_d|\mathbf{x}_{&lt;d}) = \mathrm{Bernoulli}(p_d)$. For the input data that contains only $k$ discrete values, you may want to use <a href="https://en.wikipedia.org/wiki/Multinomial_distribution" target="_blank" rel="noopener">Multinomial distribution</a>, i.e. $p(x_d|\mathbf{x}_{&lt;d}) = \mathrm{Multinomal}(p_{d1},\ldots, p_{dk})$. Of course, different distributions will introduce different number of parameters, e.g. $p_d$ in Bernoulli or $p_{d1},\ldots, p_{dk}$ in Multinomial. Now the remaining task is to model these parameters with some neural network, which will be described in the next section. </p>
<h2><span id="nade-lstm-using-raw_rnn">NADE-LSTM using <code>raw_rnn</code></span></h2><p>The original NADE paper uses a simple feed-forward network to model the parameter of the distribution. But we don’t have to strictly follow their idea. Here I choose LSTM as the recurrent cell as it doesn’t suffer from <a href="http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf" target="_blank" rel="noopener">the exploding and vanishing gradients</a> problem.</p>
<p>Let’s start with a simple Bernoulli distribution. The following figure depicts the NADE-LSTM network with binary pixel as the input (at the bottom). The pixels on the top are the <strong>sampled</strong> outputs from the Bernoulli distribution.</p>
<img src="/2017/08/16/Why-I-use-raw-rnn-Instead-of-dynamic-rnn-in-Tensorflow-So-Should-You-0/a8b9dbde.png">
<p>One may observe that, I add a <em>shared</em> dense layer for mapping the LSTM cell’s output to the parameter of the distribution. To make sure that the output of the dense layer  is a valid Bernoulli parameter, i.e. $0&lt;p_i&lt;1$, I also put a sigmoid activation on it. This can be done via:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.layers.dense(tf.zeros([<span class="number">1</span>, cell.output_size]), units=<span class="number">1</span>,</span><br><span class="line">                activation=tf.nn.sigmoid,</span><br><span class="line">                name=<span class="string">'RNN/output_to_p'</span>, reuse=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>Note that although all cells share the same dense layer, their corresponding Bernoulli distributions are <em>not</em> the same due to the information carried on the recurrent structure.</p>
<p>To use <code>raw_rnn</code> to create an RNN, the key is to write your own loop function <code>loop_fn</code>. Generally, you need to clarify these four things in the <code>loop_fn</code>:</p>
<ul>
<li>What is the initial state or the input to the cell? (<code>if cell_output is None:</code> branch)</li>
<li>What is the next state or the next input to the cell? (<code>else</code> branch)</li>
<li>What information do you want to propagate through the network? (<code>loop_state.write</code>)</li>
<li>When will the recurrence stop? (<code>elements_finished</code>)</li>
</ul>
<p>Below is the code for <code>loop_fn</code> in NADE-LSTM, I will explain it in details. I also recommend readers to check <a href="https://www.tensorflow.org/api_docs/python/tf/nn/raw_rnn" target="_blank" rel="noopener">the official documentation</a> for reference.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">output_ta = tf.TensorArray(size=<span class="number">784</span>, dtype=tf.float32)  <span class="comment"># store trained/sampled pixel</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loop_fn</span><span class="params">(time, cell_output, cell_state, loop_state)</span>:</span></span><br><span class="line">    emit_output = cell_output  <span class="comment"># == None for time == 0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> cell_output <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># time=0, everything here will be used for initialization only</span></span><br><span class="line">        next_cell_state = cell_init_state</span><br><span class="line">        next_pixel = cell_init_pixel</span><br><span class="line">        next_loop_state = output_ta</span><br><span class="line">    <span class="keyword">else</span>:  </span><br><span class="line">        <span class="comment"># pass the last state to the next</span></span><br><span class="line">        next_cell_state = cell_state</span><br><span class="line">        next_pixel = tf.cond(is_training,</span><br><span class="line">                         <span class="keyword">lambda</span>: inputs_ta.read(time - <span class="number">1</span>),</span><br><span class="line">                         <span class="keyword">lambda</span>: tf.contrib.distributions.Bernoulli(</span><br><span class="line">                             probs=tf.nn.sigmoid(tf.layers.dense(cell_output, <span class="number">1</span>, </span><br><span class="line">                                 name=<span class="string">'output_to_p'</span>, activation=tf.nn.sigmoid,</span><br><span class="line">                                 reuse=<span class="literal">True</span>)),</span><br><span class="line">                             dtype=tf.float32).sample())</span><br><span class="line">        next_loop_state = loop_state.write(time - <span class="number">1</span>, next_pixel)</span><br><span class="line"></span><br><span class="line">    elements_finished = (time &gt;= <span class="number">784</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (elements_finished, next_pixel, next_cell_state,</span><br><span class="line">            emit_output, next_loop_state)</span><br></pre></td></tr></table></figure></p>
<h3><span id="altering-input-to-the-cell">Altering Input to the Cell</span></h3><p>One of the biggest advantages of <code>raw_rnn</code> is that you can easily modify the next input to feed to the cell, whereas in <code>dynamic_rnn</code> the input is fixed and usually given the placeholder. This feature is <strong>extremely useful</strong> when you do sampling. For example, in the code snippet below, I implement the sampling procedure by conditioning on a boolean placeholder <code>is_training</code>:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">next_pixel = tf.cond(is_training,</span><br><span class="line">                     <span class="keyword">lambda</span>: inputs_ta.read(time - <span class="number">1</span>),</span><br><span class="line">                     <span class="keyword">lambda</span>: tf.contrib.distributions.Bernoulli(</span><br><span class="line">                         probs=tf.nn.sigmoid(tf.layers.dense(cell_output, <span class="number">1</span>, </span><br><span class="line">                             name=<span class="string">'output_to_p'</span>, activation=tf.nn.sigmoid,</span><br><span class="line">                             reuse=<span class="literal">True</span>)),</span><br><span class="line">                         dtype=tf.float32).sample())</span><br></pre></td></tr></table></figure></p>
<p>If <code>is_training</code> is <code>true</code>, then we read from the input placeholder given by <code>feed_dict</code>. Otherwise, we do sampling from a Bernoulli distribution and use the sample as the next input. The following figure visualizes the sampling procedure.</p>
<img src="/2017/08/16/Why-I-use-raw-rnn-Instead-of-dynamic-rnn-in-Tensorflow-So-Should-You-0/b891305e.png">
<p>As one can see, with <code>raw_rnn</code> there is no need to adapt or squeeze the graph for sampling. You just set <code>is_training</code> flag to false and that’s it. Moreover, the sampling is highly efficient as you only need to run <code>sess.run</code> once. Comparing to the most Tensorflow sampling implementations you can find on the web (e.g. <a href="https://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html" target="_blank" rel="noopener">this</a>, <a href="http://deeplearningathome.com/2016/10/Text-generation-using-deep-recurrent-neural-networks.html" target="_blank" rel="noopener">this</a> and <a href="http://blog.otoro.net/2015/12/12/handwriting-generation-demo-in-tensorflow/" target="_blank" rel="noopener">this</a>), they are roughly based on the following pattern:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">samples = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(max_time):</span><br><span class="line">    new_input = update_feed_dict(current_input, new_state)</span><br><span class="line">    preds, new_state = sess.run([node_sample, node_final_state], </span><br><span class="line">                                feed_dict=new_input)</span><br><span class="line">    samples.append(preds)</span><br></pre></td></tr></table></figure></p>
<p>, where <code>max_time</code> is the length of the sampled sequence. On MNIST data set, <code>max_time</code> is 784. The problem of this code is that it will call <code>sess.run()</code> <code>max_time</code> times, whereas each time only with a tiny job. If your graph computation is on GPU, then frequently alternating the context  between CPU and GPU is inefficient especially when <code>max_time</code> is large.</p>
<p>Here is another usage. Say if we want to learn the conditional density of pixels given class labels, i.e. $p(\mathbf{x}|y)$, where $y\in \{ 0, 1, \ldots, 9 \} $. We can simply let Y be a <code>tf.placeholder</code> and add the following line at the end of <code>loop_fn</code> scope:<br> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">next_pixel = tf.concat([next_pixel, Y], axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p>
<p>Isn’t that much simpler than adding/changing nodes in computation graph?</p>
<h3><span id="propagate-information-through-rnn-loop">Propagate Information through RNN Loop</span></h3><p>With <code>raw_rnn</code> you can propagate any information through the loop, or send it out to feed your downstream nodes. In the aforementioned NADE-LSTM example, I’m interested in what the network samples at each step, i.e. <code>next_pixel</code> drawed from the Bernoulli distribution. This is how I store it:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">output_ta = tf.TensorArray(size=<span class="number">784</span>, dtype=tf.float32)  <span class="comment"># store sampled pixels</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loop_fn</span><span class="params">(time, cell_output, cell_state, loop_state)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> cell_output <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment">#...</span></span><br><span class="line">        next_loop_state = output_ta</span><br><span class="line">    <span class="keyword">else</span>:  </span><br><span class="line">        <span class="comment">#...</span></span><br><span class="line">        next_loop_state = loop_state.write(time - <span class="number">1</span>, next_pixel)</span><br><span class="line">    <span class="comment">#...</span></span><br></pre></td></tr></table></figure></p>
<p>To read <code>output_ta</code> from an RNN:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.python.ops.rnn <span class="keyword">import</span> _transpose_batch_time</span><br><span class="line">_, _, loop_state_ta = tf.nn.raw_rnn(LSTMCell, loop_fn)</span><br><span class="line">X_sampled = _transpose_batch_time(loop_state_ta.stack())</span><br></pre></td></tr></table></figure></p>
<p>Note that <code>loop_state_ta</code> is a TensorArray and is in [time, batch, input_depth] shape. Therefore you need to first <code>stack()</code> it to make it a Tensor. Then transpose the batch and time dimensions of it to make it consistent with your input format. </p>
<p>What if I want to write more information at each step? No problem. Just make <code>output_ta</code> a tuple of TensorArray. Here is an example:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">output_ta = (tf.TensorArray(size=<span class="number">784</span>, dtype=tf.float32),  <span class="comment"># save sampled pixels</span></span><br><span class="line">             tf.TensorArray(size=<span class="number">784</span>, dtype=tf.float32))  <span class="comment"># save model loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loop_fn</span><span class="params">(time, cell_output, cell_state, loop_state)</span>:</span></span><br><span class="line">    <span class="comment">#...</span></span><br><span class="line">    next_loop_state = (loop_state[<span class="number">0</span>].write(time - <span class="number">1</span>, next_pixel),</span><br><span class="line">                       loop_state[<span class="number">1</span>].write(time - <span class="number">1</span>, logp_loss(cell_output, next_pixel)</span><br><span class="line">                                           <span class="keyword">if</span> save_memory <span class="keyword">else</span> <span class="number">1</span>))</span><br></pre></td></tr></table></figure></p>
<p>In the code above, I write the next pixel as well as the <em>aggregated</em>  log-probability loss to <code>output_ta</code> at each time step. One may argue that a more time-efficient way is to concatenate the whole sequence of the cell outputs in time, and then compute the loss on this batch-concatenated sequence. However, when the sequence is very long and the number of hidden units of the cell is large, such method won’t work due to the GPU memory limit. Imagine we use a LSTM with 512 hidden units on MNIST data (784-dim) and set with size of the batch to 400, then the overall output from the network will be a [400, 512, 784] <code>float32</code> Tensor, about 0.64 GB of memory. And did I mention you need to run it for validation set as well (which is often much bigger than 400)? In the example above, I use a boolean flag <code>save_memory</code>. If it is set to true, then I only keep a scalar value for the loss each time step and reduce them afterwards. This allows me to switch between a quick test (e.g. local debugging) and some real deals with big data and large network.</p>
<p>For those who are interested in details inside the RNN loop, and for those who simply want to debug by inspecting intermediate values, <code>raw_rnn</code> gives an elegant solution.</p>
<p>Now let’s have some fun. I do sampling after every epoch and check how NADE-LSTM captures the density of input pixels. Remember, all you need is letting <code>is_training=False</code>. Here is how it looks like:</p>
<img src="/2017/08/16/Why-I-use-raw-rnn-Instead-of-dynamic-rnn-in-Tensorflow-So-Should-You-0/mnist-bernoulli.gif">
<p>Without any modification on the algorithm, let’s apply it to <a href="https://github.com/zalandoresearch/fashion-mnist" target="_blank" rel="noopener">Fashion-MNIST</a> dataset, which is a direct drop-in replacement for the original MNIST.</p>
<img src="/2017/08/16/Why-I-use-raw-rnn-Instead-of-dynamic-rnn-in-Tensorflow-So-Should-You-0/fashion-mnist-bernoulli.gif">
<p>Results are not as good as the original MNIST. But that’s fine. After all, NADE is a simple autoregressive model. Besides, Fashion-MNIST <em>is more challenging</em> than the original MNIST.</p>
<h2><span id="a-sequence-to-sequence-model-for-query-embedding">A Sequence-to-Sequence Model for Query Embedding</span></h2><p>Before I wrap up this article, I want to give another example of a sequence-to-sequence model for query embedding. In this example, I will use both <code>dynamic_rnn</code> and <code>raw_rnn</code> to build the network, which nicely leverages the simplicity of <code>dynamic_rnn</code> and the flexibility of <code>raw_rnn</code>. If you are not familiar with the sequence-to-sequence model, I highly recommend you to read <a href="http://arxiv.org/pdf/1406.1078.pdf" target="_blank" rel="noopener">this paper</a>. If you already understand what I wrote in the previous sections, then please consider this as a small exercise.</p>
<p>Let’s start by drawing the architecture first.</p>
<img src="/2017/08/16/Why-I-use-raw-rnn-Instead-of-dynamic-rnn-in-Tensorflow-So-Should-You-0/cc3232c0.png">
<p>In general, our architecture follows the <strong>encoder-decoder pattern</strong>. Both encoder and decoder are character-based LSTM. The encoder observes the training data and receives input character by character. The last state from encoder is then fed to the decoder as initial state. The decoder <strong>does not</strong> observe the training data. It only receives the encoder’s  last output and state. Inside the loop of decoder, it keeps sampling and use the sample as next input to the LSTM cell. The network’s parameters are optimized by making the sampled sequence (orange letters in the <em>decoder box</em>) close to the input sequence (black letters in the <em>encoder box</em>), which can be quantified with sigmoid cross-entropy loss. Finally, the last output from encoder is used as our query embedding. </p>
<p>There is nothing really special about the encoder, thus I use <code>dynamic_rnn</code> to implement it:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'Encoder'</span>):</span><br><span class="line">    encoder_outputs, last_enc_state = tf.nn.dynamic_rnn(LSTMCell,</span><br><span class="line">                                                        inputs=X_embd,</span><br><span class="line">                                                        sequence_length=L,</span><br><span class="line">                                                        dtype=tf.float32)</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'Query_Embedding'</span>):</span><br><span class="line">    query_embedding = get_last_output(encoder_outputs, L, <span class="string">'embedding_vector'</span>)</span><br></pre></td></tr></table></figure></p>
<p>Here <code>X_embd</code> is the one-hot embedding for the input characters and <code>L</code> is a [batch_size] Tensor represents the length of each sequence in the batch. </p>
<p>For sampling and feeding the sample to the cell in the decoder, I use <code>raw_rnn</code>:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'Decoder'</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'Initial_GO_Input'</span>):</span><br><span class="line">        dummy_zero_input = tf.zeros(shape=[cur_batch_size,</span><br><span class="line">                                           cur_batch_depth], dtype=tf.float32,</span><br><span class="line">                                    name=<span class="string">'dummy_zero_input'</span>)</span><br><span class="line"></span><br><span class="line">    output_ta = tf.TensorArray(size=cur_batch_time, dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loop_fn</span><span class="params">(time, cell_output, cell_state, loop_state)</span>:</span></span><br><span class="line">        emit_output = cell_output  <span class="comment"># == None for time == 0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> cell_output <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            next_cell_state = last_enc_state</span><br><span class="line">            next_sampled_onehot = dummy_zero_input</span><br><span class="line">            next_loop_state = output_ta</span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># pass the last state to the next</span></span><br><span class="line">            next_cell_state = cell_state</span><br><span class="line">            next_sampled_input = get_sample(cell_output)  <span class="comment"># sampling from multinomial</span></span><br><span class="line">            next_sampled_onehot = tf.nn.embedding_lookup(embeddings, next_sampled_input)</span><br><span class="line"></span><br><span class="line">            next_loop_state = loop_state.write(time - <span class="number">1</span>, next_sampled_input)</span><br><span class="line"></span><br><span class="line">        elements_finished = (time &gt;= cur_batch_time)</span><br><span class="line">        next_input = next_sampled_onehot</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)</span><br><span class="line"></span><br><span class="line">    decoder_emit_ta, _, loop_state_ta = tf.nn.raw_rnn(LSTMCell, loop_fn)</span><br></pre></td></tr></table></figure></p>
<p>In <code>get_sample</code>, I use <code>tf.distributions.Categorical.sample</code> to draw from cell-specific multinomial distribution, which is parametrized by a dense layer of size [<code>num_hidden_units</code>, <code>num_chars</code>]. <code>get_sample</code> returns an integer Tensor representing int-indexed characters. This is very useful for me to visually check what the network is decoding, so I stroe it in <code>loop_state</code> and send it out at the end of the loop. On the other hand, the LSTM cell needs a vector representation as input. Thus, I transform the sample to one-hot embedding with <code>embedding_lookup</code> and use as next input to the cell.</p>
<p>Now let’s build the loss function and some metrics.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'Output'</span>):</span><br><span class="line">    decoder_emits = _transpose_batch_time(decoder_emit_ta.stack())</span><br><span class="line">    logits = get_logits(decoder_emits)</span><br><span class="line">    X_sampled_int = _transpose_batch_time(loop_state_ta.stack())</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'Decoder_Loss'</span>):</span><br><span class="line">    logp_loss = -tf.reduce_mean(tf.log(<span class="number">1e-6</span> + get_prob(decoder_emits, X)))</span><br><span class="line">    xentropy_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=X_embd, logits=logits))</span><br><span class="line">    sampled_acc = tf.reduce_mean(tf.cast(tf.equal(X, X_sampled_int), tf.float32))</span><br></pre></td></tr></table></figure></p>
<p>Readers may notice that this time I compute the loss function outside the <code>loop_fn</code> unlike what I wrote in the last section. This is because query sequences are short in general, much shorter than 784 (i.e. the MNIST dimensions) at least. In this case, computing the loss in batch gives better efficiency.<br>Finally, you can choose your favorite optimizer and minimizing <code>xentropy_loss</code> or <code>logp_loss</code>. For visualizing the training progress in a more intuitive way, I add <code>sampled_acc</code> for measuring the accuracy of sampled sequence.</p>
<p>I plot how these metrics change over time. One can observe that the sampled accuracy for training and validation set converge to 80%~90% eventually. This was made using LSTM with 64 hidden units. Increasing the capacity of LSTM would yield better accuracy.</p>
<img src="/2017/08/16/Why-I-use-raw-rnn-Instead-of-dynamic-rnn-in-Tensorflow-So-Should-You-0/140ed858.png">
<p>Now who wants to see the how the query embedding look like? In the following figures, I plot the 64-dimensional query embedding with blue and red points. Green points represent positive values, reds are negative. All values are in [-1, +1]. Note how embedding is robust to the misspelled words, and how it changes while I typing. </p>
<img src="/2017/08/16/Why-I-use-raw-rnn-Instead-of-dynamic-rnn-in-Tensorflow-So-Should-You-0/nikeperformance.gif">
<img src="/2017/08/16/Why-I-use-raw-rnn-Instead-of-dynamic-rnn-in-Tensorflow-So-Should-You-0/sportshirt.gif">
<img src="/2017/08/16/Why-I-use-raw-rnn-Instead-of-dynamic-rnn-in-Tensorflow-So-Should-You-0/14cf8845.png">
<h2><span id="pitfalls-of-raw_rnn">Pitfalls of <code>raw_rnn</code></span></h2><ul>
<li><p><code>raw_rnn</code> uses TensorArray for the input and outputs, in which Tensor must be in [time, batch_size, input_depth] shape. This is different from the shape we are familiar with, i.e. [batch_size, time, input_depth]. So don’t forget to transform your input into the correct format before feeding it to <code>raw_rnn</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">inputs_ta = tf.TensorArray(size=cur_batch_time, dtype=tf.float32).unstack(</span><br><span class="line">    _transpose_batch_time(self.input), <span class="string">'TBD_Input'</span>)</span><br></pre></td></tr></table></figure>
<p>And transform it back when you read the output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rnn_outputs = _transpose_batch_time(rnn_output_ta.stack())</span><br></pre></td></tr></table></figure>
</li>
<li><p>When you alter <code>next_input</code> inside the <code>loop_fn</code>, Tensorflow may lose the track for back-propagating the gradient. Consequently, you will receive the following warning:<div class="tip"><br>UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.<br>  “Converting sparse IndexedSlices to a dense Tensor of unknown shape. “<br></div>In this case, just add <code>stop_gradient</code> to stop gradient propagating through your input, like this:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">next_pixel = tf.stop_gradient(tf.cond(is_training,</span><br><span class="line">                                  <span class="keyword">lambda</span>: inputs_ta.read(time - <span class="number">1</span>),</span><br><span class="line">                                  <span class="keyword">lambda</span>: get_sample(cell_output)))</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2><span id="conclusion">Conclusion</span></h2><p>To summarize, <code>raw_rnn</code> provides you the flexibility to customize recurrent neural network and helps you understand the recurrence mechanism better. It allows you to control what should output, what should be fed next, and when should it end. This is extremely useful when you want to design sophisticated recurrent neural network. I hope this article could shed a light on this useful API and bring your RNN to the next level.</p>
</div></article></div></main><footer><div class="paginator"><a href="/2017/08/26/Fashion-MNIST-a-Drop-In-Replacement-of-MNIST-for-Benchmarking-Machine-Learning-Algorithms/" class="prev">&nbsp;❮&nbsp;&nbsp;Fashion-MNIST: a Dro...</a><a href="/2017/07/07/Get-10x-Speedup-in-Tensorflow-Multi-Task-Learning-using-Python-Multiprocessing/" class="next">Get 10x Speedup in T...&nbsp;&nbsp;❯&nbsp;</a></div><div class="footer-section"><h2><img src="/flower.png" alt="Checkout this">Check out these posts too!</h2><div class="archive-readmore"><div style="display: flex; gap: 10px; align-items: flex-start;" class="post-item"><a href="/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0/"><img src="https://hanxiao.io/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0//9ba076d0.png" alt="Video Semantic Search in Large Scale using GNES and Tensorflow 2.0" style="width: 120px; height: 90px; object-fit: cover;"></a><div style="flex: 1;" class="book-title"><h5 style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; margin: 0;" class="post-title"><a href="/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0/" class="post-title-link">Video Semantic Search in Large Scale using GNES and Tensorflow 2.0</a></h5></div></div><div style="display: flex; gap: 10px; align-items: flex-start;" class="post-item"><a href="/2019/11/07/A-Better-Practice-for-Managing-extras-require-Dependencies-in-Python/"><img src="https://hanxiao.io/2019/11/07/A-Better-Practice-for-Managing-extras-require-Dependencies-in-Python//banner.png" alt="A Better Practice for Managing Many &lt;code&gt;extras_require&lt;/code&gt; Dependencies in Python" style="width: 120px; height: 90px; object-fit: cover;"></a><div style="flex: 1;" class="book-title"><h5 style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; margin: 0;" class="post-title"><a href="/2019/11/07/A-Better-Practice-for-Managing-extras-require-Dependencies-in-Python/" class="post-title-link">A Better Practice for Managing Many <code>extras_require</code> Dependencies in Python</a></h5></div></div><div style="display: flex; gap: 10px; align-items: flex-start;" class="post-item"><a href="/2019/10/18/GNES-Flow-a-Pythonic-Way-to-Build-Cloud-Native-Neural-Search-Pipelines/"><img src="https://hanxiao.io/2019/10/18/GNES-Flow-a-Pythonic-Way-to-Build-Cloud-Native-Neural-Search-Pipelines//gnes-flow-banner.png" alt="GNES Flow: a Pythonic Way to Build Cloud-Native Neural Search Pipelines" style="width: 120px; height: 90px; object-fit: cover;"></a><div style="flex: 1;" class="book-title"><h5 style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; margin: 0;" class="post-title"><a href="/2019/10/18/GNES-Flow-a-Pythonic-Way-to-Build-Cloud-Native-Neural-Search-Pipelines/" class="post-title-link">GNES Flow: a Pythonic Way to Build Cloud-Native Neural Search Pipelines</a></h5></div></div><div style="display: flex; gap: 10px; align-items: flex-start;" class="post-item"><a href="/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/"><img src="https://hanxiao.io/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond//gnes-team-1600.JPG" alt="Generic Neural Elastic Search: From &lt;code&gt;bert-as-service&lt;/code&gt; and Go Way Beyond" style="width: 120px; height: 90px; object-fit: cover;"></a><div style="flex: 1;" class="book-title"><h5 style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; margin: 0;" class="post-title"><a href="/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/" class="post-title-link">Generic Neural Elastic Search: From <code>bert-as-service</code> and Go Way Beyond</a></h5></div></div></div></div><div class="copyright"><p>© 2017 - 2025 <a href="https://hanxiao.io">Han Xiao</a>. <img src="/by-nc-sa.svg" alt="Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License." class="image"></p></div></footer></div><link rel="stylesheet" href="/css/post/katex.min.css"><!--link(rel="stylesheet", href=url_for("css/post/gitment.css"))--><script src="/js/katex.min.js"></script><script src="/js/auto-render.min.js"></script><script>renderMathInElement(
    document.body,
    {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "\\[", right: "\\]", display: true},
            {left: "$", right: "$", display: false},
            {left: "\\(", right: "\\)", display: false}
        ]
    }
);</script><!--script(src=url_for("js/gitment.browser.js"))--><!--script(src=url_for("js/gitment.loader.js"))--><script src="/js/jquery-3.4.1.min.js"></script><script src="/js/reading_progress.min.js"></script><script src="/js/highlighter.min.js"></script><script src="/js/init-highlighter.js"></script><script async src="https://www.google-analytics.com/analytics.js"></script><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create',"UA-52114253-1",'auto');ga('send','pageview');</script></body></html>