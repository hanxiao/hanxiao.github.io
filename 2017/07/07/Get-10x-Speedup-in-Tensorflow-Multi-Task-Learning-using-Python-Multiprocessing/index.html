<!DOCTYPE html><html lang="en"><style>html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}article,header,main{display:block}a{background-color:transparent}h1{font-size:2em;margin:.67em 0}img{border:0}body,html{width:100%;height:100%}html{width:100%;height:100vh;display:flex;flex-direction:column;justify-content:center;align-items:center;background:var(--bgColor);--bgColor:#fff;--textColor:#2c3e50;--bg2ndColor:none;--bg3rdColor:#f8f8f8;--preCodeColor:#525252;--imgOpacity:1.0}@media (prefers-color-scheme:dark){html{background:var(--bgColor);--bgColor:#121212;--textColor:#fff;--bg2ndColor:#fff;--bg3rdColor:#332940;--preCodeColor:#f8f8f8;--imgOpacity:0.5}}body{margin:0;color:var(--textColor);font-size:18px;line-height:1.6;background-color:var(--bgColor);font-family:sourcesanspro,'Helvetica Neue',Arial,sans-serif}ul.nav{margin:0;padding:0;list-style-type:none}ul{margin:1rem 0}a{color:var(--textColor);text-decoration:none}.flag-icon{height:25px;width:25px;display:inline;border-radius:50%;vertical-align:sub}.icon_item{padding-left:5px!important;padding-right:5px!important}.reading-progress-bar{background:#42b983;display:block;height:2px;left:0;position:fixed;top:0;width:0;z-index:10001}header{min-height:60px}header .logo-link{float:left}header .nav{float:right;left:80px}header .logo-link img{height:60px}header .nav-list-item{display:inline-block;padding:19px 10px}header .nav-list-item a{line-height:1.4}@media screen and (max-width:900px){header .nav-list-item a{font-size:12px}}@media screen and (min-width:900px){header .nav-list-item a{font-size:18px}}.post{padding-top:1em}.post-block .post-title{margin:.65em 0;color:var(--textColor);font-size:1.5em}.post-block .post-info{color:#7f8c8d}.post-block .post-info .read-time{text-align:right}.post-content h2,.post-content h4{position:relative;margin:1em 0}.post-content h2 :before,.post-content h4 :before{content:"#";color:#42b983;position:absolute;left:-.7em;top:-4px;font-size:1.2em;font-weight:700}.post-content h4 :before{content:">"}.post-content h2{font-size:22px}.post-content h4{font-size:18px}.post-content a{color:#42b983;word-break:break-all}main.container{margin:2em 10px}@media screen and (min-width:900px){.wrap{width:900px;margin:0 auto}header{padding:20px 60px}}@media screen and (max-width:900px){.wrap{width:100%}header{min-height:50px;padding:2px 2px;position:fixed;z-index:10000;border-radius:15px;left:50%;-webkit-transform:translateX(-50%);transform:translateX(-50%);width:-webkit-fit-content;width:-moz-fit-content;width:fit-content}header a.logo-link,header ul.nav.nav-list{float:none;display:inline;text-align:center}header li.nav-list-item{padding:10px 5px}header .logo-link img{height:20px;vertical-align:sub}header .flag-icon{height:20px;width:20px}header{background-color:rgba(255,255,255,.9)}@supports ((-webkit-backdrop-filter:blur(2em)) or (backdrop-filter:blur(2em))){header{background-color:rgba(255,255,255,.3);-webkit-backdrop-filter:blur(10px);backdrop-filter:blur(10px)}}main.container{padding-top:2em}main.container{margin:0 20px}.post-content h2,.post-content h4{max-width:300px;left:15px}}@font-face{font-family:sourcesanspro;src:url(/font/sourcesanspro.woff2) format("woff2"),url(/font/sourcesanspro.woff) format("woff");font-weight:400;font-style:normal}</style><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Get 10x Speedup in Tensorflow Multi-Task Learning using Python Multiprocessing · Han Xiao Tech Blog - Neural Search & AI Engineering</title><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content="@hxiao"><meta name="twitter:creator" content="@hxiao"><meta name="description" content="Recently I started to model user search queries using Tensorflow. After some discussion with my team, the original problem boils down to a set of clas ... · Han Xiao"><meta property="og:title" content="Get 10x Speedup in Tensorflow Multi-Task Learning using Python Multiprocessing · Han Xiao Tech Blog - Neural Search &amp; AI Engineering"><meta property="og:description" content="Recently I started to model user search queries using Tensorflow. After some discussion with my team, the original problem boils down to a set of clas ... · Han Xiao"><meta property="og:url" content="https://hanxiao.io/2017/07/07/Get-10x-Speedup-in-Tensorflow-Multi-Task-Learning-using-Python-Multiprocessing/"><meta property="og:image" content="https://hanxiao.io/2017/07/07/Get-10x-Speedup-in-Tensorflow-Multi-Task-Learning-using-Python-Multiprocessing//ab73616b.png"><meta property="og:type" content="article"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/myavatar.png"><link rel="alternate" type="application/rss+xml" title="Han Xiao Tech Blog - Neural Search &amp; AI Engineering" href="https://hanxiao.io/atom.xml"><!-- - use css preload trick--><link rel="preload" href="/css/apollo.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="/css/apollo.css"></noscript><script>/*! loadCSS. [c]2017 Filament Group, Inc. MIT License */
/* This file is meant as a standalone workflow for
- testing support for link[rel=preload]
- enabling async CSS loading in browsers that do not support rel=preload
- applying rel preload css once loaded, whether supported or not.
*/
(function( w ){
    "use strict";
    // rel=preload support test
    if( !w.loadCSS ){
        w.loadCSS = function(){};
    }
    // define on the loadCSS obj
    var rp = loadCSS.relpreload = {};
    // rel=preload feature support test
    // runs once and returns a function for compat purposes
    rp.support = (function(){
        var ret;
        try {
            ret = w.document.createElement( "link" ).relList.supports( "preload" );
        } catch (e) {
            ret = false;
        }
        return function(){
            return ret;
        };
    })();

    // if preload isn't supported, get an asynchronous load by using a non-matching media attribute
    // then change that media back to its intended value on load
    rp.bindMediaToggle = function( link ){
        // remember existing media attr for ultimate state, or default to 'all'
        var finalMedia = link.media || "all";

        function enableStylesheet(){
            link.media = finalMedia;
        }

        // bind load handlers to enable media
        if( link.addEventListener ){
            link.addEventListener( "load", enableStylesheet );
        } else if( link.attachEvent ){
            link.attachEvent( "onload", enableStylesheet );
        }

        // Set rel and non-applicable media type to start an async request
        // note: timeout allows this to happen async to let rendering continue in IE
        setTimeout(function(){
            link.rel = "stylesheet";
            link.media = "only x";
        });
        // also enable media after 3 seconds,
        // which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
        setTimeout( enableStylesheet, 3000 );
    };

    // loop through link elements in DOM
    rp.poly = function(){
        // double check this to prevent external calls from running
        if( rp.support() ){
            return;
        }
        var links = w.document.getElementsByTagName( "link" );
        for( var i = 0; i < links.length; i++ ){
            var link = links[ i ];
            // qualify links to those with rel=preload and as=style attrs
            if( link.rel === "preload" && link.getAttribute( "as" ) === "style" && !link.getAttribute( "data-loadcss" ) ){
                // prevent rerunning on link
                link.setAttribute( "data-loadcss", true );
                // bind listeners to toggle media back
                rp.bindMediaToggle( link );
            }
        }
    };

    // if unsupported, run the polyfill
    if( !rp.support() ){
        // run once at least
        rp.poly();

        // rerun poly on an interval until onload
        var run = w.setInterval( rp.poly, 500 );
        if( w.addEventListener ){
            w.addEventListener( "load", function(){
                rp.poly();
                w.clearInterval( run );
            } );
        } else if( w.attachEvent ){
            w.attachEvent( "onload", function(){
                rp.poly();
                w.clearInterval( run );
            } );
        }
    }


    // commonjs
    if( typeof exports !== "undefined" ){
        exports.loadCSS = loadCSS;
    }
    else {
        w.loadCSS = loadCSS;
    }
}( typeof global !== "undefined" ? global : this ) );</script><script id="mcjs">!function(c,h,i,m,p){m=c.createElement(h),p=c.getElementsByTagName(h)[0],m.async=1,m.src=i,p.parentNode.insertBefore(m,p)}(document,"script","https://chimpstatic.com/mcjs-connected/js/users/7da58fc9885cb85d4a9f0ad9a/987f901145f1749fd3e800e86.js");</script><link rel="search" type="application/opensearchdescription+xml" href="https://hanxiao.io/atom.xml" title="Han Xiao Tech Blog - Neural Search &amp; AI Engineering"></head><body><div class="reading-progress-bar"></div><div class="wrap"><header><ul class="nav nav-list"><li class="nav-list-item"><a href="/about/" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="https://www.linkedin.com/in/hxiao87" target="_blank" class="nav-list-link">LINKEDIN</a></li><li class="nav-list-item"><a href="https://x.com/hxiao" target="_blank" class="nav-list-link">X</a></li><li class="nav-list-item"><a href="https://github.com/hanxiao" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="https://scholar.google.com/citations?user=jp7swwIAAAAJ" target="_blank" class="nav-list-link">SCHOLAR</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">Get 10x Speedup in Tensorflow Multi-Task Learning using Python Multiprocessing</h1><div class="post-info">Jul 7, 2017 by &nbsp;&nbsp;&nbsp;<img src="/myavatar.png" alt="logo" width="18px" height="18px" style="vertical-align: sub;">&nbsp;<a href="/about" target="_blank" class="nav-list-link">Han Xiao - <i>ex</i> Senior Research Scientist @ Zalando Research</a></div><div class="post-info"><div class="read-time">◷&nbsp;&nbsp;&nbsp; 16  min read</div></div><div class="post-content"><h2><span id="background">Background</span></h2><p>Recently I started to model user search queries using Tensorflow. After some discussion with my team, the original problem boils down to a set of classification tasks, where each task is a <em>multi-label</em> classification problem. One interesting observation here is that the tasks are highly related: knowing the labels of one task could help one to guess the labels of another task. So perhaps it would be a good idea to train all those tasks in the same neural network simultaneously, hoping that the commonality across the tasks will be exploited during the learning and improve the final performance.</p>
<p>As the number of tasks increases, my training data becomes larger and larger. As a consequence, directly feeding such <a id="more"></a>training data into Tensorflow’s session becomes laggy and results in <strong>poor GPU utilization</strong>. Moreover, it is <strong>vulnerable to memory leaks</strong> and can’t be used for days-long training. In this article, I will explain data feeding problems and design a <strong>much more efficient and robust</strong> input pipeline for days-long training. If you have ever struggled with Tensorflow’s multithread-based queue and frustrated by its low performance, then you should definitely give this multiprocess-based implementation a try.</p>
<div class="tip"><br>  This post was originally published when Tensorflow was at 1.1, in which data pipeline  was still inmature. If you are using Tensorflow &gt;=1.4, you may leverage the new <a href="https://www.tensorflow.org/programmers_guide/datasets" target="_blank" rel="noopener">Dataset API</a>. It implements a similar idea as described in this post.<br></div>

<h2><span id="a-short-brief-on-multi-task-rnn">A Short Brief on Multi-Task RNN</span></h2><p>The idea of jointly learning multiple goals is nothing new and has been well-studied in the machine learning community. Specifically, this problem is called the <a href="https://en.wikipedia.org/wiki/Multi-task_learning" target="_blank" rel="noopener"><strong>multi-task learning</strong></a>. Comparing to training the models separately, multi-task learning learns tasks in parallel while using a shared representation. By transferring knowledge across tasks via the shared representation, it can improve learning efficiency and prediction accuracy for the task-specific models. Interested readers are encouraged to <a href="http://ruder.io/multi-task/" target="_blank" rel="noopener">read this post for a summary of state-of-art multi-task learning methods.</a></p>
<p>For employing multi-task learning on sequence data, I use recurrent neural networks as the backbone of the network. The structure of my multi-task RNN looks like as follows:</p>
<img src="/2017/07/07/Get-10x-Speedup-in-Tensorflow-Multi-Task-Learning-using-Python-Multiprocessing/3f9ea6de.png">
<p>For the sake of clarity, I only draw a very basic version of my network here. One can certainly try different variants such as replacing RNN cell with LSTM and GRU cell, adding bidirectional or stacked RNN layer on top of the chain. What’s important here is that the final output of RNN chain should be shared by all task-specific fully connected layers, which are then followed by softmax layers to obtain the final label distribution on each task. This can be done via:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Y_logit = &#123;</span><br><span class="line">    k: tf.layers.dense(inputs=last, units=v,</span><br><span class="line">                        kernel_initializer=glorot_uniform_initializer(),</span><br><span class="line">                        bias_initializer=constant_initializer(<span class="number">0.1</span>), </span><br><span class="line">                        name=<span class="string">'logit_'</span> + k) <span class="keyword">for</span> k <span class="keyword">in</span> all_tasks.keys()&#125;</span><br></pre></td></tr></table></figure>
<p>Given the <code>logit</code> output and groundtruth labels on each task, we can compute the task-specific cross-entropy loss and total cross entropy loss as follows:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">task_loss = &#123;</span><br><span class="line">    k: tf.reduce_mean(</span><br><span class="line">        tf.nn.softmax_cross_entropy_with_logits(</span><br><span class="line">            labels=Y_soft[k], logits=Y_logit[k]), </span><br><span class="line">            name=<span class="string">'softmax_xentropy_'</span> + k) <span class="keyword">for</span> k <span class="keyword">in</span> all_tasks.keys()&#125;</span><br><span class="line"></span><br><span class="line">total_loss = reduce(tf.add, task_loss.values())</span><br></pre></td></tr></table></figure></p>
<p>At this point, there are at least two straightforward ways to solve this optimization problem. </p>
<ul>
<li>alternatively optimizing each task-specific loss, i.e. <code>task_loss[k]</code>;</li>
<li>jointly optimizing <code>total_loss</code>.</li>
</ul>
<p>Which one outperforms the other usually depends on the training data. Note that, the first method does not require training data to be aligned across tasks, whereas in second method one needs labels of all tasks for every sequence in the batch. On the other hand, one can add adaptive weight on each task in the second method to obtain more task-sensitive learning. I choose the second method as we happen to have aligned training data. Plus, I want to explore adaptive weighting in the future.</p>
<h2><span id="data-format-and-preprocessing">Data Format and Preprocessing</span></h2><p>Our data comes (almost) directly from the production system. Training data of each task is a JSON file and looks like this:</p>
<p><strong>1st task:</strong> Animal’s type<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[&#123;</span><br><span class="line">    <span class="string">'query'</span>: <span class="string">'cute'</span>,</span><br><span class="line">    <span class="string">'label'</span>: [</span><br><span class="line">        <span class="string">'CAT'</span>: <span class="number">123</span>,</span><br><span class="line">        <span class="string">'DOG'</span>: <span class="number">23</span>,</span><br><span class="line">        <span class="string">'SHEEP'</span>: <span class="number">10</span>,</span><br><span class="line">        ...]&#125;,</span><br><span class="line"> &#123;</span><br><span class="line">    <span class="string">'query'</span>: <span class="string">'grassland'</span>,</span><br><span class="line">    <span class="string">'label'</span>: [</span><br><span class="line">        <span class="string">'SHEEP'</span>: <span class="number">14</span>,</span><br><span class="line">        <span class="string">'WOLF'</span>: <span class="number">3</span>,</span><br><span class="line">        ...]&#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure></p>
<p><strong>2nd task:</strong> Animal’s color<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[&#123;</span><br><span class="line">    <span class="string">'query'</span>: <span class="string">'cute'</span>,</span><br><span class="line">    <span class="string">'label'</span>: [</span><br><span class="line">        <span class="string">'PINK'</span>: <span class="number">120</span>,</span><br><span class="line">        <span class="string">'WHITE'</span>: <span class="number">36</span>,</span><br><span class="line">        ...]&#125;,</span><br><span class="line"> &#123;</span><br><span class="line">    <span class="string">'query'</span>: <span class="string">'grasslands'</span>,</span><br><span class="line">    <span class="string">'label'</span>: [</span><br><span class="line">        <span class="string">'GREEN'</span>: <span class="number">10</span>,</span><br><span class="line">        <span class="string">'WHITE'</span>: <span class="number">5</span>,</span><br><span class="line">        <span class="string">'BEIGE'</span>: <span class="number">1</span>,</span><br><span class="line">        ...]&#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure></p>
<p>As I mentioned before, the good news is that data is aligned. Queries from different JSONs are sorted in the same order, which is perfect for the joint training method. However, we can not feed such JSON files directly to Tensorflow, preprocessing is required. Specifically, one needs to do the following steps before feeding the data to a Tensorflow session:</p>
<ul>
<li>bucketing and padding <code>query</code> field so that each batch contains sequences in the same length;</li>
<li>transforming the content in <code>label</code> field to a (sparse) labeling matrix;</li>
<li>normalizing the labeling matrix row-wise.</li>
</ul>
<p>Some preprocessing steps (e.g. bucketing) can be done in advance, whereas others need to be done batch-wise especially when data is large. For example, building and normalizing labeling matrix of each batch. </p>
<p>A straightforward way is running batch generation and training procedure sequentially in the same process, such as:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> cur_epoch &lt; MAX_EPOCH:</span><br><span class="line">    <span class="comment"># do all preprocessing in `next_batch()`</span></span><br><span class="line">    train_data = next_batch(batch_size)  </span><br><span class="line">    </span><br><span class="line">    train_dict = &#123;</span><br><span class="line">        X: train_data[<span class="string">'queries'</span>],</span><br><span class="line">        Y: train_data[<span class="string">'sparse_label_matrix'</span>],</span><br><span class="line">        L: train_data[<span class="string">'query_lengths'</span>],</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># do actual training</span></span><br><span class="line">    sess.run(train_op, feed_dict=train_dict)</span><br></pre></td></tr></table></figure></p>
<p>If we visualize the above code, the workflow looks like the following graph:</p>
<img src="/2017/07/07/Get-10x-Speedup-in-Tensorflow-Multi-Task-Learning-using-Python-Multiprocessing/024a196e.png">
<p>There are at least two problems of this workflow:</p>
<ul>
<li><strong>low GPU utilization.</strong> GPU keeps idle until CPU finishes the batch preparation. Especially when the network is small and training takes less time than batch preparation, one can notice a significant lag between each training cycle. In order to keep GPU busy, one can increase the size of each batch, which unfortunately also increases the preparation time on CPU.</li>
<li><strong>vulnerable to memory leak.</strong> Larger models often require hours or days of training, it is very hard to guarantee all memory resources are correctly released during the whole training time (particularly in Python). At some point, the system won’t be able to allocate any memory for a new batch. As a consequence, the program crashes and you have to use checkpoints for recovery.</li>
</ul>
<p>I have tested this implementation on a machine with a quad-core CPU and NVIDIA Tesla K80. The average GPU utilization is below 30% and only one CPU core is used. Increasing batch size only makes it worse.</p>
<h2><span id="feeding-data-more-efficiently-and-more-reliably">Feeding Data More Efficiently and More Reliably</span></h2><p>Having these problems in mind, I resort to Python <code>multiprocessing</code> <a href="https://docs.python.org/3/library/multiprocessing.html" target="_blank" rel="noopener">package</a> to introduce some asynchronous and parallel computation into the workflow. I’m aware that Tensorflow has a <a href="https://www.tensorflow.org/programmers_guide/threading_and_queues" target="_blank" rel="noopener">thread-based queue</a> API already. Unfortunately, after hacking with it for one day, TF thread-based feeding pipeline still performs poorly in my case. In fact, it is even slower than the above naive implementation. Despite the bad performance of Python threading (due to Global Interpreter Lock), I also found several reports regarding the low performance of Tensorflow queue on <a href="https://stackoverflow.com/questions/40322461/my-tensorflow-queue-get-filled-slower-when-i-increase-the-number-of-threads-push" target="_blank" rel="noopener">stackoverflow</a> and <a href="https://github.com/tensorflow/tensorflow/issues/7817" target="_blank" rel="noopener">github</a>. As I’m writing this blog post, there is an open discussion on github about <a href="https://github.com/tensorflow/tensorflow/issues/7951" target="_blank" rel="noopener">redesigning TensorFlow’s input pipelines</a>. Given that Tensorflow’s queue API may change in the next versions, I decide to build asynchronous queue by myself. Thanks to the <code>multiprocessing</code> package, implementation is very easy and straightforward. </p>
<p>Here is an overview of my data feeding workflow. I will explain the ideas behind this graph later.</p>
<img src="/2017/07/07/Get-10x-Speedup-in-Tensorflow-Multi-Task-Learning-using-Python-Multiprocessing/ab73616b.png">
<p>The main ideas are following:</p>
<ul>
<li>In the main process, I create two queues (via <code>multiprocessing.Queue</code>): one is for storing task-specific batches, the other is for storing multi-task batches that are ready for feeding to Tensorflow. </li>
<li>I spawn multiple sub-processes (via <code>multiprocessing.Process</code>), each process keeps preprocessing the JSON input and generating task-specific batch and putting the batch to the “single task queue”.</li>
<li>I spawn another sub-process for aggregating the task-specific batches, and putting the result to the “multi-task queue”. </li>
<li>Whenever GPU needs a new batch for training, I directly get a batch from the “multi-task queue” and feed to <code>session.run(..., feed_dict=...)</code>. <strong>No more waiting.</strong></li>
<li>When training ends I trigger a stop-event (via <code>multiprocessing.Event</code>) to all sub-processes and gently terminate them.</li>
<li>I monitor the memory usage in the main process using <a href="https://pypi.python.org/pypi/psutil" target="_blank" rel="noopener"><code>psutil</code> package</a>. Once the memory usage reaches some threshold, the main process will terminate all sub-processes and respawn them. The actual training session in the main process is not affected and can continue once respawn is finished. <strong>No more out-of-memory.</strong></li>
</ul>
<p>Here is a very basic implementation of those ideas. Again, my code is in Python 3  with type annotation.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Process, Queue, Event</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SingleBatchGenerator</span><span class="params">(Process)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, single_task_q: Queue, stop_event: Event)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.done_q = single_task_q</span><br><span class="line">        self.stop_event = stop_event</span><br><span class="line">        self.myseed = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># bucketing, padding sequences; and transforming, normalizing labelling matrix</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">next_batch</span><span class="params">(self, seed: int)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> self.stop_event.is_set():</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self.done_q.full():</span><br><span class="line">                self.done_q.put(self.next_batch(self.myseed))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BatchAggregator</span><span class="params">(Process)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, single_task_q: Queue, multi_task_q: Queue, stop_event: Event)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.pending_q = single_task_q</span><br><span class="line">        self.done_q = multi_task_q</span><br><span class="line">        self.stop_event = stop_event</span><br><span class="line">        self.mt_batch = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># merge single task batches with same job seed to a multi-task batch</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">merge_st_batches</span><span class="params">(self, st_batches: Dict[str, Any])</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># check whether the multi-task batch contains all tasks</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">is_complete</span><span class="params">(self, st_batches: Dict[str, Any])</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> self.stop_event.is_set():</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self.done_q.full():</span><br><span class="line">                st_batch = self.pending_q.get()</span><br><span class="line">                job_task = st_batch[<span class="string">'task_name'</span>]</span><br><span class="line">                job_seed = st_batch[<span class="string">'myseed'</span>]</span><br><span class="line">                self.mt_batch[job_seed][job_task] = st_batch</span><br><span class="line">                <span class="keyword">if</span> is_complete(self.mt_batch[job_seed]):</span><br><span class="line">                    self.done_q.put(merge_st_batches(self.mt_batch.pop(job_seed)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiTaskBatchManager</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.stop_event = Event()</span><br><span class="line">        self.single_task_q = Queue(MAX_CAPACITY)</span><br><span class="line">        self.multi_task_train_q = Queue(MAX_CAPACITY)</span><br><span class="line">        self.batch_aggregator = BatchAggregator(self.single_task_q, self.multi_task_train_q, self.stop_event)</span><br><span class="line">        self.batch_generator = &#123;task: SingleBatchGenerator(self.single_task_q, self.stop_event) <span class="keyword">for</span> task <span class="keyword">in</span> all_tasks&#125;</span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> self.batch_generator.values():</span><br><span class="line">            w.start()</span><br><span class="line">        self.batch_aggregator.start()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">next_batch</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.multi_task_train_q.get()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close</span><span class="params">(self, timeout: int = <span class="number">5</span>)</span>:</span></span><br><span class="line">        self.stop_event.set()</span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> self.batch_generator.values():</span><br><span class="line">            w.join(timeout=timeout)</span><br><span class="line">            w.terminate()</span><br><span class="line">        self.batch_aggregator.join(timeout=timeout)</span><br><span class="line">        self.batch_aggregator.terminate()</span><br></pre></td></tr></table></figure></p>
<p>I tested the new data feeding pipeline on the same machine with a quad-core CPU and NVIDIA Tesla K80. Setting the batch size to 5000, I’m able to reach on average 90% GPU utilization and 95% CPU utilization on all four cores. Needless to say, I’m very happy with this result. Efficient and reliable data feeding allows me to focus more on the model itself and explore more with large-scale data, which may reveal interesting patterns that otherwise won’t be significant or even observable.</p>
<h2><span id="using-tensorflow-for-preprocessing-in-subprocess">Using Tensorflow for Preprocessing in Subprocess</span></h2><p>In some cases such as image-related task, data preprocessing means much more than a simple normalization. Therefore one may need to build a computation graph for preprocessing as well. Luckily, adding Tensorflow sessions to our new data feeding pipeline is very straightforward. One can simply modify <code>run()</code> method in <code>SingleBatchGenerator</code> class as follows:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">    sess = tf.Session(config=tf.ConfigProto(device_count=&#123;<span class="string">'GPU'</span>: <span class="number">0</span>, <span class="string">'CPU'</span>: <span class="number">4</span>&#125;, log_device_placement=<span class="literal">False</span>))</span><br><span class="line">    <span class="keyword">with</span> tf.device(<span class="string">"/cpu:0"</span>):</span><br><span class="line">       <span class="comment"># your compute-graph</span></span><br><span class="line">       graph = ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># keep generating training data...</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> self.stop_event.is_set():</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.done_q.full():</span><br><span class="line">            self.done_q.put(sess.run(graph, self.next_batch(self.myseed)))</span><br><span class="line"></span><br><span class="line">    sess.close()</span><br></pre></td></tr></table></figure>
<p>In this example, I open a TF session in each subprocess and restrict the computation to CPU-only via <code>with tf.device(&quot;/cpu:0&quot;)</code>. This will leave all GPU resource to main training procedure. When you use <code>nvidia-smi</code> to check the GPU status, you will find something similar to this:<br><img src="/2017/07/07/Get-10x-Speedup-in-Tensorflow-Multi-Task-Learning-using-Python-Multiprocessing/e4072d42.png"></p>
<p>Judging from the GPU memory usage, one can observe that only the main process (with <code>PID: 9961</code>) is running on GPU. All subprocesses (<code>PID:10021 - 10046</code>) are mainly on CPU. However, the reason that <code>nvidia-smi</code> still captures and shows those CPU processes isn’t very clear to me. Perhaps because of the implementation in <code>tensorflow-gpu</code> package. </p>
<h2><span id="is-memory-leak-a-real-problem">Is Memory Leak a Real Problem?</span></h2><p>Yes, it is. Some memory leaks are crafty and hard to notice if the training procedure only takes an hour. I can recall many times that my program crashes during the days-long training because of the memory issue. There are many places that memory leaks can happen, e.g. <a href="https://bugs.python.org/issue28894" target="_blank" rel="noopener"><code>dict.pop()</code> in Python 3.6</a> and <code>tf.sparse_tensor_to_dense</code>. But when it comes to training DNN with Tensorflow, memory leaks are more likely to hide in the data preprocessing and batch preparation steps.</p>
<p>The new data feeding pipeline is less prone to memory leaks. Not because it won’t generate any memory leak during preprocessing, but because it gives one better control by restricting the memory leaks to a subprocess. When the memory usage reaches some threshold, one can simply terminate all subprocesses and respawn them. The training procedure is separated in the main process, thus won’t be affected or interrupted. The only thing you might lose is seconds of training time while waiting respawned new subprocess ready. Nonetheless, such loss is nothing compared to an unexpected system crash.</p>
<p>Finally, I’m not saying you should leave memory leak as it is. Instead, you should actively diagnose it and fix it. I often use <a href="https://pypi.python.org/pypi/memory_profiler" target="_blank" rel="noopener"><code>memory_profiler</code></a> for monitoring memory consumption and diagnosing the problem. Its visualization feature is particularly helpful.</p>
</div></article></div></main><footer><div class="paginator"><a href="/2017/08/16/Why-I-use-raw-rnn-Instead-of-dynamic-rnn-in-Tensorflow-So-Should-You-0/" class="prev">&nbsp;❮&nbsp;&nbsp;Why I Use raw_...</a><a href="/2017/05/19/Extremely-Stupid-Mistakes-I-made-with-Tensorflow-and-Python/" class="next">Extremely Stupid Mis...&nbsp;&nbsp;❯&nbsp;</a></div><div class="footer-section"><h2><img src="/flower.png" alt="Checkout this">Check out these posts too!</h2><div class="archive-readmore"><div style="display: flex; gap: 10px; align-items: flex-start;" class="post-item"><a href="/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0/"><img src="https://hanxiao.io/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0//9ba076d0.png" alt="Video Semantic Search in Large Scale using GNES and Tensorflow 2.0" style="width: 120px; height: 90px; object-fit: cover;"></a><div style="flex: 1;" class="book-title"><h5 style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; margin: 0;" class="post-title"><a href="/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0/" class="post-title-link">Video Semantic Search in Large Scale using GNES and Tensorflow 2.0</a></h5></div></div><div style="display: flex; gap: 10px; align-items: flex-start;" class="post-item"><a href="/2019/11/07/A-Better-Practice-for-Managing-extras-require-Dependencies-in-Python/"><img src="https://hanxiao.io/2019/11/07/A-Better-Practice-for-Managing-extras-require-Dependencies-in-Python//banner.png" alt="A Better Practice for Managing Many &lt;code&gt;extras_require&lt;/code&gt; Dependencies in Python" style="width: 120px; height: 90px; object-fit: cover;"></a><div style="flex: 1;" class="book-title"><h5 style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; margin: 0;" class="post-title"><a href="/2019/11/07/A-Better-Practice-for-Managing-extras-require-Dependencies-in-Python/" class="post-title-link">A Better Practice for Managing Many <code>extras_require</code> Dependencies in Python</a></h5></div></div><div style="display: flex; gap: 10px; align-items: flex-start;" class="post-item"><a href="/2019/10/18/GNES-Flow-a-Pythonic-Way-to-Build-Cloud-Native-Neural-Search-Pipelines/"><img src="https://hanxiao.io/2019/10/18/GNES-Flow-a-Pythonic-Way-to-Build-Cloud-Native-Neural-Search-Pipelines//gnes-flow-banner.png" alt="GNES Flow: a Pythonic Way to Build Cloud-Native Neural Search Pipelines" style="width: 120px; height: 90px; object-fit: cover;"></a><div style="flex: 1;" class="book-title"><h5 style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; margin: 0;" class="post-title"><a href="/2019/10/18/GNES-Flow-a-Pythonic-Way-to-Build-Cloud-Native-Neural-Search-Pipelines/" class="post-title-link">GNES Flow: a Pythonic Way to Build Cloud-Native Neural Search Pipelines</a></h5></div></div><div style="display: flex; gap: 10px; align-items: flex-start;" class="post-item"><a href="/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/"><img src="https://hanxiao.io/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond//gnes-team-1600.JPG" alt="Generic Neural Elastic Search: From &lt;code&gt;bert-as-service&lt;/code&gt; and Go Way Beyond" style="width: 120px; height: 90px; object-fit: cover;"></a><div style="flex: 1;" class="book-title"><h5 style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; margin: 0;" class="post-title"><a href="/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/" class="post-title-link">Generic Neural Elastic Search: From <code>bert-as-service</code> and Go Way Beyond</a></h5></div></div></div></div><div class="copyright"><p>© 2017 - 2025 <a href="https://hanxiao.io">Han Xiao</a>. <img src="/by-nc-sa.svg" alt="Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License." class="image"></p></div></footer></div><link rel="stylesheet" href="/css/post/katex.min.css"><!--link(rel="stylesheet", href=url_for("css/post/gitment.css"))--><script src="/js/katex.min.js"></script><script src="/js/auto-render.min.js"></script><script>renderMathInElement(
    document.body,
    {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "\\[", right: "\\]", display: true},
            {left: "$", right: "$", display: false},
            {left: "\\(", right: "\\)", display: false}
        ]
    }
);</script><!--script(src=url_for("js/gitment.browser.js"))--><!--script(src=url_for("js/gitment.loader.js"))--><script src="/js/jquery-3.4.1.min.js"></script><script src="/js/reading_progress.min.js"></script><script src="/js/highlighter.min.js"></script><script src="/js/init-highlighter.js"></script><script async src="https://www.google-analytics.com/analytics.js"></script><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create',"UA-52114253-1",'auto');ga('send','pageview');</script></body></html>