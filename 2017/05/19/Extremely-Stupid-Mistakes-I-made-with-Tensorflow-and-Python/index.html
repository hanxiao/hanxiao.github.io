<!DOCTYPE html><html lang="en"><style>html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}article,header,main{display:block}a{background-color:transparent}h1{font-size:2em;margin:.67em 0}img{border:0}body,html{width:100%;height:100%}html{width:100%;height:100vh;display:flex;flex-direction:column;justify-content:center;align-items:center;background:var(--bgColor);--bgColor:#fff;--textColor:#2c3e50;--bg2ndColor:none;--bg3rdColor:#f8f8f8;--preCodeColor:#525252;--imgOpacity:1.0}@media (prefers-color-scheme:dark){html{background:var(--bgColor);--bgColor:#121212;--textColor:#fff;--bg2ndColor:#fff;--bg3rdColor:#332940;--preCodeColor:#f8f8f8;--imgOpacity:0.5}}body{margin:0;color:var(--textColor);font-size:18px;line-height:1.6;background-color:var(--bgColor);font-family:sourcesanspro,'Helvetica Neue',Arial,sans-serif}ul.nav{margin:0;padding:0;list-style-type:none}ul{margin:1rem 0}a{color:var(--textColor);text-decoration:none}.flag-icon{height:25px;width:25px;display:inline;border-radius:50%;vertical-align:sub}.icon_item{padding-left:5px!important;padding-right:5px!important}.reading-progress-bar{background:#42b983;display:block;height:2px;left:0;position:fixed;top:0;width:0;z-index:10001}header{min-height:60px}header .logo-link{float:left}header .nav{float:right;left:80px}header .logo-link img{height:60px}header .nav-list-item{display:inline-block;padding:19px 10px}header .nav-list-item a{line-height:1.4}@media screen and (max-width:900px){header .nav-list-item a{font-size:12px}}@media screen and (min-width:900px){header .nav-list-item a{font-size:18px}}.post{padding-top:1em}.post-block .post-title{margin:.65em 0;color:var(--textColor);font-size:1.5em}.post-block .post-info{color:#7f8c8d}.post-block .post-info .read-time{text-align:right}.post-content h2,.post-content h4{position:relative;margin:1em 0}.post-content h2 :before,.post-content h4 :before{content:"#";color:#42b983;position:absolute;left:-.7em;top:-4px;font-size:1.2em;font-weight:700}.post-content h4 :before{content:">"}.post-content h2{font-size:22px}.post-content h4{font-size:18px}.post-content a{color:#42b983;word-break:break-all}main.container{margin:2em 10px}@media screen and (min-width:900px){.wrap{width:900px;margin:0 auto}header{padding:20px 60px}}@media screen and (max-width:900px){.wrap{width:100%}header{min-height:50px;padding:2px 2px;position:fixed;z-index:10000;border-radius:15px;left:50%;-webkit-transform:translateX(-50%);transform:translateX(-50%);width:-webkit-fit-content;width:-moz-fit-content;width:fit-content}header a.logo-link,header ul.nav.nav-list{float:none;display:inline;text-align:center}header li.nav-list-item{padding:10px 5px}header .logo-link img{height:20px;vertical-align:sub}header .flag-icon{height:20px;width:20px}header{background-color:rgba(255,255,255,.9)}@supports ((-webkit-backdrop-filter:blur(2em)) or (backdrop-filter:blur(2em))){header{background-color:rgba(255,255,255,.3);-webkit-backdrop-filter:blur(10px);backdrop-filter:blur(10px)}}main.container{padding-top:2em}main.container{margin:0 20px}.post-content h2,.post-content h4{max-width:300px;left:15px}}@font-face{font-family:sourcesanspro;src:url(/font/sourcesanspro.woff2) format("woff2"),url(/font/sourcesanspro.woff) format("woff");font-weight:400;font-style:normal}</style><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Extremely Stupid Mistakes I Made With Tensorflow and Python · Han Xiao Tech Blog - Neural Search & AI Engineering</title><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content="@hxiao"><meta name="twitter:creator" content="@hxiao"><meta name="description" content="Recently I started with Tensorflow for developing some RNN-based system. I choose Python 3 as the main language since TF has most stable API support f ... · Han Xiao"><meta property="og:title" content="Extremely Stupid Mistakes I Made With Tensorflow and Python · Han Xiao Tech Blog - Neural Search &amp; AI Engineering"><meta property="og:description" content="Recently I started with Tensorflow for developing some RNN-based system. I choose Python 3 as the main language since TF has most stable API support f ... · Han Xiao"><meta property="og:url" content="https://hanxiao.io/2017/05/19/Extremely-Stupid-Mistakes-I-made-with-Tensorflow-and-Python/"><meta property="og:image" content="https://hanxiao.io/2017/05/19/Extremely-Stupid-Mistakes-I-made-with-Tensorflow-and-Python//graph-loss-trace.png"><meta property="og:type" content="article"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/myavatar.png"><link rel="alternate" type="application/rss+xml" title="Han Xiao Tech Blog - Neural Search &amp; AI Engineering" href="https://hanxiao.io/atom.xml"><!-- - use css preload trick--><link rel="preload" href="/css/apollo.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="/css/apollo.css"></noscript><script>/*! loadCSS. [c]2017 Filament Group, Inc. MIT License */
/* This file is meant as a standalone workflow for
- testing support for link[rel=preload]
- enabling async CSS loading in browsers that do not support rel=preload
- applying rel preload css once loaded, whether supported or not.
*/
(function( w ){
    "use strict";
    // rel=preload support test
    if( !w.loadCSS ){
        w.loadCSS = function(){};
    }
    // define on the loadCSS obj
    var rp = loadCSS.relpreload = {};
    // rel=preload feature support test
    // runs once and returns a function for compat purposes
    rp.support = (function(){
        var ret;
        try {
            ret = w.document.createElement( "link" ).relList.supports( "preload" );
        } catch (e) {
            ret = false;
        }
        return function(){
            return ret;
        };
    })();

    // if preload isn't supported, get an asynchronous load by using a non-matching media attribute
    // then change that media back to its intended value on load
    rp.bindMediaToggle = function( link ){
        // remember existing media attr for ultimate state, or default to 'all'
        var finalMedia = link.media || "all";

        function enableStylesheet(){
            link.media = finalMedia;
        }

        // bind load handlers to enable media
        if( link.addEventListener ){
            link.addEventListener( "load", enableStylesheet );
        } else if( link.attachEvent ){
            link.attachEvent( "onload", enableStylesheet );
        }

        // Set rel and non-applicable media type to start an async request
        // note: timeout allows this to happen async to let rendering continue in IE
        setTimeout(function(){
            link.rel = "stylesheet";
            link.media = "only x";
        });
        // also enable media after 3 seconds,
        // which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
        setTimeout( enableStylesheet, 3000 );
    };

    // loop through link elements in DOM
    rp.poly = function(){
        // double check this to prevent external calls from running
        if( rp.support() ){
            return;
        }
        var links = w.document.getElementsByTagName( "link" );
        for( var i = 0; i < links.length; i++ ){
            var link = links[ i ];
            // qualify links to those with rel=preload and as=style attrs
            if( link.rel === "preload" && link.getAttribute( "as" ) === "style" && !link.getAttribute( "data-loadcss" ) ){
                // prevent rerunning on link
                link.setAttribute( "data-loadcss", true );
                // bind listeners to toggle media back
                rp.bindMediaToggle( link );
            }
        }
    };

    // if unsupported, run the polyfill
    if( !rp.support() ){
        // run once at least
        rp.poly();

        // rerun poly on an interval until onload
        var run = w.setInterval( rp.poly, 500 );
        if( w.addEventListener ){
            w.addEventListener( "load", function(){
                rp.poly();
                w.clearInterval( run );
            } );
        } else if( w.attachEvent ){
            w.attachEvent( "onload", function(){
                rp.poly();
                w.clearInterval( run );
            } );
        }
    }


    // commonjs
    if( typeof exports !== "undefined" ){
        exports.loadCSS = loadCSS;
    }
    else {
        w.loadCSS = loadCSS;
    }
}( typeof global !== "undefined" ? global : this ) );</script><script id="mcjs">!function(c,h,i,m,p){m=c.createElement(h),p=c.getElementsByTagName(h)[0],m.async=1,m.src=i,p.parentNode.insertBefore(m,p)}(document,"script","https://chimpstatic.com/mcjs-connected/js/users/7da58fc9885cb85d4a9f0ad9a/987f901145f1749fd3e800e86.js");</script><link rel="search" type="application/opensearchdescription+xml" href="https://hanxiao.io/atom.xml" title="Han Xiao Tech Blog - Neural Search &amp; AI Engineering"></head><body><div class="reading-progress-bar"></div><div class="wrap"><header><ul class="nav nav-list"><li class="nav-list-item"><a href="/about/" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="https://www.linkedin.com/in/hxiao87" target="_blank" class="nav-list-link">LINKEDIN</a></li><li class="nav-list-item"><a href="https://x.com/hxiao" target="_blank" class="nav-list-link">X</a></li><li class="nav-list-item"><a href="https://github.com/hanxiao" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="https://scholar.google.com/citations?user=jp7swwIAAAAJ" target="_blank" class="nav-list-link">SCHOLAR</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">Extremely Stupid Mistakes I Made With Tensorflow and Python</h1><div class="post-info">May 19, 2017 by &nbsp;&nbsp;&nbsp;<img src="/myavatar.png" alt="logo" width="18px" height="18px" style="vertical-align: sub;">&nbsp;<a href="/about" target="_blank" class="nav-list-link">Han Xiao - <i>ex</i> Senior Research Scientist @ Zalando Research</a></div><div class="post-info"><div class="read-time">◷&nbsp;&nbsp;&nbsp; 12  min read</div></div><div class="post-content"><h2><span id="background">Background</span></h2><p>Recently I started with Tensorflow for developing some RNN-based system. I choose Python 3 as the main language since TF has most stable API support for it. Plus, I can quickly set up web services via Flask and uWSGI. Previously I had some experience with this technology stack (Python+Flask+uWSGI) in production and I want to make it better this time. Although Java + Spring or Scala + akka may be better options for building a more scalable web app, they are probably overkill in my project, at least for now.</p>
<p>Building a highly scalable and available deep learning system is a topic for another day, here I want to talk about <a id="more"></a>some mistakes I made when using Tensorflow with python. This section is expected to grow continuously as I’m still learning Tensorflow. Some mistakes are extremely embarrassing as they become such obvious once I understand them. </p>
<h2><span id="mistake-1-i-used-dictget-as-switch-case">Mistake 1: I used <code>dict.get()</code> as switch-case</span></h2><p>Python doesn’t have switch-case, which means you have to write a long <code>if-elif-elif-else</code> block to do switching. As I used Scala a lot in my last project, I really miss the powerful and functional <a href="http://docs.scala-lang.org/tutorials/tour/pattern-matching.html" target="_blank" rel="noopener">pattern matching</a> in Scala.<br>As a workaround, <a href="http://stackoverflow.com/questions/60208/replacements-for-switch-statement-in-python" target="_blank" rel="noopener">the highest-voted solution on StackOverflow</a> teaches me to use <code>dict.get</code>:<br> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">'a'</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="string">'b'</span>: <span class="number">2</span>,</span><br><span class="line">    &#125;[x]</span><br></pre></td></tr></table></figure></p>
<p>Looks legit to me! And to follow this spirit, I wrote something like:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_loss_node</span><span class="params">(loss_type: str)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">'sigmoid'</span>: tf.nn.sigmoid_cross_entropy_with_logits(labels=self.Y_logit, logits=d_conf.Y),</span><br><span class="line">        <span class="string">'weight-sigmoid'</span>: tf.nn.weighted_cross_entropy_with_logits(targets=d_conf.Y, logits=self.Y_logit, pos_weight=tr_conf.pos_weight),</span><br><span class="line">        <span class="string">'rank-hinge'</span>: tf.map_fn(rank_hinge_loss, tf.stack([self.Y_logit, d_conf.Y], axis=<span class="number">1</span>),</span><br><span class="line">        <span class="string">'rank-sigmoid'</span>: tf.map_fn(rank_sigmoid_loss, tf.stack([self.Y_logit, d_conf.Y], axis=<span class="number">1</span>)</span><br><span class="line">        &#125;[loss_type]</span><br></pre></td></tr></table></figure></p>
<p>I use the Python 3 type annotation feature, that’s why you see <code>str</code> in the function argument. The main idea of this code snippet is to build different types of “loss node” in the Tensorflow computational graph according to the parameter <code>loss_type</code>. Comparing to the lame <code>if-else</code> block, I managed to save some lines. Pretty neat, right? Actually, NO!</p>
<p>The reason is: when you run this code, Python first constructs a <code>dict</code> variable with four items, each of which has a <code>str</code> key and <code>tf.Tensor</code> as value. Then it picks the one that matches the given <code>loss_type</code>. In other words, you create three useless nodes in the computational graph! To show how much time is wasted here, I made a small test.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> timeit</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo1</span><span class="params">(choice: str)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> choice == <span class="string">'a'</span>:</span><br><span class="line">        <span class="keyword">return</span> tf.Variable(<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">elif</span> choice == <span class="string">'b'</span>:</span><br><span class="line">        <span class="keyword">return</span> tf.Variable(<span class="number">6</span>)</span><br><span class="line">    <span class="keyword">elif</span> choice == <span class="string">'c'</span>:</span><br><span class="line">        <span class="keyword">return</span> tf.Variable(<span class="number">9</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo2</span><span class="params">(choice: str)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">'a'</span>: tf.Variable(<span class="number">3</span>),</span><br><span class="line">        <span class="string">'b'</span>: tf.Variable(<span class="number">6</span>),</span><br><span class="line">        <span class="string">'c'</span>: tf.Variable(<span class="number">9</span>)</span><br><span class="line">    &#125;[choice]</span><br><span class="line"></span><br><span class="line">node_types = [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo1_from_list</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [foo1(v) <span class="keyword">for</span> v <span class="keyword">in</span> node_types]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo2_from_list</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [foo2(v) <span class="keyword">for</span> v <span class="keyword">in</span> node_types]</span><br><span class="line"></span><br><span class="line">print(<span class="string">'foo1 takes %.3fs'</span> % timeit.timeit(foo1_from_list, number=<span class="number">100</span>))</span><br><span class="line">tf.reset_default_graph()</span><br><span class="line">print(<span class="string">'foo2 takes %.3fs'</span> % timeit.timeit(foo2_from_list, number=<span class="number">100</span>))</span><br></pre></td></tr></table></figure></p>
<p>which gives:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">foo1 takes 1.026s</span><br><span class="line">foo2 takes 3.091s</span><br></pre></td></tr></table></figure></p>
<p>Our <code>dict</code> trick is about three times slower than the lame <code>if-else</code>. However, slowness isn’t the only problem, it also significantly complicates the computational graph and induce some error. See the example below, where I create a bidirectional RNN chain:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">fw_cell = rnn.LSTMCell(<span class="number">5</span>)</span><br><span class="line">bk_cell = rnn.LSTMCell(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">bi_direction = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">X = tf.placeholder(tf.float32, [<span class="number">10</span>, <span class="number">5</span>, <span class="number">4</span>], name=<span class="string">'X'</span>)</span><br><span class="line">L = tf.placeholder(tf.int32, [<span class="number">10</span>], name=<span class="string">'L'</span>)</span><br><span class="line"></span><br><span class="line">val, _ = &#123;</span><br><span class="line">    <span class="literal">True</span>: tf.nn.bidirectional_dynamic_rnn(fw_cell,</span><br><span class="line">                                          bk_cell,</span><br><span class="line">                                          inputs=X,</span><br><span class="line">                                          sequence_length=L,</span><br><span class="line">                                          dtype=tf.float32),</span><br><span class="line">    <span class="literal">False</span>: tf.nn.dynamic_rnn(fw_cell,</span><br><span class="line">                             inputs=X,</span><br><span class="line">                             sequence_length=L,</span><br><span class="line">                             dtype=tf.float32)</span><br><span class="line">&#125;[bi_direction]</span><br></pre></td></tr></table></figure></p>
<p>We can use Tensorboard to visualize the computational graph behind, the <code>dict</code> trick produces:<br><img src="/2017/05/19/Extremely-Stupid-Mistakes-I-made-with-Tensorflow-and-Python/graph-com.png"><br>whereas the <code>if-else</code> gives a much simpler one:<br><img src="/2017/05/19/Extremely-Stupid-Mistakes-I-made-with-Tensorflow-and-Python/graph-simple.png"></p>
<p>On the latest Tensorflow (&gt; 1.1, probably starting from May 2017), the above code will also throw an error: </p>
<p><div class="tip"><br>    ValueError: Attempt to reuse RNNCell &lt;tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.BasicLSTMCell object at 0x10210d5c0&gt; with a different variable scope than its first use. First use of cell was with scope ‘rnn/multi_rnn_cell/cell_0/basic_lstm_cell’, this attempt is with scope ‘rnn/multi_rnn_cell/cell_1/basic_lstm_cell’. Please create a new instance of the cell if you would like it to use a different set of weights. If before you were using: MultiRNNCell([BasicLSTMCell(…)] * num_layers), change to: MultiRNNCell([BasicLSTMCell(…) for _ in range(num_layers)])<br></div><br>The reason is that <code>dict</code> creates two RNN chains that share the same RNN cell, i.e. <code>fw_cell</code>. </p>
<h4><span id="but-i-still-want-to-be-cool">But I still want to be cool</span></h4><p>How come I didn’t notice this before? Well, probably because I mainly used this trick on some simple task such as printing constants, where the side-effect of the dictionary construction is neglectable. But what if one still want to use this trick instead of writing the <code>if-else</code> block? <code>lambda</code> expression can be a workaround.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo3</span><span class="params">(choice: str)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">'a'</span>: <span class="keyword">lambda</span>: tf.Variable(<span class="number">3</span>),</span><br><span class="line">        <span class="string">'b'</span>: <span class="keyword">lambda</span>: tf.Variable(<span class="number">6</span>),</span><br><span class="line">        <span class="string">'c'</span>: <span class="keyword">lambda</span>: tf.Variable(<span class="number">9</span>)</span><br><span class="line">    &#125;[choice]()</span><br></pre></td></tr></table></figure></p>
<p>In this code, I wrap the variable with a python lambda expression (i.e. an in-line function), so that the value of a dictionary item is a function handler rather than a TF variable. In fact, TF variable is not constructed until <code>()</code> is called (after <code>[choice]</code>). The speed test also shows that it takes the same time as <code>if-else</code> block.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">foo1 takes 1.132s</span><br><span class="line">foo3 takes 1.096s</span><br></pre></td></tr></table></figure></p>
<h2><span id="mistake-2-i-used-lambda-expressions-a-lot">Mistake 2: I used lambda expressions a lot</span></h2><p>Do I and lambda expression live happily ever after? Well, until I realized that lambda expression is <strong>the main reason of low GPU utilization</strong>. </p>
<p>The code below employs lambda expression to unify the function arguments of different loss functions in a multi-label classification problem:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">my_loss = &#123;</span><br><span class="line">    <span class="string">'rank-hinge'</span>: <span class="keyword">lambda</span> y_l, y_t: rank_hinge_loss(y_l, y_t),</span><br><span class="line">    <span class="string">'rank-sigmoid'</span>: <span class="keyword">lambda</span> y_l, y_t: rank_sigmoid_loss(y_l, y_t),</span><br><span class="line">    <span class="string">'weight-sigmoid'</span>: <span class="keyword">lambda</span> y_l, y_t: tf.nn.weighted_cross_entropy_with_logits(targets=y_t, logits=y_l, pos_weight=<span class="number">3</span>),</span><br><span class="line">    <span class="string">'sigmoid'</span>: <span class="keyword">lambda</span> y_l, y_t: tf.nn.sigmoid_cross_entropy_with_logits(labels=y_t, logits=y_l)</span><br><span class="line">&#125;[loss_type]</span><br></pre></td></tr></table></figure></p>
<p><code>rank_hinge_loss</code> and <code>rank_sigmoid_loss</code> are customized loss function written by me, which could also be a topic on another day. <code>sigmoid_cross_entropy_with_logits</code> and <code>weighted_cross_entropy_with_logits</code> are Tensorflow built-in loss functions for multi-label problem which somehow have different names for the groundtruth (<code>targets</code> and <code>labels</code>). Anyway, by using lambda expression all those differences should go away, and we can just write something like:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">self.Y_logit = tf.add(tf.matmul(last, weight_o), bias_o, name=<span class="string">'logit'</span>)</span><br><span class="line">self.loss = my_loss(self.Y_logit, self.Y)</span><br><span class="line"></span><br><span class="line">train_step = self.train_conf.optimizer.minimize(self.loss)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(self.train_conf.num_epoch):</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;</span><br><span class="line">                                     self.X: train_data[<span class="number">0</span>],</span><br><span class="line">                                     self.Y: train_data[<span class="number">1</span>],</span><br><span class="line">                                     self.L: train_data[<span class="number">2</span>]</span><br><span class="line">                                 &#125;)</span><br></pre></td></tr></table></figure></p>
<p>However, when I deploy the code on a GPU instance, I found the GPU utilization is far from 100%. On average it’s only 20% utilization, and training speed is not faster than my 4-core laptop. </p>
<p>I added <code>with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:</code> and looked into the variable placement. Surprisingly, all loss-related TF variables are located on CPU not GPU, even though I force them to via <code>with tf.device(&#39;/gpu:0&#39;):</code>.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Loss_1/tags: (Const): /job:localhost/replica:0/task:0/cpu:0</span><br><span class="line">Loss/Const_3: (Const): /job:localhost/replica:0/task:0/cpu:0</span><br><span class="line">Loss/Const_2: (Const): /job:localhost/replica:0/task:0/cpu:0</span><br><span class="line">Loss/logistic_loss/sub/x: (Const): /job:localhost/replica:0/task:0/cpu:0</span><br><span class="line">Loss/logistic_loss/add/x: (Const): /job:localhost/replica:0/task:0/cpu:0</span><br><span class="line">Loss/logistic_loss/mul/x: (Const): /job:localhost/replica:0/task:0/cpu:0</span><br></pre></td></tr></table></figure></p>
<p>Things did not change when I set <code>loss_type = &#39;sigmoid&#39;</code> and used TF built-in loss. What is the problem?</p>
<p>The problem is by wrapping the loss function with a lambda expression, you make a TF variable / tensor / operator a standard Python function, which can not be efficiently evaluated on GPU with TF. As the loss function is evaluated in every iteration, this implementation is significantly slower than the standard TF loss operator. Once again, slowness is not the complete story. If we look at the computational graph with Tensorboard. The lambda version of <code>sigmoid</code> loss looks like this:</p>
<img src="/2017/05/19/Extremely-Stupid-Mistakes-I-made-with-Tensorflow-and-Python/graph-loss.png">
<p>The expression <code>lambda x, y: tf.someloss(x, y)</code> adds three extra nodes for <code>x</code>, <code>y</code> and <code>tf.someloss(x, y)</code> to the graph which are not used at all when computing the gradient. To see it more clearly, one can highlight the input traces of the <code>gradients</code> node.</p>
<img src="/2017/05/19/Extremely-Stupid-Mistakes-I-made-with-Tensorflow-and-Python/graph-loss-trace.png">
<p>One can observe that, only <code>logistic_loss_1</code> is connected to the <code>gradients</code> node. The other three nodes are completely useless while training. After I refactored the code by removing unnecessary lambda expressions, the GPU utilization grows to 70%. A significant speedup can be observed. What about the remaining 30%? Did I give up on that and let it go? Of course not, but it is a story for another day. </p>
<h2><span id="conclusion">Conclusion</span></h2><p>Having done my last project with Scala, I kind of into the functional programming and sugar-code in Scala. The mistakes above may not happen in the first place if some one follows the Pythonic way of programming. But hey, life is more fun when you do some exploration. The lesson learned here is to keep the code simple and readable. When you refactor the code, always take care of the possible side-effect, especially with the computational graph in Tensorflow. In Python, the code is usually interpreted and executed line by line. In Tensorflow <code>sess.run()</code> is the actual starting point, meaning that there is no actual computation until <code>sess.run()</code> invoked. This subtle difference may cause some troubles especially for beginners.</p>
</div></article></div></main><footer><div class="paginator"><a href="/2017/07/07/Get-10x-Speedup-in-Tensorflow-Multi-Task-Learning-using-Python-Multiprocessing/" class="prev">&nbsp;❮&nbsp;&nbsp;Get 10x Speedup in T...</a></div><div class="footer-section"><h2><img src="/flower.png" alt="Checkout this">Check out these posts too!</h2><div class="archive-readmore"><div style="display: flex; gap: 10px; align-items: flex-start;" class="post-item"><a href="/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0/"><img src="https://hanxiao.io/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0//9ba076d0.png" alt="Video Semantic Search in Large Scale using GNES and Tensorflow 2.0" style="width: 120px; height: 90px; object-fit: cover;"></a><div style="flex: 1;" class="book-title"><h5 style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; margin: 0;" class="post-title"><a href="/2019/11/22/Video-Semantic-Search-in-Large-Scale-using-GNES-and-TF-2-0/" class="post-title-link">Video Semantic Search in Large Scale using GNES and Tensorflow 2.0</a></h5></div></div><div style="display: flex; gap: 10px; align-items: flex-start;" class="post-item"><a href="/2019/11/07/A-Better-Practice-for-Managing-extras-require-Dependencies-in-Python/"><img src="https://hanxiao.io/2019/11/07/A-Better-Practice-for-Managing-extras-require-Dependencies-in-Python//banner.png" alt="A Better Practice for Managing Many &lt;code&gt;extras_require&lt;/code&gt; Dependencies in Python" style="width: 120px; height: 90px; object-fit: cover;"></a><div style="flex: 1;" class="book-title"><h5 style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; margin: 0;" class="post-title"><a href="/2019/11/07/A-Better-Practice-for-Managing-extras-require-Dependencies-in-Python/" class="post-title-link">A Better Practice for Managing Many <code>extras_require</code> Dependencies in Python</a></h5></div></div><div style="display: flex; gap: 10px; align-items: flex-start;" class="post-item"><a href="/2019/10/18/GNES-Flow-a-Pythonic-Way-to-Build-Cloud-Native-Neural-Search-Pipelines/"><img src="https://hanxiao.io/2019/10/18/GNES-Flow-a-Pythonic-Way-to-Build-Cloud-Native-Neural-Search-Pipelines//gnes-flow-banner.png" alt="GNES Flow: a Pythonic Way to Build Cloud-Native Neural Search Pipelines" style="width: 120px; height: 90px; object-fit: cover;"></a><div style="flex: 1;" class="book-title"><h5 style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; margin: 0;" class="post-title"><a href="/2019/10/18/GNES-Flow-a-Pythonic-Way-to-Build-Cloud-Native-Neural-Search-Pipelines/" class="post-title-link">GNES Flow: a Pythonic Way to Build Cloud-Native Neural Search Pipelines</a></h5></div></div><div style="display: flex; gap: 10px; align-items: flex-start;" class="post-item"><a href="/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/"><img src="https://hanxiao.io/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond//gnes-team-1600.JPG" alt="Generic Neural Elastic Search: From &lt;code&gt;bert-as-service&lt;/code&gt; and Go Way Beyond" style="width: 120px; height: 90px; object-fit: cover;"></a><div style="flex: 1;" class="book-title"><h5 style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; margin: 0;" class="post-title"><a href="/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond/" class="post-title-link">Generic Neural Elastic Search: From <code>bert-as-service</code> and Go Way Beyond</a></h5></div></div></div></div><div class="copyright"><p>© 2017 - 2025 <a href="https://hanxiao.io">Han Xiao</a>. <img src="/by-nc-sa.svg" alt="Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License." class="image"></p></div></footer></div><link rel="stylesheet" href="/css/post/katex.min.css"><!--link(rel="stylesheet", href=url_for("css/post/gitment.css"))--><script src="/js/katex.min.js"></script><script src="/js/auto-render.min.js"></script><script>renderMathInElement(
    document.body,
    {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "\\[", right: "\\]", display: true},
            {left: "$", right: "$", display: false},
            {left: "\\(", right: "\\)", display: false}
        ]
    }
);</script><!--script(src=url_for("js/gitment.browser.js"))--><!--script(src=url_for("js/gitment.loader.js"))--><script src="/js/jquery-3.4.1.min.js"></script><script src="/js/reading_progress.min.js"></script><script src="/js/highlighter.min.js"></script><script src="/js/init-highlighter.js"></script><script async src="https://www.google-analytics.com/analytics.js"></script><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create',"UA-52114253-1",'auto');ga('send','pageview');</script></body></html>